20240219 18:59:12[INFO] main start ...
20240219 18:59:12[INFO] [{'id': 0, 'mem_use': 5475.0, 'mem_total': 24564.0, 'pwr_use': 55.0, 'pwr_total': 450.0, 'gpu_util': 0.0}, {'id': 1, 'mem_use': 19777.0, 'mem_total': 24564.0, 'pwr_use': 387.0, 'pwr_total': 450.0, 'gpu_util': 99.0}, {'id': 2, 'mem_use': 5.0, 'mem_total': 24564.0, 'pwr_use': 76.0, 'pwr_total': 450.0, 'gpu_util': 0.0}, {'id': 3, 'mem_use': 5.0, 'mem_total': 24564.0, 'pwr_use': 107.0, 'pwr_total': 450.0, 'gpu_util': 0.0}, {'id': 4, 'mem_use': 5.0, 'mem_total': 24564.0, 'pwr_use': 94.0, 'pwr_total': 450.0, 'gpu_util': 0.0}, {'id': 5, 'mem_use': 5.0, 'mem_total': 24564.0, 'pwr_use': 114.0, 'pwr_total': 450.0, 'gpu_util': 0.0}, {'id': 6, 'mem_use': 5.0, 'mem_total': 24564.0, 'pwr_use': 65.0, 'pwr_total': 450.0, 'gpu_util': 0.0}, {'id': 7, 'mem_use': 5.0, 'mem_total': 24564.0, 'pwr_use': 80.0, 'pwr_total': 450.0, 'gpu_util': 0.0}]
20240219 18:59:13[INFO] start process-0
20240219 18:59:13[INFO] start process-1
20240219 18:59:13[INFO] start process-2
20240219 18:59:13[INFO] start process-3
20240219 18:59:13[INFO] start process-4
20240219 18:59:13[INFO] start process-5
20240219 18:59:13[INFO] start process-6
20240219 18:59:13[INFO] main exit.
20240219 18:59:13[INFO] start process-7
20240219 18:59:14[INFO] gpu-1 is busy, wait 300s
20240219 18:59:28[INFO] gpu-0 is busy, wait 300s
20240219 19:04:15[INFO] gpu-1 is busy, wait 300s
20240219 19:04:36[INFO] gpu-0 is busy, wait 300s
20240219 19:09:16[INFO] gpu-1 is busy, wait 300s
20240219 19:10:00[INFO] gpu-0 is busy, wait 300s
20240219 19:14:17[INFO] gpu-1 is busy, wait 300s
20240219 19:15:24[INFO] gpu-0 is busy, wait 300s
20240219 19:19:18[INFO] gpu-1 is busy, wait 300s
20240219 19:20:31[INFO] gpu-0 is busy, wait 300s
20240219 19:24:18[INFO] gpu-1 is busy, wait 300s
20240219 19:25:46[INFO] gpu-0 is busy, wait 300s
20240219 19:29:19[INFO] gpu-1 is busy, wait 300s
20240219 19:31:06[INFO] gpu-0 is busy, wait 300s
20240219 19:34:20[INFO] gpu-1 is busy, wait 300s
20240219 19:36:59[INFO] gpu-0 is busy, wait 300s
20240219 19:39:21[INFO] gpu-1 is busy, wait 300s
20240219 19:42:13[INFO] gpu-0 is busy, wait 300s
20240219 19:44:22[INFO] gpu-1 is busy, wait 300s
20240219 19:47:41[INFO] gpu-0 is busy, wait 300s
20240219 19:49:23[INFO] gpu-1 is busy, wait 300s
20240219 19:52:49[INFO] gpu-0 is busy, wait 300s
20240219 19:54:24[INFO] gpu-1 is busy, wait 300s
20240219 19:58:01[INFO] gpu-0 is busy, wait 300s
20240219 19:59:25[INFO] gpu-1 is busy, wait 300s
20240219 20:03:13[INFO] gpu-0 is busy, wait 300s
20240219 20:04:26[INFO] gpu-1 is busy, wait 300s
20240219 20:08:25[INFO] gpu-0 is busy, wait 300s
20240219 20:09:26[INFO] gpu-1 is busy, wait 300s
20240219 20:14:00[INFO] gpu-0 is busy, wait 300s
20240219 20:14:27[INFO] gpu-1 is busy, wait 300s
20240219 20:19:07[INFO] gpu-0 is busy, wait 300s
20240219 20:19:28[INFO] gpu-1 is busy, wait 300s
20240219 20:24:14[INFO] gpu-0 is busy, wait 300s
20240219 20:24:29[INFO] gpu-1 is busy, wait 300s
20240219 20:29:29[INFO] gpu-1 is busy, wait 300s
20240219 20:29:37[INFO] gpu-0 is busy, wait 300s
20240219 20:34:30[INFO] gpu-1 is busy, wait 300s
20240219 20:35:07[INFO] gpu-0 is busy, wait 300s
20240219 20:39:31[INFO] gpu-1 is busy, wait 300s
20240219 20:40:41[INFO] gpu-0 is busy, wait 300s
20240219 20:44:32[INFO] gpu-1 is busy, wait 300s
20240219 20:45:48[INFO] gpu-0 is busy, wait 300s
20240219 20:49:33[INFO] gpu-1 is busy, wait 300s
20240219 20:51:16[INFO] gpu-0 is busy, wait 300s
20240219 20:54:34[INFO] gpu-1 is busy, wait 300s
20240219 20:56:23[INFO] gpu-0 is busy, wait 300s
20240219 20:59:35[INFO] gpu-1 is busy, wait 300s
20240219 21:01:35[INFO] gpu-0 is busy, wait 300s
20240219 21:28:45[INFO] gpu-7 is busy, wait 300s
20240219 21:28:52[INFO] gpu-4 is busy, wait 300s
20240219 21:29:35[INFO] gpu-1 is busy, wait 300s
20240219 21:29:46[INFO] gpu-3 is busy, wait 300s
20240219 21:30:07[INFO] gpu-5 is busy, wait 300s
20240219 21:30:35[INFO] gpu-6 is busy, wait 300s
20240219 21:30:36[INFO] gpu-2 is busy, wait 300s
20240219 21:33:13[INFO] gpu-0 is busy, wait 300s
20240219 21:33:45[INFO] gpu-7 is busy, wait 300s
20240219 21:33:53[INFO] gpu-4 is busy, wait 300s
20240219 21:34:36[INFO] gpu-1 is busy, wait 300s
20240219 21:34:46[INFO] gpu-3 is busy, wait 300s
20240219 21:35:08[INFO] gpu-5 is busy, wait 300s
20240219 21:35:36[INFO] gpu-6 is busy, wait 300s
20240219 21:35:37[INFO] gpu-2 is busy, wait 300s
20240219 21:38:17[INFO] gpu-0 is busy, wait 300s
20240219 21:38:46[INFO] gpu-7 is busy, wait 300s
20240219 21:38:53[INFO] gpu-4 is busy, wait 300s
20240219 21:39:36[INFO] gpu-1 is busy, wait 300s
20240219 21:39:47[INFO] gpu-3 is busy, wait 300s
20240219 21:40:09[INFO] gpu-5 is busy, wait 300s
20240219 21:40:37[INFO] gpu-6 is busy, wait 300s
20240219 21:40:38[INFO] gpu-2 is busy, wait 300s
20240219 21:43:18[INFO] gpu-0 is busy, wait 300s
20240219 21:43:46[INFO] gpu-7 is busy, wait 300s
20240219 21:43:54[INFO] gpu-4 is busy, wait 300s
20240219 21:44:37[INFO] gpu-1 is busy, wait 300s
20240219 21:44:48[INFO] gpu-3 is busy, wait 300s
20240219 21:45:09[INFO] gpu-5 is busy, wait 300s
20240219 21:45:37[INFO] gpu-6 is busy, wait 300s
20240219 21:45:38[INFO] gpu-2 is busy, wait 300s
20240219 21:48:46[INFO] gpu-0 is busy, wait 300s
20240219 21:48:47[INFO] gpu-7 is busy, wait 300s
20240219 21:48:55[INFO] gpu-4 is busy, wait 300s
20240219 21:49:38[INFO] gpu-1 is busy, wait 300s
20240219 21:49:48[INFO] gpu-3 is busy, wait 300s
20240219 21:50:10[INFO] gpu-5 is busy, wait 300s
20240219 21:50:38[INFO] gpu-6 is busy, wait 300s
20240219 21:50:39[INFO] gpu-2 is busy, wait 300s
20240219 21:53:48[INFO] gpu-7 is busy, wait 300s
20240219 21:53:53[INFO] gpu-0 is busy, wait 300s
20240219 21:53:55[INFO] gpu-4 is busy, wait 300s
20240219 21:54:38[INFO] gpu-1 is busy, wait 300s
20240219 21:54:49[INFO] gpu-3 is busy, wait 300s
20240219 21:55:10[INFO] gpu-5 is busy, wait 300s
20240219 21:55:38[INFO] gpu-6 is busy, wait 300s
20240219 21:55:39[INFO] gpu-2 is busy, wait 300s
20240219 21:58:48[INFO] gpu-7 is busy, wait 300s
20240219 21:58:53[INFO] gpu-0 is busy, wait 300s
20240219 21:58:56[INFO] gpu-4 is busy, wait 300s
20240219 21:59:39[INFO] gpu-1 is busy, wait 300s
20240219 21:59:49[INFO] gpu-3 is busy, wait 300s
20240219 22:00:11[INFO] gpu-5 is busy, wait 300s
20240219 22:00:39[INFO] gpu-6 is busy, wait 300s
20240219 22:00:40[INFO] gpu-2 is busy, wait 300s
20240219 22:03:49[INFO] gpu-7 is busy, wait 300s
20240219 22:03:56[INFO] gpu-4 is busy, wait 300s
20240219 22:04:00[INFO] gpu-0 is busy, wait 300s
20240219 22:04:39[INFO] gpu-1 is busy, wait 300s
20240219 22:04:50[INFO] gpu-3 is busy, wait 300s
20240219 22:05:11[INFO] gpu-5 is busy, wait 300s
20240219 22:05:39[INFO] gpu-6 is busy, wait 300s
20240219 22:05:41[INFO] gpu-2 is busy, wait 300s
20240219 22:08:49[INFO] gpu-7 is busy, wait 300s
20240219 22:08:57[INFO] gpu-4 is busy, wait 300s
20240219 22:09:00[INFO] gpu-0 is busy, wait 300s
20240219 22:09:40[INFO] gpu-1 is busy, wait 300s
20240219 22:09:51[INFO] gpu-3 is busy, wait 300s
20240219 22:10:12[INFO] gpu-5 is busy, wait 300s
20240219 22:10:40[INFO] gpu-6 is busy, wait 300s
20240219 22:10:41[INFO] gpu-2 is busy, wait 300s
20240220 10:20:02[INFO] gpu-0 is busy, wait 300s
20240220 10:25:03[INFO] gpu-0 is busy, wait 300s
20240220 10:30:04[INFO] gpu-0 is busy, wait 300s
20240220 11:26:25[INFO] gpu-0 is busy, wait 300s
20240220 11:32:54[INFO] gpu-0 is busy, wait 300s
20240220 11:38:48[INFO] gpu-0 is busy, wait 300s
20240220 11:45:20[INFO] gpu-0 is busy, wait 300s
20240220 11:51:10[INFO] gpu-0 is busy, wait 300s
20240220 11:57:38[INFO] gpu-0 is busy, wait 300s
20240220 12:04:20[INFO] gpu-0 is busy, wait 300s
20240220 12:09:30[INFO] gpu-0 is busy, wait 300s
20240220 12:14:30[INFO] gpu-0 is busy, wait 300s
20240220 13:21:38[INFO] gpu-0 is busy, wait 300s
20240220 13:26:39[INFO] gpu-0 is busy, wait 300s
20240220 13:31:39[INFO] gpu-0 is busy, wait 300s
20240220 13:44:39[INFO] gpu-0 is busy, wait 300s
20240220 13:49:59[INFO] gpu-0 is busy, wait 300s
20240220 15:41:38[INFO] gpu-0 is busy, wait 300s
20240220 15:46:39[INFO] gpu-0 is busy, wait 300s
20240220 16:44:17[INFO] gpu-0 is busy, wait 300s
20240220 16:50:28[INFO] gpu-0 is busy, wait 300s
20240220 16:55:36[INFO] gpu-0 is busy, wait 300s
20240220 17:00:37[INFO] gpu-0 is busy, wait 300s
20240220 17:07:00[INFO] gpu-0 is busy, wait 300s
20240220 17:12:35[INFO] gpu-0 is busy, wait 300s
20240220 17:17:36[INFO] gpu-0 is busy, wait 300s
20240220 17:23:21[INFO] gpu-0 is busy, wait 300s
20240220 17:28:48[INFO] gpu-0 is busy, wait 300s
20240220 17:33:57[INFO] gpu-0 is busy, wait 300s
20240220 17:39:04[INFO] gpu-0 is busy, wait 300s
20240220 17:44:25[INFO] gpu-0 is busy, wait 300s
20240220 17:50:14[INFO] gpu-0 is busy, wait 300s
20240220 19:52:10[INFO] gpu-0 is busy, wait 300s
20240220 19:57:17[INFO] gpu-0 is busy, wait 300s
20240220 20:02:17[INFO] gpu-0 is busy, wait 300s
20240220 20:07:39[INFO] gpu-0 is busy, wait 300s
20240220 20:12:45[INFO] gpu-0 is busy, wait 300s
20240220 20:18:02[INFO] gpu-0 is busy, wait 300s
20240220 20:32:46[INFO] gpu-2 is busy, wait 300s
20240220 20:34:37[INFO] gpu-0 is busy, wait 300s
20240220 20:38:23[INFO] gpu-2 is busy, wait 300s
20240220 20:39:38[INFO] gpu-0 is busy, wait 300s
20240220 20:44:38[INFO] gpu-0 is busy, wait 300s
20240220 20:49:39[INFO] gpu-0 is busy, wait 300s
20240220 20:55:07[INFO] gpu-0 is busy, wait 300s
20240220 21:00:34[INFO] gpu-0 is busy, wait 300s
20240220 21:06:26[INFO] gpu-0 is busy, wait 300s
20240220 21:11:55[INFO] gpu-0 is busy, wait 300s
20240220 21:17:02[INFO] gpu-0 is busy, wait 300s
20240220 21:22:16[INFO] gpu-0 is busy, wait 300s
20240220 21:27:17[INFO] gpu-0 is busy, wait 300s
20240220 21:32:32[INFO] gpu-0 is busy, wait 300s
20240220 21:37:38[INFO] gpu-0 is busy, wait 300s
20240220 21:42:46[INFO] gpu-0 is busy, wait 300s
20240220 21:48:12[INFO] gpu-0 is busy, wait 300s
20240220 21:53:21[INFO] gpu-0 is busy, wait 300s
20240220 21:58:27[INFO] gpu-0 is busy, wait 300s
20240220 22:03:28[INFO] gpu-0 is busy, wait 300s
20240220 22:08:39[INFO] gpu-0 is busy, wait 300s
20240220 22:13:45[INFO] gpu-0 is busy, wait 300s
20240220 22:15:47[INFO] gpu-4 is busy, wait 300s
20240220 22:15:51[INFO] gpu-2 is busy, wait 300s
20240220 22:16:13[INFO] gpu-6 is busy, wait 300s
20240220 22:16:15[INFO] gpu-1 is busy, wait 300s
20240220 22:16:26[INFO] gpu-3 is busy, wait 300s
20240220 22:16:35[INFO] gpu-5 is busy, wait 300s
20240220 22:18:50[INFO] gpu-0 is busy, wait 300s
20240220 22:21:02[INFO] gpu-4 is busy, wait 300s
20240220 22:21:03[INFO] gpu-2 is busy, wait 300s
20240220 22:21:26[INFO] gpu-6 is busy, wait 300s
20240220 22:21:31[INFO] gpu-3 is busy, wait 300s
20240220 22:21:42[INFO] gpu-5 is busy, wait 300s
20240220 22:21:43[INFO] gpu-1 is busy, wait 300s
20240220 22:24:02[INFO] gpu-0 is busy, wait 300s
20240220 22:25:26[INFO] gpu-7 is busy, wait 300s
20240220 22:26:03[INFO] gpu-4 is busy, wait 300s
20240220 22:26:03[INFO] gpu-2 is busy, wait 300s
20240220 22:26:26[INFO] gpu-6 is busy, wait 300s
20240220 22:26:31[INFO] gpu-3 is busy, wait 300s
20240220 22:26:42[INFO] gpu-5 is busy, wait 300s
20240220 22:26:44[INFO] gpu-1 is busy, wait 300s
20240220 22:29:02[INFO] gpu-0 is busy, wait 300s
20240220 22:31:44[INFO] gpu-1 is busy, wait 300s
20240220 22:34:07[INFO] gpu-0 is busy, wait 300s
20240220 22:34:20[INFO] gpu-2 is busy, wait 300s
20240220 22:34:21[INFO] gpu-4 is busy, wait 300s
20240220 22:34:25[INFO] gpu-5 is busy, wait 300s
20240220 22:34:30[INFO] gpu-3 is busy, wait 300s
20240220 22:34:30[INFO] gpu-6 is busy, wait 300s
20240220 22:36:50[INFO] gpu-1 is busy, wait 300s
20240220 22:39:13[INFO] gpu-0 is busy, wait 300s
20240220 22:39:26[INFO] gpu-2 is busy, wait 300s
20240220 22:39:32[INFO] gpu-5 is busy, wait 300s
20240220 22:39:35[INFO] gpu-3 is busy, wait 300s
20240220 22:39:39[INFO] gpu-4 is busy, wait 300s
20240220 22:39:40[INFO] gpu-6 is busy, wait 300s
20240220 22:41:54[INFO] gpu-1 is busy, wait 300s
20240220 22:43:59[INFO] gpu-7 is busy, wait 300s
20240220 22:44:14[INFO] gpu-0 is busy, wait 300s
20240220 22:44:26[INFO] gpu-2 is busy, wait 300s
20240220 22:44:32[INFO] gpu-5 is busy, wait 300s
20240220 22:44:35[INFO] gpu-3 is busy, wait 300s
20240220 22:44:40[INFO] gpu-4 is busy, wait 300s
20240220 22:44:41[INFO] gpu-6 is busy, wait 300s
20240220 22:46:54[INFO] gpu-1 is busy, wait 300s
20240220 22:49:19[INFO] gpu-0 is busy, wait 300s
20240220 22:54:44[INFO] gpu-0 is busy, wait 300s
20240220 22:59:59[INFO] gpu-0 is busy, wait 300s
20240220 23:05:04[INFO] gpu-0 is busy, wait 300s
20240220 23:10:09[INFO] gpu-0 is busy, wait 300s
20240220 23:14:08[INFO] gpu-1 is busy, wait 300s
20240220 23:15:30[INFO] gpu-0 is busy, wait 300s
20240220 23:19:09[INFO] gpu-1 is busy, wait 300s
20240220 23:20:37[INFO] gpu-0 is busy, wait 300s
20240220 23:24:09[INFO] gpu-1 is busy, wait 300s
20240220 23:25:51[INFO] gpu-0 is busy, wait 300s
20240220 23:31:06[INFO] gpu-0 is busy, wait 300s
20240220 23:36:21[INFO] gpu-0 is busy, wait 300s
20240220 23:41:37[INFO] gpu-0 is busy, wait 300s
20240220 23:47:15[INFO] gpu-0 is busy, wait 300s
20240220 23:52:31[INFO] gpu-0 is busy, wait 300s
20240220 23:57:39[INFO] gpu-0 is busy, wait 300s
20240221 00:04:11[INFO] gpu-0 is busy, wait 300s
20240221 00:09:25[INFO] gpu-0 is busy, wait 300s
20240221 00:14:38[INFO] gpu-0 is busy, wait 300s
20240221 00:20:01[INFO] gpu-0 is busy, wait 300s
20240221 00:25:14[INFO] gpu-0 is busy, wait 300s
20240221 00:30:52[INFO] gpu-0 is busy, wait 300s
20240221 00:36:13[INFO] gpu-0 is busy, wait 300s
20240221 00:41:42[INFO] gpu-0 is busy, wait 300s
20240221 00:47:12[INFO] gpu-0 is busy, wait 300s
20240221 00:52:27[INFO] gpu-0 is busy, wait 300s
20240221 00:57:33[INFO] gpu-0 is busy, wait 300s
20240221 01:02:40[INFO] gpu-0 is busy, wait 300s
20240221 01:08:12[INFO] gpu-0 is busy, wait 300s
20240221 01:13:39[INFO] gpu-0 is busy, wait 300s
20240221 01:18:53[INFO] gpu-0 is busy, wait 300s
20240221 01:24:41[INFO] gpu-0 is busy, wait 300s
20240221 01:29:49[INFO] gpu-0 is busy, wait 300s
20240221 01:34:54[INFO] gpu-0 is busy, wait 300s
20240221 01:40:02[INFO] gpu-0 is busy, wait 300s
20240221 01:45:12[INFO] gpu-0 is busy, wait 300s
20240221 01:50:30[INFO] gpu-0 is busy, wait 300s
20240221 01:55:48[INFO] gpu-0 is busy, wait 300s
20240221 02:01:03[INFO] gpu-0 is busy, wait 300s
20240221 02:06:10[INFO] gpu-0 is busy, wait 300s
20240221 02:11:18[INFO] gpu-0 is busy, wait 300s
20240221 02:16:42[INFO] gpu-0 is busy, wait 300s
20240221 02:21:57[INFO] gpu-0 is busy, wait 300s
20240221 02:27:18[INFO] gpu-0 is busy, wait 300s
20240221 02:32:41[INFO] gpu-0 is busy, wait 300s
20240221 02:37:58[INFO] gpu-0 is busy, wait 300s
20240221 02:43:05[INFO] gpu-0 is busy, wait 300s
20240221 02:48:11[INFO] gpu-0 is busy, wait 300s
20240221 02:54:07[INFO] gpu-0 is busy, wait 300s
20240221 02:59:30[INFO] gpu-0 is busy, wait 300s
20240221 03:04:38[INFO] gpu-0 is busy, wait 300s
20240221 03:10:21[INFO] gpu-0 is busy, wait 300s
20240221 03:15:54[INFO] gpu-0 is busy, wait 300s
20240221 03:21:02[INFO] gpu-0 is busy, wait 300s
20240221 03:26:41[INFO] gpu-0 is busy, wait 300s
20240221 03:31:49[INFO] gpu-0 is busy, wait 300s
20240221 03:36:55[INFO] gpu-0 is busy, wait 300s
20240221 03:42:20[INFO] gpu-0 is busy, wait 300s
20240221 03:47:28[INFO] gpu-0 is busy, wait 300s
20240221 03:52:34[INFO] gpu-0 is busy, wait 300s
20240221 03:57:41[INFO] gpu-0 is busy, wait 300s
20240221 04:02:48[INFO] gpu-0 is busy, wait 300s
20240221 04:08:08[INFO] gpu-0 is busy, wait 300s
20240221 04:13:29[INFO] gpu-0 is busy, wait 300s
20240221 04:18:38[INFO] gpu-0 is busy, wait 300s
20240221 04:25:27[INFO] gpu-0 is busy, wait 300s
20240221 04:31:08[INFO] gpu-0 is busy, wait 300s
20240221 04:36:35[INFO] gpu-0 is busy, wait 300s
20240221 04:41:58[INFO] gpu-0 is busy, wait 300s
20240221 04:47:12[INFO] gpu-0 is busy, wait 300s
20240221 04:52:34[INFO] gpu-0 is busy, wait 300s
20240221 04:57:55[INFO] gpu-0 is busy, wait 300s
20240221 05:03:18[INFO] gpu-0 is busy, wait 300s
20240221 05:08:23[INFO] gpu-0 is busy, wait 300s
20240221 05:13:47[INFO] gpu-0 is busy, wait 300s
20240221 05:19:37[INFO] gpu-0 is busy, wait 300s
20240221 05:24:42[INFO] gpu-0 is busy, wait 300s
20240221 05:30:48[INFO] gpu-0 is busy, wait 300s
20240221 05:36:03[INFO] gpu-0 is busy, wait 300s
20240221 05:41:11[INFO] gpu-0 is busy, wait 300s
20240221 05:46:18[INFO] gpu-0 is busy, wait 300s
20240221 05:51:24[INFO] gpu-0 is busy, wait 300s
20240221 05:56:30[INFO] gpu-0 is busy, wait 300s
20240221 06:02:01[INFO] gpu-0 is busy, wait 300s
20240221 06:07:17[INFO] gpu-0 is busy, wait 300s
20240221 06:12:25[INFO] gpu-0 is busy, wait 300s
20240221 06:17:33[INFO] gpu-0 is busy, wait 300s
20240221 06:22:58[INFO] gpu-0 is busy, wait 300s
20240221 06:28:06[INFO] gpu-0 is busy, wait 300s
20240221 06:33:14[INFO] gpu-0 is busy, wait 300s
20240221 06:38:49[INFO] gpu-0 is busy, wait 300s
20240221 06:44:17[INFO] gpu-0 is busy, wait 300s
20240221 06:49:28[INFO] gpu-0 is busy, wait 300s
20240221 06:54:56[INFO] gpu-0 is busy, wait 300s
20240221 07:00:17[INFO] gpu-0 is busy, wait 300s
20240221 07:05:49[INFO] gpu-0 is busy, wait 300s
20240221 07:11:09[INFO] gpu-0 is busy, wait 300s
20240221 07:16:16[INFO] gpu-0 is busy, wait 300s
20240221 07:21:47[INFO] gpu-0 is busy, wait 300s
20240221 07:27:46[INFO] gpu-0 is busy, wait 300s
20240221 07:33:01[INFO] gpu-0 is busy, wait 300s
20240221 07:38:07[INFO] gpu-0 is busy, wait 300s
20240221 07:43:14[INFO] gpu-0 is busy, wait 300s
20240221 07:48:22[INFO] gpu-0 is busy, wait 300s
20240221 07:54:11[INFO] gpu-0 is busy, wait 300s
20240221 07:59:19[INFO] gpu-0 is busy, wait 300s
20240221 08:04:25[INFO] gpu-0 is busy, wait 300s
20240221 08:09:43[INFO] gpu-0 is busy, wait 300s
20240221 08:14:49[INFO] gpu-0 is busy, wait 300s
20240221 08:20:05[INFO] gpu-0 is busy, wait 300s
20240221 08:25:11[INFO] gpu-0 is busy, wait 300s
20240221 08:30:19[INFO] gpu-0 is busy, wait 300s
20240221 08:36:38[INFO] gpu-0 is busy, wait 300s
20240221 08:41:43[INFO] gpu-0 is busy, wait 300s
20240221 08:47:08[INFO] gpu-0 is busy, wait 300s
20240221 08:52:37[INFO] gpu-0 is busy, wait 300s
20240221 08:57:53[INFO] gpu-0 is busy, wait 300s
20240221 09:02:58[INFO] gpu-0 is busy, wait 300s
20240221 09:08:29[INFO] gpu-0 is busy, wait 300s
20240221 09:13:37[INFO] gpu-0 is busy, wait 300s
20240221 09:19:43[INFO] gpu-0 is busy, wait 300s
20240221 09:24:51[INFO] gpu-0 is busy, wait 300s
20240221 09:30:00[INFO] gpu-0 is busy, wait 300s
20240221 09:35:05[INFO] gpu-0 is busy, wait 300s
20240221 09:40:18[INFO] gpu-0 is busy, wait 300s
20240221 09:45:25[INFO] gpu-0 is busy, wait 300s
20240221 09:50:45[INFO] gpu-0 is busy, wait 300s
20240221 09:55:52[INFO] gpu-0 is busy, wait 300s
20240221 10:00:58[INFO] gpu-0 is busy, wait 300s
20240221 10:06:27[INFO] gpu-0 is busy, wait 300s
20240221 10:11:39[INFO] gpu-0 is busy, wait 300s
20240221 10:17:41[INFO] gpu-0 is busy, wait 300s
20240221 10:18:45[INFO] gpu-1 is busy, wait 300s
20240221 10:22:49[INFO] gpu-0 is busy, wait 300s
20240221 10:23:46[INFO] gpu-1 is busy, wait 300s
20240221 10:27:50[INFO] gpu-0 is busy, wait 300s
20240221 10:33:17[INFO] gpu-0 is busy, wait 300s
20240221 10:37:07[INFO] gpu-1 is busy, wait 300s
20240221 10:38:17[INFO] gpu-0 is busy, wait 300s
Process Process-1:10937:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 546.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 436.94 MiB is free. Process 1735428 has 11.60 GiB memory in use. Process 1789927 has 8.14 GiB memory in use. Process 1797412 has 3.47 GiB memory in use. Of the allocated memory 2.61 GiB is allocated by PyTorch, and 422.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:10940:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 566.94 MiB is free. Process 1735428 has 11.60 GiB memory in use. Process 1789927 has 8.14 GiB memory in use. Process 1798029 has 3.34 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240221 10:43:48[INFO] gpu-0 is busy, wait 300s
20240221 10:49:02[INFO] gpu-0 is busy, wait 300s
20240221 10:54:17[INFO] gpu-0 is busy, wait 300s
20240221 10:59:18[INFO] gpu-0 is busy, wait 300s
20240221 11:04:29[INFO] gpu-0 is busy, wait 300s
20240221 11:10:28[INFO] gpu-0 is busy, wait 300s
20240221 11:15:39[INFO] gpu-0 is busy, wait 300s
20240221 11:20:46[INFO] gpu-0 is busy, wait 300s
20240221 11:25:53[INFO] gpu-0 is busy, wait 300s
20240221 11:31:00[INFO] gpu-0 is busy, wait 300s
20240221 11:36:06[INFO] gpu-0 is busy, wait 300s
20240221 11:41:31[INFO] gpu-0 is busy, wait 300s
20240221 11:46:36[INFO] gpu-0 is busy, wait 300s
20240221 11:52:03[INFO] gpu-0 is busy, wait 300s
20240221 11:57:17[INFO] gpu-0 is busy, wait 300s
20240221 12:02:28[INFO] gpu-0 is busy, wait 300s
20240221 12:04:53[INFO] gpu-1 is busy, wait 300s
20240221 12:07:35[INFO] gpu-0 is busy, wait 300s
20240221 12:09:54[INFO] gpu-1 is busy, wait 300s
20240221 12:12:36[INFO] gpu-0 is busy, wait 300s
20240221 12:14:55[INFO] gpu-1 is busy, wait 300s
20240221 12:17:46[INFO] gpu-0 is busy, wait 300s
20240221 12:19:56[INFO] gpu-1 is busy, wait 300s
20240221 12:23:01[INFO] gpu-0 is busy, wait 300s
20240221 12:24:57[INFO] gpu-1 is busy, wait 300s
20240221 12:28:08[INFO] gpu-0 is busy, wait 300s
20240221 12:29:58[INFO] gpu-1 is busy, wait 300s
20240221 12:33:29[INFO] gpu-0 is busy, wait 300s
20240221 12:34:59[INFO] gpu-1 is busy, wait 300s
20240221 12:39:59[INFO] gpu-1 is busy, wait 300s
20240221 12:40:08[INFO] gpu-0 is busy, wait 300s
20240221 12:45:00[INFO] gpu-1 is busy, wait 300s
20240221 12:45:29[INFO] gpu-0 is busy, wait 300s
20240221 12:50:02[INFO] gpu-1 is busy, wait 300s
20240221 12:50:55[INFO] gpu-0 is busy, wait 300s
20240221 12:55:02[INFO] gpu-1 is busy, wait 300s
20240221 12:56:13[INFO] gpu-0 is busy, wait 300s
20240221 13:00:03[INFO] gpu-1 is busy, wait 300s
20240221 13:01:21[INFO] gpu-0 is busy, wait 300s
20240221 13:05:04[INFO] gpu-1 is busy, wait 300s
20240221 13:06:33[INFO] gpu-0 is busy, wait 300s
20240221 13:10:04[INFO] gpu-1 is busy, wait 300s
20240221 13:11:38[INFO] gpu-0 is busy, wait 300s
20240221 13:15:05[INFO] gpu-1 is busy, wait 300s
20240221 13:16:48[INFO] gpu-0 is busy, wait 300s
Process Process-2:20068:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 584.00 MiB. GPU 1 has a total capacty of 23.65 GiB of which 140.50 MiB is free. Process 1910159 has 19.25 GiB memory in use. Process 1984111 has 4.25 GiB memory in use. Of the allocated memory 3.14 GiB is allocated by PyTorch, and 679.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240221 13:20:08[INFO] gpu-1 is busy, wait 300s
20240221 13:21:54[INFO] gpu-0 is busy, wait 300s
20240221 13:25:09[INFO] gpu-1 is busy, wait 300s
20240221 13:27:06[INFO] gpu-0 is busy, wait 300s
20240221 13:30:09[INFO] gpu-1 is busy, wait 300s
20240221 13:32:27[INFO] gpu-0 is busy, wait 300s
20240221 13:35:10[INFO] gpu-1 is busy, wait 300s
20240221 13:37:41[INFO] gpu-0 is busy, wait 300s
20240221 13:40:11[INFO] gpu-1 is busy, wait 300s
20240221 13:42:56[INFO] gpu-0 is busy, wait 300s
20240221 13:45:12[INFO] gpu-1 is busy, wait 300s
20240221 13:48:02[INFO] gpu-0 is busy, wait 300s
20240221 13:50:14[INFO] gpu-1 is busy, wait 300s
20240221 13:53:09[INFO] gpu-0 is busy, wait 300s
20240221 13:55:14[INFO] gpu-1 is busy, wait 300s
20240221 13:56:50[INFO] gpu-3 is busy, wait 300s
20240221 13:58:23[INFO] gpu-0 is busy, wait 300s
20240221 14:00:15[INFO] gpu-1 is busy, wait 300s
20240221 14:01:52[INFO] gpu-3 is busy, wait 300s
20240221 14:03:35[INFO] gpu-0 is busy, wait 300s
20240221 14:05:23[INFO] gpu-1 is busy, wait 300s
20240221 14:08:43[INFO] gpu-0 is busy, wait 300s
20240221 14:10:23[INFO] gpu-1 is busy, wait 300s
20240221 14:13:57[INFO] gpu-0 is busy, wait 300s
20240221 14:14:17[INFO] gpu-3 is busy, wait 300s
20240221 14:15:24[INFO] gpu-1 is busy, wait 300s
20240221 14:19:18[INFO] gpu-0 is busy, wait 300s
20240221 14:20:25[INFO] gpu-1 is busy, wait 300s
20240221 14:24:34[INFO] gpu-0 is busy, wait 300s
20240221 14:25:25[INFO] gpu-1 is busy, wait 300s
20240221 14:29:42[INFO] gpu-0 is busy, wait 300s
20240221 14:30:26[INFO] gpu-1 is busy, wait 300s
20240221 14:41:38[INFO] gpu-3 is busy, wait 300s
20240221 14:42:50[INFO] gpu-0 is busy, wait 300s
20240221 14:49:04[INFO] gpu-0 is busy, wait 300s
20240221 15:35:49[INFO] gpu-3 is busy, wait 300s
20240221 16:06:53[INFO] gpu-0 is busy, wait 300s
20240221 16:11:54[INFO] gpu-0 is busy, wait 300s
20240221 16:12:34[INFO] gpu-3 is busy, wait 300s
20240221 16:29:25[INFO] gpu-0 is busy, wait 300s
20240221 16:34:26[INFO] gpu-0 is busy, wait 300s
20240221 16:46:58[INFO] gpu-0 is busy, wait 300s
20240221 16:51:59[INFO] gpu-0 is busy, wait 300s
20240221 17:02:38[INFO] gpu-0 is busy, wait 300s
20240221 17:07:45[INFO] gpu-0 is busy, wait 300s
20240221 17:13:59[INFO] gpu-0 is busy, wait 300s
20240221 17:16:44[INFO] gpu-3 is busy, wait 300s
20240221 17:19:04[INFO] gpu-0 is busy, wait 300s
20240221 17:22:03[INFO] gpu-3 is busy, wait 300s
20240221 17:24:05[INFO] gpu-0 is busy, wait 300s
20240221 17:27:04[INFO] gpu-3 is busy, wait 300s
20240221 17:27:29[INFO] gpu-2 is busy, wait 300s
20240221 17:32:05[INFO] gpu-3 is busy, wait 300s
20240221 17:32:59[INFO] gpu-2 is busy, wait 300s
20240221 17:33:31[INFO] gpu-0 is busy, wait 300s
20240221 17:38:19[INFO] gpu-2 is busy, wait 300s
20240221 17:38:32[INFO] gpu-0 is busy, wait 300s
20240221 17:40:44[INFO] gpu-3 is busy, wait 300s
20240221 17:43:25[INFO] gpu-2 is busy, wait 300s
20240221 17:45:44[INFO] gpu-3 is busy, wait 300s
20240221 17:48:32[INFO] gpu-2 is busy, wait 300s
20240221 17:53:41[INFO] gpu-2 is busy, wait 300s
20240221 17:59:32[INFO] gpu-2 is busy, wait 300s
20240221 18:02:17[INFO] gpu-6 is busy, wait 300s
20240221 18:03:44[INFO] gpu-3 is busy, wait 300s
20240221 18:07:23[INFO] gpu-6 is busy, wait 300s
20240221 18:08:45[INFO] gpu-3 is busy, wait 300s
20240221 18:12:31[INFO] gpu-6 is busy, wait 300s
20240221 18:13:52[INFO] gpu-0 is busy, wait 300s
20240221 18:14:00[INFO] gpu-3 is busy, wait 300s
20240221 18:17:43[INFO] gpu-6 is busy, wait 300s
20240221 18:18:52[INFO] gpu-0 is busy, wait 300s
20240221 18:22:58[INFO] gpu-6 is busy, wait 300s
20240221 18:28:24[INFO] gpu-6 is busy, wait 300s
20240221 18:33:29[INFO] gpu-6 is busy, wait 300s
20240221 18:38:53[INFO] gpu-6 is busy, wait 300s
20240221 18:42:19[INFO] gpu-0 is busy, wait 300s
20240221 18:44:08[INFO] gpu-6 is busy, wait 300s
20240221 18:44:21[INFO] gpu-3 is busy, wait 300s
20240221 18:47:20[INFO] gpu-0 is busy, wait 300s
20240221 18:49:14[INFO] gpu-6 is busy, wait 300s
20240221 18:49:22[INFO] gpu-3 is busy, wait 300s
20240221 18:52:27[INFO] gpu-0 is busy, wait 300s
20240221 18:54:20[INFO] gpu-6 is busy, wait 300s
20240221 18:57:47[INFO] gpu-0 is busy, wait 300s
20240221 18:59:38[INFO] gpu-6 is busy, wait 300s
20240221 19:03:04[INFO] gpu-0 is busy, wait 300s
20240221 19:04:23[INFO] gpu-3 is busy, wait 300s
20240221 19:04:54[INFO] gpu-6 is busy, wait 300s
20240221 19:08:10[INFO] gpu-0 is busy, wait 300s
20240221 19:10:13[INFO] gpu-6 is busy, wait 300s
20240221 19:15:22[INFO] gpu-6 is busy, wait 300s
20240221 19:20:28[INFO] gpu-6 is busy, wait 300s
20240221 19:21:02[INFO] gpu-0 is busy, wait 300s
20240221 19:25:33[INFO] gpu-6 is busy, wait 300s
20240221 19:26:19[INFO] gpu-0 is busy, wait 300s
20240221 19:30:41[INFO] gpu-6 is busy, wait 300s
20240221 19:31:24[INFO] gpu-0 is busy, wait 300s
20240221 19:36:04[INFO] gpu-6 is busy, wait 300s
20240221 19:36:31[INFO] gpu-0 is busy, wait 300s
20240221 19:41:10[INFO] gpu-6 is busy, wait 300s
20240221 19:46:27[INFO] gpu-6 is busy, wait 300s
20240221 19:51:42[INFO] gpu-6 is busy, wait 300s
20240221 19:55:11[INFO] gpu-3 is busy, wait 300s
20240221 19:57:05[INFO] gpu-6 is busy, wait 300s
20240221 20:00:12[INFO] gpu-3 is busy, wait 300s
20240221 20:02:32[INFO] gpu-6 is busy, wait 300s
20240221 20:07:37[INFO] gpu-6 is busy, wait 300s
20240221 20:12:45[INFO] gpu-6 is busy, wait 300s
20240221 20:17:53[INFO] gpu-6 is busy, wait 300s
20240221 20:23:16[INFO] gpu-6 is busy, wait 300s
20240221 20:24:10[INFO] gpu-3 is busy, wait 300s
20240221 20:28:31[INFO] gpu-6 is busy, wait 300s
20240221 20:33:38[INFO] gpu-6 is busy, wait 300s
20240221 20:33:50[INFO] gpu-3 is busy, wait 300s
20240221 20:38:46[INFO] gpu-6 is busy, wait 300s
20240221 20:43:52[INFO] gpu-6 is busy, wait 300s
20240221 20:48:24[INFO] gpu-3 is busy, wait 300s
20240221 20:48:58[INFO] gpu-6 is busy, wait 300s
20240221 20:54:03[INFO] gpu-6 is busy, wait 300s
20240221 20:57:49[INFO] gpu-3 is busy, wait 300s
20240221 20:59:33[INFO] gpu-6 is busy, wait 300s
20240221 21:03:50[INFO] gpu-3 is busy, wait 300s
20240221 21:04:40[INFO] gpu-6 is busy, wait 300s
20240221 21:09:53[INFO] gpu-6 is busy, wait 300s
20240221 21:15:16[INFO] gpu-6 is busy, wait 300s
20240221 21:20:48[INFO] gpu-6 is busy, wait 300s
20240221 21:26:02[INFO] gpu-6 is busy, wait 300s
20240221 21:29:01[INFO] gpu-5 is busy, wait 300s
20240221 21:29:04[INFO] gpu-4 is busy, wait 300s
20240221 21:29:35[INFO] gpu-0 is busy, wait 300s
20240221 21:30:16[INFO] gpu-1 is busy, wait 300s
20240221 21:31:18[INFO] gpu-6 is busy, wait 300s
20240221 21:36:24[INFO] gpu-6 is busy, wait 300s
20240221 21:41:29[INFO] gpu-6 is busy, wait 300s
20240221 21:46:35[INFO] gpu-6 is busy, wait 300s
20240221 21:51:43[INFO] gpu-6 is busy, wait 300s
20240221 21:56:44[INFO] gpu-6 is busy, wait 300s
20240221 22:01:45[INFO] gpu-6 is busy, wait 300s
20240221 22:06:46[INFO] gpu-6 is busy, wait 300s
20240221 22:11:53[INFO] gpu-6 is busy, wait 300s
20240221 22:17:37[INFO] gpu-6 is busy, wait 300s
20240221 22:22:45[INFO] gpu-6 is busy, wait 300s
20240221 22:28:29[INFO] gpu-6 is busy, wait 300s
20240221 22:33:52[INFO] gpu-6 is busy, wait 300s
20240221 22:39:37[INFO] gpu-6 is busy, wait 300s
20240221 22:44:44[INFO] gpu-6 is busy, wait 300s
20240221 22:44:46[INFO] gpu-2 is busy, wait 300s
20240221 22:45:12[INFO] gpu-3 is busy, wait 300s
20240221 22:45:21[INFO] gpu-7 is busy, wait 300s
20240221 22:46:14[INFO] gpu-1 is busy, wait 300s
20240221 22:46:16[INFO] gpu-4 is busy, wait 300s
20240221 22:46:17[INFO] gpu-0 is busy, wait 300s
20240221 22:48:30[INFO] gpu-5 is busy, wait 300s
20240221 22:49:45[INFO] gpu-6 is busy, wait 300s
20240221 22:49:46[INFO] gpu-2 is busy, wait 300s
20240221 22:50:12[INFO] gpu-3 is busy, wait 300s
20240221 22:50:28[INFO] gpu-7 is busy, wait 300s
20240221 22:51:15[INFO] gpu-1 is busy, wait 300s
20240221 22:51:16[INFO] gpu-4 is busy, wait 300s
20240221 22:51:18[INFO] gpu-0 is busy, wait 300s
20240221 22:53:31[INFO] gpu-5 is busy, wait 300s
20240221 22:54:45[INFO] gpu-6 is busy, wait 300s
20240221 22:54:47[INFO] gpu-2 is busy, wait 300s
20240221 22:55:13[INFO] gpu-3 is busy, wait 300s
20240221 22:55:29[INFO] gpu-7 is busy, wait 300s
20240221 22:56:16[INFO] gpu-1 is busy, wait 300s
20240221 22:56:17[INFO] gpu-4 is busy, wait 300s
20240221 22:56:18[INFO] gpu-0 is busy, wait 300s
20240221 22:58:32[INFO] gpu-5 is busy, wait 300s
20240221 22:59:46[INFO] gpu-6 is busy, wait 300s
20240221 22:59:47[INFO] gpu-2 is busy, wait 300s
20240221 23:00:13[INFO] gpu-3 is busy, wait 300s
20240221 23:00:29[INFO] gpu-7 is busy, wait 300s
20240221 23:02:49[INFO] gpu-1 is busy, wait 300s
20240221 23:02:50[INFO] gpu-4 is busy, wait 300s
20240221 23:02:50[INFO] gpu-0 is busy, wait 300s
20240221 23:05:05[INFO] gpu-6 is busy, wait 300s
20240221 23:06:37[INFO] gpu-7 is busy, wait 300s
20240221 23:06:38[INFO] gpu-2 is busy, wait 300s
20240221 23:06:41[INFO] gpu-3 is busy, wait 300s
20240221 23:07:54[INFO] gpu-4 is busy, wait 300s
20240221 23:07:55[INFO] gpu-1 is busy, wait 300s
20240221 23:07:57[INFO] gpu-0 is busy, wait 300s
20240221 23:10:32[INFO] gpu-6 is busy, wait 300s
20240221 23:13:07[INFO] gpu-4 is busy, wait 300s
20240221 23:13:12[INFO] gpu-2 is busy, wait 300s
20240221 23:13:16[INFO] gpu-1 is busy, wait 300s
20240221 23:13:19[INFO] gpu-0 is busy, wait 300s
20240221 23:13:22[INFO] gpu-3 is busy, wait 300s
20240221 23:13:52[INFO] gpu-7 is busy, wait 300s
20240221 23:15:39[INFO] gpu-6 is busy, wait 300s
20240221 23:18:21[INFO] gpu-1 is busy, wait 300s
20240221 23:18:27[INFO] gpu-3 is busy, wait 300s
20240221 23:18:29[INFO] gpu-2 is busy, wait 300s
20240221 23:18:30[INFO] gpu-0 is busy, wait 300s
20240221 23:18:34[INFO] gpu-4 is busy, wait 300s
20240221 23:19:00[INFO] gpu-7 is busy, wait 300s
20240221 23:20:49[INFO] gpu-6 is busy, wait 300s
20240221 23:26:02[INFO] gpu-6 is busy, wait 300s
20240221 23:31:14[INFO] gpu-6 is busy, wait 300s
20240221 23:33:08[INFO] gpu-0 is busy, wait 300s
20240221 23:36:21[INFO] gpu-6 is busy, wait 300s
20240221 23:41:36[INFO] gpu-6 is busy, wait 300s
20240221 23:41:46[INFO] gpu-0 is busy, wait 300s
20240221 23:43:15[INFO] gpu-2 is busy, wait 300s
20240221 23:43:17[INFO] gpu-4 is busy, wait 300s
20240221 23:43:17[INFO] gpu-1 is busy, wait 300s
20240221 23:43:22[INFO] gpu-5 is busy, wait 300s
20240221 23:43:25[INFO] gpu-7 is busy, wait 300s
20240221 23:46:53[INFO] gpu-6 is busy, wait 300s
20240221 23:47:00[INFO] gpu-0 is busy, wait 300s
20240221 23:48:30[INFO] gpu-4 is busy, wait 300s
20240221 23:48:33[INFO] gpu-1 is busy, wait 300s
20240221 23:48:37[INFO] gpu-2 is busy, wait 300s
20240221 23:48:57[INFO] gpu-7 is busy, wait 300s
20240221 23:48:59[INFO] gpu-5 is busy, wait 300s
20240221 23:51:54[INFO] gpu-6 is busy, wait 300s
20240221 23:52:13[INFO] gpu-0 is busy, wait 300s
20240221 23:53:20[INFO] gpu-3 is busy, wait 300s
20240221 23:53:30[INFO] gpu-4 is busy, wait 300s
20240221 23:53:38[INFO] gpu-2 is busy, wait 300s
20240221 23:53:41[INFO] gpu-1 is busy, wait 300s
20240221 23:53:57[INFO] gpu-7 is busy, wait 300s
20240221 23:54:00[INFO] gpu-5 is busy, wait 300s
20240221 23:57:06[INFO] gpu-6 is busy, wait 300s
20240221 23:57:14[INFO] gpu-0 is busy, wait 300s
20240222 00:02:08[INFO] gpu-6 is busy, wait 300s
20240222 00:07:16[INFO] gpu-6 is busy, wait 300s
20240222 00:07:42[INFO] gpu-0 is busy, wait 300s
20240222 00:12:23[INFO] gpu-6 is busy, wait 300s
20240222 00:12:55[INFO] gpu-0 is busy, wait 300s
20240222 00:17:24[INFO] gpu-6 is busy, wait 300s
20240222 00:22:31[INFO] gpu-6 is busy, wait 300s
20240222 00:27:53[INFO] gpu-6 is busy, wait 300s
20240222 00:33:10[INFO] gpu-6 is busy, wait 300s
20240222 00:38:11[INFO] gpu-6 is busy, wait 300s
20240222 00:43:12[INFO] gpu-6 is busy, wait 300s
20240222 00:48:17[INFO] gpu-6 is busy, wait 300s
20240222 00:53:22[INFO] gpu-6 is busy, wait 300s
20240222 00:58:23[INFO] gpu-6 is busy, wait 300s
20240222 01:03:24[INFO] gpu-6 is busy, wait 300s
20240222 01:08:24[INFO] gpu-6 is busy, wait 300s
20240222 01:13:41[INFO] gpu-6 is busy, wait 300s
20240222 01:18:49[INFO] gpu-6 is busy, wait 300s
20240222 01:24:00[INFO] gpu-6 is busy, wait 300s
20240222 01:29:59[INFO] gpu-6 is busy, wait 300s
20240222 01:35:24[INFO] gpu-6 is busy, wait 300s
20240222 01:40:32[INFO] gpu-6 is busy, wait 300s
20240222 01:45:49[INFO] gpu-6 is busy, wait 300s
20240222 01:51:27[INFO] gpu-6 is busy, wait 300s
20240222 01:56:40[INFO] gpu-6 is busy, wait 300s
20240222 02:01:47[INFO] gpu-6 is busy, wait 300s
20240222 02:06:55[INFO] gpu-6 is busy, wait 300s
20240222 02:12:08[INFO] gpu-6 is busy, wait 300s
20240222 02:17:27[INFO] gpu-6 is busy, wait 300s
20240222 02:23:21[INFO] gpu-6 is busy, wait 300s
20240222 02:28:34[INFO] gpu-6 is busy, wait 300s
20240222 02:34:04[INFO] gpu-6 is busy, wait 300s
20240222 02:39:11[INFO] gpu-6 is busy, wait 300s
20240222 02:44:18[INFO] gpu-6 is busy, wait 300s
20240222 02:49:39[INFO] gpu-6 is busy, wait 300s
20240222 02:54:46[INFO] gpu-6 is busy, wait 300s
20240222 02:59:58[INFO] gpu-6 is busy, wait 300s
20240222 03:05:05[INFO] gpu-6 is busy, wait 300s
20240222 03:10:45[INFO] gpu-6 is busy, wait 300s
20240222 03:15:50[INFO] gpu-6 is busy, wait 300s
20240222 03:21:00[INFO] gpu-6 is busy, wait 300s
20240222 03:27:23[INFO] gpu-6 is busy, wait 300s
20240222 03:32:33[INFO] gpu-6 is busy, wait 300s
20240222 03:38:10[INFO] gpu-6 is busy, wait 300s
20240222 03:43:37[INFO] gpu-6 is busy, wait 300s
20240222 03:49:35[INFO] gpu-6 is busy, wait 300s
20240222 03:54:48[INFO] gpu-6 is busy, wait 300s
20240222 04:00:50[INFO] gpu-6 is busy, wait 300s
20240222 04:05:58[INFO] gpu-6 is busy, wait 300s
20240222 04:11:37[INFO] gpu-6 is busy, wait 300s
20240222 04:17:12[INFO] gpu-6 is busy, wait 300s
20240222 04:23:13[INFO] gpu-6 is busy, wait 300s
20240222 04:28:48[INFO] gpu-6 is busy, wait 300s
20240222 04:34:04[INFO] gpu-6 is busy, wait 300s
20240222 04:39:16[INFO] gpu-6 is busy, wait 300s
20240222 04:44:22[INFO] gpu-6 is busy, wait 300s
20240222 04:49:29[INFO] gpu-6 is busy, wait 300s
20240222 04:54:42[INFO] gpu-6 is busy, wait 300s
20240222 05:00:23[INFO] gpu-6 is busy, wait 300s
20240222 05:05:30[INFO] gpu-6 is busy, wait 300s
20240222 05:10:52[INFO] gpu-6 is busy, wait 300s
20240222 05:16:05[INFO] gpu-6 is busy, wait 300s
20240222 05:21:36[INFO] gpu-6 is busy, wait 300s
20240222 05:26:43[INFO] gpu-6 is busy, wait 300s
20240222 05:32:03[INFO] gpu-6 is busy, wait 300s
20240222 05:37:14[INFO] gpu-6 is busy, wait 300s
20240222 05:42:36[INFO] gpu-6 is busy, wait 300s
20240222 05:47:44[INFO] gpu-6 is busy, wait 300s
20240222 05:52:50[INFO] gpu-6 is busy, wait 300s
20240222 05:58:31[INFO] gpu-6 is busy, wait 300s
20240222 06:03:38[INFO] gpu-6 is busy, wait 300s
20240222 06:09:16[INFO] gpu-6 is busy, wait 300s
20240222 06:15:24[INFO] gpu-6 is busy, wait 300s
20240222 06:21:00[INFO] gpu-6 is busy, wait 300s
20240222 06:26:29[INFO] gpu-6 is busy, wait 300s
20240222 06:31:47[INFO] gpu-6 is busy, wait 300s
20240222 06:36:57[INFO] gpu-6 is busy, wait 300s
20240222 06:42:04[INFO] gpu-6 is busy, wait 300s
20240222 06:47:11[INFO] gpu-6 is busy, wait 300s
20240222 06:52:33[INFO] gpu-6 is busy, wait 300s
20240222 06:57:59[INFO] gpu-6 is busy, wait 300s
20240222 07:03:17[INFO] gpu-6 is busy, wait 300s
20240222 07:08:30[INFO] gpu-6 is busy, wait 300s
20240222 07:13:39[INFO] gpu-6 is busy, wait 300s
20240222 07:19:29[INFO] gpu-6 is busy, wait 300s
20240222 07:24:51[INFO] gpu-6 is busy, wait 300s
20240222 07:30:00[INFO] gpu-6 is busy, wait 300s
20240222 07:35:11[INFO] gpu-6 is busy, wait 300s
20240222 07:40:19[INFO] gpu-6 is busy, wait 300s
20240222 07:45:34[INFO] gpu-6 is busy, wait 300s
20240222 07:50:54[INFO] gpu-6 is busy, wait 300s
20240222 07:56:03[INFO] gpu-6 is busy, wait 300s
20240222 08:01:08[INFO] gpu-6 is busy, wait 300s
20240222 08:06:22[INFO] gpu-6 is busy, wait 300s
20240222 08:11:52[INFO] gpu-6 is busy, wait 300s
20240222 08:17:12[INFO] gpu-6 is busy, wait 300s
20240222 08:22:27[INFO] gpu-6 is busy, wait 300s
20240222 08:27:47[INFO] gpu-6 is busy, wait 300s
20240222 08:33:09[INFO] gpu-6 is busy, wait 300s
20240222 08:38:22[INFO] gpu-6 is busy, wait 300s
20240222 08:43:50[INFO] gpu-6 is busy, wait 300s
20240222 08:49:05[INFO] gpu-6 is busy, wait 300s
20240222 08:54:14[INFO] gpu-6 is busy, wait 300s
20240222 08:59:15[INFO] gpu-6 is busy, wait 300s
20240222 09:05:54[INFO] gpu-6 is busy, wait 300s
20240222 09:10:55[INFO] gpu-6 is busy, wait 300s
20240222 09:16:10[INFO] gpu-6 is busy, wait 300s
20240222 09:21:19[INFO] gpu-6 is busy, wait 300s
20240222 09:27:09[INFO] gpu-6 is busy, wait 300s
20240222 09:32:16[INFO] gpu-6 is busy, wait 300s
20240222 09:37:17[INFO] gpu-6 is busy, wait 300s
20240222 09:42:33[INFO] gpu-6 is busy, wait 300s
20240222 09:47:34[INFO] gpu-6 is busy, wait 300s
20240222 09:52:40[INFO] gpu-6 is busy, wait 300s
20240222 09:57:40[INFO] gpu-6 is busy, wait 300s
20240222 09:58:09[INFO] gpu-0 is busy, wait 300s
20240222 10:02:51[INFO] gpu-6 is busy, wait 300s
20240222 10:08:01[INFO] gpu-6 is busy, wait 300s
20240222 10:13:09[INFO] gpu-6 is busy, wait 300s
20240222 10:18:36[INFO] gpu-6 is busy, wait 300s
20240222 10:23:43[INFO] gpu-6 is busy, wait 300s
20240222 10:29:19[INFO] gpu-6 is busy, wait 300s
20240222 10:34:27[INFO] gpu-6 is busy, wait 300s
20240222 10:39:39[INFO] gpu-6 is busy, wait 300s
20240222 10:45:37[INFO] gpu-6 is busy, wait 300s
20240222 10:50:51[INFO] gpu-6 is busy, wait 300s
20240222 10:54:51[INFO] gpu-1 is busy, wait 300s
20240222 10:55:30[INFO] gpu-0 is busy, wait 300s
20240222 10:56:05[INFO] gpu-6 is busy, wait 300s
Process Process-2:30715:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

20240222 10:59:55[INFO] gpu-1 is busy, wait 300s
20240222 11:00:35[INFO] gpu-0 is busy, wait 300s
20240222 11:01:21[INFO] gpu-6 is busy, wait 300s
20240222 11:04:07[INFO] gpu-3 is busy, wait 300s
20240222 11:04:56[INFO] gpu-1 is busy, wait 300s
20240222 11:05:43[INFO] gpu-0 is busy, wait 300s
20240222 11:06:41[INFO] gpu-6 is busy, wait 300s
20240222 11:09:56[INFO] gpu-1 is busy, wait 300s
20240222 11:10:56[INFO] gpu-0 is busy, wait 300s
20240222 11:11:51[INFO] gpu-6 is busy, wait 300s
20240222 11:12:42[INFO] gpu-3 is busy, wait 300s
20240222 11:14:57[INFO] gpu-1 is busy, wait 300s
20240222 11:16:04[INFO] gpu-0 is busy, wait 300s
20240222 11:16:59[INFO] gpu-6 is busy, wait 300s
20240222 11:17:43[INFO] gpu-3 is busy, wait 300s
20240222 11:19:58[INFO] gpu-1 is busy, wait 300s
20240222 11:21:09[INFO] gpu-0 is busy, wait 300s
20240222 11:22:24[INFO] gpu-6 is busy, wait 300s
20240222 11:24:58[INFO] gpu-1 is busy, wait 300s
20240222 11:26:27[INFO] gpu-0 is busy, wait 300s
20240222 11:27:43[INFO] gpu-6 is busy, wait 300s
20240222 11:29:59[INFO] gpu-1 is busy, wait 300s
20240222 11:31:32[INFO] gpu-0 is busy, wait 300s
20240222 11:33:32[INFO] gpu-6 is busy, wait 300s
20240222 11:35:00[INFO] gpu-1 is busy, wait 300s
20240222 11:36:45[INFO] gpu-0 is busy, wait 300s
20240222 11:38:57[INFO] gpu-6 is busy, wait 300s
Process Process-2:30716:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30717:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30718:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30719:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30720:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30721:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30722:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-2:30723:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

20240222 11:40:12[INFO] gpu-1 is busy, wait 300s
20240222 11:41:52[INFO] gpu-0 is busy, wait 300s
20240222 11:44:20[INFO] gpu-6 is busy, wait 300s
20240222 11:45:13[INFO] gpu-1 is busy, wait 300s
20240222 11:46:57[INFO] gpu-0 is busy, wait 300s
20240222 11:49:32[INFO] gpu-6 is busy, wait 300s
20240222 11:50:14[INFO] gpu-1 is busy, wait 300s
20240222 11:52:06[INFO] gpu-0 is busy, wait 300s
20240222 11:54:40[INFO] gpu-6 is busy, wait 300s
20240222 11:55:14[INFO] gpu-1 is busy, wait 300s
20240222 11:57:16[INFO] gpu-0 is busy, wait 300s
20240222 11:59:46[INFO] gpu-6 is busy, wait 300s
20240222 12:00:15[INFO] gpu-1 is busy, wait 300s
20240222 12:02:17[INFO] gpu-0 is busy, wait 300s
20240222 12:04:51[INFO] gpu-6 is busy, wait 300s
20240222 12:05:16[INFO] gpu-1 is busy, wait 300s
20240222 12:07:24[INFO] gpu-0 is busy, wait 300s
20240222 12:10:03[INFO] gpu-6 is busy, wait 300s
20240222 12:10:17[INFO] gpu-1 is busy, wait 300s
20240222 12:12:33[INFO] gpu-0 is busy, wait 300s
20240222 12:15:08[INFO] gpu-6 is busy, wait 300s
20240222 12:15:17[INFO] gpu-1 is busy, wait 300s
20240222 12:17:40[INFO] gpu-0 is busy, wait 300s
20240222 12:20:18[INFO] gpu-1 is busy, wait 300s
20240222 12:20:38[INFO] gpu-6 is busy, wait 300s
20240222 12:23:08[INFO] gpu-0 is busy, wait 300s
20240222 12:25:19[INFO] gpu-1 is busy, wait 300s
20240222 12:25:45[INFO] gpu-6 is busy, wait 300s
20240222 12:30:53[INFO] gpu-6 is busy, wait 300s
20240222 12:36:43[INFO] gpu-6 is busy, wait 300s
20240222 12:41:03[INFO] gpu-0 is busy, wait 300s
20240222 12:41:50[INFO] gpu-6 is busy, wait 300s
20240222 12:47:43[INFO] gpu-6 is busy, wait 300s
20240222 12:52:55[INFO] gpu-6 is busy, wait 300s
20240222 12:58:18[INFO] gpu-6 is busy, wait 300s
20240222 13:03:22[INFO] gpu-6 is busy, wait 300s
20240222 13:08:42[INFO] gpu-6 is busy, wait 300s
20240222 13:14:16[INFO] gpu-6 is busy, wait 300s
20240222 13:20:45[INFO] gpu-6 is busy, wait 300s
20240222 13:25:54[INFO] gpu-6 is busy, wait 300s
20240222 13:31:51[INFO] gpu-6 is busy, wait 300s
20240222 13:37:12[INFO] gpu-6 is busy, wait 300s
20240222 13:42:28[INFO] gpu-6 is busy, wait 300s
20240222 13:44:44[INFO] gpu-1 is busy, wait 300s
20240222 13:45:16[INFO] gpu-0 is busy, wait 300s
20240222 13:47:35[INFO] gpu-6 is busy, wait 300s
20240222 13:49:44[INFO] gpu-1 is busy, wait 300s
20240222 13:50:20[INFO] gpu-0 is busy, wait 300s
20240222 13:52:54[INFO] gpu-6 is busy, wait 300s
20240222 13:54:45[INFO] gpu-1 is busy, wait 300s
20240222 13:55:34[INFO] gpu-0 is busy, wait 300s
20240222 13:58:07[INFO] gpu-6 is busy, wait 300s
20240222 13:59:45[INFO] gpu-1 is busy, wait 300s
20240222 14:00:41[INFO] gpu-0 is busy, wait 300s
20240222 14:03:12[INFO] gpu-6 is busy, wait 300s
20240222 14:04:46[INFO] gpu-1 is busy, wait 300s
20240222 14:06:02[INFO] gpu-0 is busy, wait 300s
20240222 14:08:25[INFO] gpu-6 is busy, wait 300s
20240222 14:09:47[INFO] gpu-1 is busy, wait 300s
20240222 14:11:10[INFO] gpu-0 is busy, wait 300s
20240222 14:13:33[INFO] gpu-6 is busy, wait 300s
20240222 14:14:48[INFO] gpu-1 is busy, wait 300s
20240222 14:16:22[INFO] gpu-0 is busy, wait 300s
20240222 14:18:39[INFO] gpu-6 is busy, wait 300s
20240222 14:19:49[INFO] gpu-1 is busy, wait 300s
20240222 14:21:40[INFO] gpu-0 is busy, wait 300s
20240222 14:23:44[INFO] gpu-6 is busy, wait 300s
20240222 14:24:50[INFO] gpu-1 is busy, wait 300s
20240222 14:26:45[INFO] gpu-0 is busy, wait 300s
20240222 14:28:52[INFO] gpu-6 is busy, wait 300s
20240222 14:29:50[INFO] gpu-1 is busy, wait 300s
20240222 14:31:53[INFO] gpu-0 is busy, wait 300s
20240222 14:34:00[INFO] gpu-6 is busy, wait 300s
20240222 14:34:51[INFO] gpu-1 is busy, wait 300s
20240222 14:36:59[INFO] gpu-0 is busy, wait 300s
20240222 14:39:06[INFO] gpu-6 is busy, wait 300s
20240222 14:39:52[INFO] gpu-1 is busy, wait 300s
20240222 14:42:25[INFO] gpu-0 is busy, wait 300s
20240222 14:44:15[INFO] gpu-6 is busy, wait 300s
20240222 14:44:52[INFO] gpu-1 is busy, wait 300s
20240222 14:47:32[INFO] gpu-0 is busy, wait 300s
20240222 14:49:32[INFO] gpu-6 is busy, wait 300s
20240222 14:49:53[INFO] gpu-1 is busy, wait 300s
20240222 14:52:51[INFO] gpu-0 is busy, wait 300s
20240222 14:54:51[INFO] gpu-6 is busy, wait 300s
20240222 14:54:54[INFO] gpu-1 is busy, wait 300s
20240222 14:57:58[INFO] gpu-0 is busy, wait 300s
20240222 14:59:55[INFO] gpu-1 is busy, wait 300s
20240222 15:00:25[INFO] gpu-6 is busy, wait 300s
20240222 15:03:13[INFO] gpu-0 is busy, wait 300s
20240222 15:04:55[INFO] gpu-1 is busy, wait 300s
20240222 15:05:33[INFO] gpu-6 is busy, wait 300s
20240222 15:08:22[INFO] gpu-0 is busy, wait 300s
20240222 15:09:56[INFO] gpu-1 is busy, wait 300s
20240222 15:11:35[INFO] gpu-6 is busy, wait 300s
20240222 15:13:28[INFO] gpu-0 is busy, wait 300s
20240222 15:14:57[INFO] gpu-1 is busy, wait 300s
20240222 15:17:01[INFO] gpu-6 is busy, wait 300s
20240222 15:22:16[INFO] gpu-6 is busy, wait 300s
20240222 15:27:10[INFO] gpu-0 is busy, wait 300s
20240222 15:27:41[INFO] gpu-6 is busy, wait 300s
20240222 15:32:18[INFO] gpu-0 is busy, wait 300s
20240222 15:33:55[INFO] gpu-1 is busy, wait 300s
20240222 15:34:32[INFO] gpu-6 is busy, wait 300s
20240222 15:37:25[INFO] gpu-0 is busy, wait 300s
20240222 15:38:57[INFO] gpu-1 is busy, wait 300s
20240222 15:40:05[INFO] gpu-6 is busy, wait 300s
20240222 15:42:48[INFO] gpu-0 is busy, wait 300s
20240222 15:43:58[INFO] gpu-1 is busy, wait 300s
20240222 15:45:13[INFO] gpu-6 is busy, wait 300s
20240222 15:48:05[INFO] gpu-0 is busy, wait 300s
20240222 15:48:59[INFO] gpu-1 is busy, wait 300s
20240222 15:50:23[INFO] gpu-6 is busy, wait 300s
20240222 15:53:19[INFO] gpu-0 is busy, wait 300s
20240222 15:54:00[INFO] gpu-1 is busy, wait 300s
20240222 15:55:32[INFO] gpu-6 is busy, wait 300s
20240222 15:58:46[INFO] gpu-0 is busy, wait 300s
20240222 15:59:01[INFO] gpu-1 is busy, wait 300s
20240222 16:01:06[INFO] gpu-6 is busy, wait 300s
20240222 16:03:52[INFO] gpu-0 is busy, wait 300s
20240222 16:04:02[INFO] gpu-1 is busy, wait 300s
20240222 16:06:15[INFO] gpu-6 is busy, wait 300s
20240222 16:09:03[INFO] gpu-1 is busy, wait 300s
20240222 16:10:25[INFO] gpu-0 is busy, wait 300s
20240222 16:11:21[INFO] gpu-6 is busy, wait 300s
20240222 16:14:05[INFO] gpu-1 is busy, wait 300s
20240222 16:16:00[INFO] gpu-0 is busy, wait 300s
20240222 16:16:35[INFO] gpu-6 is busy, wait 300s
20240222 16:19:06[INFO] gpu-1 is busy, wait 300s
20240222 16:21:38[INFO] gpu-0 is busy, wait 300s
20240222 16:21:44[INFO] gpu-6 is busy, wait 300s
Process Process-2:31532:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 90, in use_gpu
    SA = SelfAttention(D, device=device)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 67, in __init__
    self.to_q = nn.Linear(dim, inner_dim, bias=False, dtype=dtype, device=device)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

20240222 16:24:10[INFO] gpu-1 is busy, wait 300s
20240222 16:25:08[INFO] gpu-7 is busy, wait 300s
20240222 16:26:00[INFO] gpu-5 is busy, wait 300s
20240222 16:26:13[INFO] gpu-4 is busy, wait 300s
20240222 16:26:42[INFO] gpu-3 is busy, wait 300s
20240222 16:26:44[INFO] gpu-6 is busy, wait 300s
20240222 16:27:05[INFO] gpu-0 is busy, wait 300s
20240222 16:27:08[INFO] gpu-2 is busy, wait 300s
20240222 16:29:10[INFO] gpu-1 is busy, wait 300s
20240222 16:30:09[INFO] gpu-7 is busy, wait 300s
20240222 16:31:00[INFO] gpu-5 is busy, wait 300s
20240222 16:31:14[INFO] gpu-4 is busy, wait 300s
20240222 16:31:42[INFO] gpu-3 is busy, wait 300s
Process Process-7:25504:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 77, in forward
    v = self.to_v(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 272.00 MiB. GPU 6 has a total capacty of 23.65 GiB of which 94.94 MiB is free. Process 2352550 has 12.08 GiB memory in use. Process 3866197 has 10.36 GiB memory in use. Process 3878897 has 1.11 GiB memory in use. Of the allocated memory 724.63 MiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240222 16:32:09[INFO] gpu-2 is busy, wait 300s
20240222 16:32:15[INFO] gpu-6 is busy, wait 300s
20240222 16:32:26[INFO] gpu-0 is busy, wait 300s
20240222 16:34:11[INFO] gpu-1 is busy, wait 300s
20240222 16:35:09[INFO] gpu-7 is busy, wait 300s
20240222 16:36:01[INFO] gpu-5 is busy, wait 300s
20240222 16:36:14[INFO] gpu-4 is busy, wait 300s
20240222 16:36:43[INFO] gpu-3 is busy, wait 300s
20240222 16:37:10[INFO] gpu-2 is busy, wait 300s
20240222 16:37:41[INFO] gpu-0 is busy, wait 300s
20240222 16:37:44[INFO] gpu-6 is busy, wait 300s
20240222 16:40:10[INFO] gpu-7 is busy, wait 300s
20240222 16:41:02[INFO] gpu-5 is busy, wait 300s
20240222 16:41:15[INFO] gpu-4 is busy, wait 300s
20240222 16:41:44[INFO] gpu-3 is busy, wait 300s
20240222 16:42:11[INFO] gpu-2 is busy, wait 300s
20240222 16:42:42[INFO] gpu-0 is busy, wait 300s
20240222 16:43:08[INFO] gpu-6 is busy, wait 300s
20240222 16:45:10[INFO] gpu-7 is busy, wait 300s
20240222 16:45:33[INFO] gpu-1 is busy, wait 300s
20240222 16:46:03[INFO] gpu-5 is busy, wait 300s
20240222 16:46:16[INFO] gpu-4 is busy, wait 300s
20240222 16:46:45[INFO] gpu-3 is busy, wait 300s
20240222 16:47:12[INFO] gpu-2 is busy, wait 300s
20240222 16:47:47[INFO] gpu-0 is busy, wait 300s
20240222 16:48:17[INFO] gpu-6 is busy, wait 300s
20240222 16:50:11[INFO] gpu-7 is busy, wait 300s
20240222 16:50:34[INFO] gpu-1 is busy, wait 300s
20240222 16:51:04[INFO] gpu-5 is busy, wait 300s
20240222 16:51:17[INFO] gpu-4 is busy, wait 300s
20240222 16:51:46[INFO] gpu-3 is busy, wait 300s
20240222 16:52:12[INFO] gpu-2 is busy, wait 300s
20240222 16:52:53[INFO] gpu-0 is busy, wait 300s
20240222 16:53:40[INFO] gpu-6 is busy, wait 300s
20240222 16:55:12[INFO] gpu-7 is busy, wait 300s
20240222 16:55:35[INFO] gpu-1 is busy, wait 300s
20240222 16:56:05[INFO] gpu-5 is busy, wait 300s
20240222 16:56:18[INFO] gpu-4 is busy, wait 300s
20240222 16:56:47[INFO] gpu-3 is busy, wait 300s
20240222 16:57:13[INFO] gpu-2 is busy, wait 300s
20240222 16:58:00[INFO] gpu-0 is busy, wait 300s
20240222 16:58:52[INFO] gpu-6 is busy, wait 300s
20240222 17:00:13[INFO] gpu-7 is busy, wait 300s
20240222 17:00:36[INFO] gpu-1 is busy, wait 300s
20240222 17:01:06[INFO] gpu-5 is busy, wait 300s
20240222 17:01:19[INFO] gpu-4 is busy, wait 300s
20240222 17:01:48[INFO] gpu-3 is busy, wait 300s
20240222 17:02:14[INFO] gpu-2 is busy, wait 300s
20240222 17:03:08[INFO] gpu-0 is busy, wait 300s
20240222 17:04:08[INFO] gpu-6 is busy, wait 300s
20240222 17:05:13[INFO] gpu-7 is busy, wait 300s
20240222 17:05:37[INFO] gpu-1 is busy, wait 300s
20240222 17:06:07[INFO] gpu-5 is busy, wait 300s
20240222 17:06:19[INFO] gpu-4 is busy, wait 300s
20240222 17:06:49[INFO] gpu-3 is busy, wait 300s
20240222 17:07:15[INFO] gpu-2 is busy, wait 300s
20240222 17:08:15[INFO] gpu-0 is busy, wait 300s
20240222 17:09:28[INFO] gpu-6 is busy, wait 300s
20240222 17:10:14[INFO] gpu-7 is busy, wait 300s
20240222 17:10:38[INFO] gpu-1 is busy, wait 300s
20240222 17:11:07[INFO] gpu-5 is busy, wait 300s
20240222 17:11:20[INFO] gpu-4 is busy, wait 300s
20240222 17:11:49[INFO] gpu-3 is busy, wait 300s
20240222 17:12:16[INFO] gpu-2 is busy, wait 300s
20240222 17:13:36[INFO] gpu-0 is busy, wait 300s
20240222 17:14:45[INFO] gpu-6 is busy, wait 300s
20240222 17:15:15[INFO] gpu-7 is busy, wait 300s
20240222 17:15:38[INFO] gpu-1 is busy, wait 300s
20240222 17:16:08[INFO] gpu-5 is busy, wait 300s
20240222 17:16:21[INFO] gpu-4 is busy, wait 300s
20240222 17:16:50[INFO] gpu-3 is busy, wait 300s
20240222 17:17:17[INFO] gpu-2 is busy, wait 300s
20240222 17:18:37[INFO] gpu-0 is busy, wait 300s
20240222 17:19:52[INFO] gpu-6 is busy, wait 300s
20240222 17:20:15[INFO] gpu-7 is busy, wait 300s
20240222 17:20:39[INFO] gpu-1 is busy, wait 300s
20240222 17:21:09[INFO] gpu-5 is busy, wait 300s
20240222 17:21:21[INFO] gpu-4 is busy, wait 300s
20240222 17:21:50[INFO] gpu-3 is busy, wait 300s
20240222 17:22:18[INFO] gpu-2 is busy, wait 300s
20240222 17:24:09[INFO] gpu-0 is busy, wait 300s
20240222 17:25:17[INFO] gpu-7 is busy, wait 300s
20240222 17:25:30[INFO] gpu-6 is busy, wait 300s
20240222 17:25:40[INFO] gpu-1 is busy, wait 300s
20240222 17:26:10[INFO] gpu-5 is busy, wait 300s
20240222 17:26:22[INFO] gpu-4 is busy, wait 300s
20240222 17:26:51[INFO] gpu-3 is busy, wait 300s
20240222 17:27:19[INFO] gpu-2 is busy, wait 300s
20240222 17:29:17[INFO] gpu-0 is busy, wait 300s
20240222 17:30:18[INFO] gpu-7 is busy, wait 300s
20240222 17:30:41[INFO] gpu-1 is busy, wait 300s
20240222 17:31:10[INFO] gpu-5 is busy, wait 300s
20240222 17:31:16[INFO] gpu-6 is busy, wait 300s
20240222 17:31:23[INFO] gpu-4 is busy, wait 300s
20240222 17:31:52[INFO] gpu-3 is busy, wait 300s
20240222 17:32:20[INFO] gpu-2 is busy, wait 300s
20240222 17:34:28[INFO] gpu-0 is busy, wait 300s
20240222 17:35:18[INFO] gpu-7 is busy, wait 300s
20240222 17:35:41[INFO] gpu-1 is busy, wait 300s
20240222 17:36:11[INFO] gpu-5 is busy, wait 300s
20240222 17:36:24[INFO] gpu-4 is busy, wait 300s
20240222 17:36:44[INFO] gpu-6 is busy, wait 300s
20240222 17:36:53[INFO] gpu-3 is busy, wait 300s
20240222 17:37:20[INFO] gpu-2 is busy, wait 300s
20240222 17:39:36[INFO] gpu-0 is busy, wait 300s
20240222 17:40:19[INFO] gpu-7 is busy, wait 300s
20240222 17:40:42[INFO] gpu-1 is busy, wait 300s
20240222 17:41:12[INFO] gpu-5 is busy, wait 300s
20240222 17:41:24[INFO] gpu-4 is busy, wait 300s
20240222 17:41:53[INFO] gpu-3 is busy, wait 300s
20240222 17:41:58[INFO] gpu-6 is busy, wait 300s
20240222 17:42:21[INFO] gpu-2 is busy, wait 300s
20240222 17:44:42[INFO] gpu-0 is busy, wait 300s
20240222 17:45:19[INFO] gpu-7 is busy, wait 300s
20240222 17:45:42[INFO] gpu-1 is busy, wait 300s
20240222 17:46:12[INFO] gpu-5 is busy, wait 300s
20240222 17:46:25[INFO] gpu-4 is busy, wait 300s
20240222 17:46:54[INFO] gpu-3 is busy, wait 300s
20240222 17:47:21[INFO] gpu-2 is busy, wait 300s
20240222 17:47:37[INFO] gpu-6 is busy, wait 300s
20240222 17:49:47[INFO] gpu-0 is busy, wait 300s
20240222 17:50:20[INFO] gpu-7 is busy, wait 300s
20240222 17:50:43[INFO] gpu-1 is busy, wait 300s
20240222 17:51:13[INFO] gpu-5 is busy, wait 300s
20240222 17:51:25[INFO] gpu-4 is busy, wait 300s
20240222 17:51:54[INFO] gpu-3 is busy, wait 300s
20240222 17:52:22[INFO] gpu-2 is busy, wait 300s
20240222 17:52:50[INFO] gpu-6 is busy, wait 300s
20240222 17:54:48[INFO] gpu-0 is busy, wait 300s
20240222 17:55:20[INFO] gpu-7 is busy, wait 300s
20240222 17:55:43[INFO] gpu-1 is busy, wait 300s
20240222 17:56:13[INFO] gpu-5 is busy, wait 300s
20240222 17:56:26[INFO] gpu-4 is busy, wait 300s
20240222 17:56:55[INFO] gpu-3 is busy, wait 300s
20240222 17:57:22[INFO] gpu-2 is busy, wait 300s
20240222 17:57:56[INFO] gpu-6 is busy, wait 300s
20240222 18:00:14[INFO] gpu-0 is busy, wait 300s
20240222 18:00:21[INFO] gpu-7 is busy, wait 300s
20240222 18:00:44[INFO] gpu-1 is busy, wait 300s
20240222 18:01:14[INFO] gpu-5 is busy, wait 300s
20240222 18:01:27[INFO] gpu-4 is busy, wait 300s
20240222 18:01:55[INFO] gpu-3 is busy, wait 300s
20240222 18:02:23[INFO] gpu-2 is busy, wait 300s
20240222 18:03:02[INFO] gpu-6 is busy, wait 300s
20240222 18:05:22[INFO] gpu-7 is busy, wait 300s
20240222 18:06:15[INFO] gpu-5 is busy, wait 300s
20240222 18:06:27[INFO] gpu-4 is busy, wait 300s
20240222 18:06:56[INFO] gpu-3 is busy, wait 300s
20240222 18:07:23[INFO] gpu-2 is busy, wait 300s
20240222 18:08:15[INFO] gpu-6 is busy, wait 300s
20240222 18:10:22[INFO] gpu-7 is busy, wait 300s
20240222 18:11:15[INFO] gpu-5 is busy, wait 300s
20240222 18:11:28[INFO] gpu-4 is busy, wait 300s
20240222 18:11:57[INFO] gpu-3 is busy, wait 300s
20240222 18:12:24[INFO] gpu-2 is busy, wait 300s
20240222 18:13:21[INFO] gpu-6 is busy, wait 300s
20240222 18:15:23[INFO] gpu-7 is busy, wait 300s
20240222 18:16:16[INFO] gpu-5 is busy, wait 300s
20240222 18:16:28[INFO] gpu-4 is busy, wait 300s
20240222 18:16:57[INFO] gpu-3 is busy, wait 300s
20240222 18:17:25[INFO] gpu-2 is busy, wait 300s
20240222 18:18:26[INFO] gpu-6 is busy, wait 300s
20240222 18:20:23[INFO] gpu-7 is busy, wait 300s
20240222 18:21:16[INFO] gpu-5 is busy, wait 300s
20240222 18:21:29[INFO] gpu-4 is busy, wait 300s
20240222 18:21:58[INFO] gpu-3 is busy, wait 300s
20240222 18:22:25[INFO] gpu-2 is busy, wait 300s
20240222 18:23:45[INFO] gpu-6 is busy, wait 300s
20240222 18:25:24[INFO] gpu-7 is busy, wait 300s
20240222 18:26:17[INFO] gpu-5 is busy, wait 300s
20240222 18:26:30[INFO] gpu-4 is busy, wait 300s
20240222 18:26:58[INFO] gpu-3 is busy, wait 300s
20240222 18:27:26[INFO] gpu-2 is busy, wait 300s
20240222 18:28:53[INFO] gpu-6 is busy, wait 300s
20240222 18:30:24[INFO] gpu-7 is busy, wait 300s
20240222 18:31:18[INFO] gpu-5 is busy, wait 300s
20240222 18:31:30[INFO] gpu-4 is busy, wait 300s
20240222 18:31:59[INFO] gpu-3 is busy, wait 300s
20240222 18:32:26[INFO] gpu-2 is busy, wait 300s
20240222 18:33:59[INFO] gpu-6 is busy, wait 300s
20240222 18:35:25[INFO] gpu-7 is busy, wait 300s
20240222 18:36:19[INFO] gpu-5 is busy, wait 300s
20240222 18:36:31[INFO] gpu-4 is busy, wait 300s
20240222 18:37:00[INFO] gpu-3 is busy, wait 300s
20240222 18:37:27[INFO] gpu-2 is busy, wait 300s
20240222 18:39:13[INFO] gpu-6 is busy, wait 300s
20240222 18:40:26[INFO] gpu-7 is busy, wait 300s
20240222 18:41:19[INFO] gpu-5 is busy, wait 300s
20240222 18:41:31[INFO] gpu-4 is busy, wait 300s
20240222 18:42:01[INFO] gpu-3 is busy, wait 300s
20240222 18:42:28[INFO] gpu-2 is busy, wait 300s
20240222 18:44:18[INFO] gpu-6 is busy, wait 300s
20240222 18:45:26[INFO] gpu-7 is busy, wait 300s
20240222 18:46:20[INFO] gpu-5 is busy, wait 300s
20240222 18:46:32[INFO] gpu-4 is busy, wait 300s
20240222 18:47:02[INFO] gpu-3 is busy, wait 300s
20240222 18:47:29[INFO] gpu-2 is busy, wait 300s
20240222 18:49:44[INFO] gpu-6 is busy, wait 300s
20240222 18:50:27[INFO] gpu-7 is busy, wait 300s
20240222 18:51:21[INFO] gpu-5 is busy, wait 300s
20240222 18:51:33[INFO] gpu-4 is busy, wait 300s
20240222 18:52:02[INFO] gpu-3 is busy, wait 300s
20240222 18:52:29[INFO] gpu-2 is busy, wait 300s
20240222 18:55:05[INFO] gpu-6 is busy, wait 300s
20240222 18:55:27[INFO] gpu-7 is busy, wait 300s
20240222 18:56:21[INFO] gpu-5 is busy, wait 300s
20240222 18:56:34[INFO] gpu-4 is busy, wait 300s
20240222 18:57:03[INFO] gpu-3 is busy, wait 300s
20240222 18:57:30[INFO] gpu-2 is busy, wait 300s
20240222 19:00:10[INFO] gpu-6 is busy, wait 300s
20240222 19:00:28[INFO] gpu-7 is busy, wait 300s
20240222 19:01:22[INFO] gpu-5 is busy, wait 300s
20240222 19:01:34[INFO] gpu-4 is busy, wait 300s
20240222 19:02:03[INFO] gpu-3 is busy, wait 300s
20240222 19:02:30[INFO] gpu-2 is busy, wait 300s
20240222 19:05:29[INFO] gpu-7 is busy, wait 300s
20240222 19:05:41[INFO] gpu-6 is busy, wait 300s
20240222 19:06:23[INFO] gpu-5 is busy, wait 300s
20240222 19:06:35[INFO] gpu-4 is busy, wait 300s
20240222 19:07:04[INFO] gpu-3 is busy, wait 300s
20240222 19:07:31[INFO] gpu-2 is busy, wait 300s
20240222 19:10:29[INFO] gpu-7 is busy, wait 300s
20240222 19:10:54[INFO] gpu-6 is busy, wait 300s
20240222 19:11:23[INFO] gpu-5 is busy, wait 300s
20240222 19:11:36[INFO] gpu-4 is busy, wait 300s
20240222 19:12:04[INFO] gpu-3 is busy, wait 300s
20240222 19:12:32[INFO] gpu-2 is busy, wait 300s
20240222 19:15:30[INFO] gpu-7 is busy, wait 300s
20240222 19:15:59[INFO] gpu-6 is busy, wait 300s
20240222 19:16:24[INFO] gpu-5 is busy, wait 300s
20240222 19:16:36[INFO] gpu-4 is busy, wait 300s
20240222 19:17:05[INFO] gpu-3 is busy, wait 300s
20240222 19:17:33[INFO] gpu-2 is busy, wait 300s
20240222 19:20:31[INFO] gpu-7 is busy, wait 300s
20240222 19:21:19[INFO] gpu-6 is busy, wait 300s
20240222 19:21:25[INFO] gpu-5 is busy, wait 300s
20240222 19:21:37[INFO] gpu-4 is busy, wait 300s
20240222 19:22:06[INFO] gpu-3 is busy, wait 300s
20240222 19:22:34[INFO] gpu-2 is busy, wait 300s
20240222 19:25:31[INFO] gpu-7 is busy, wait 300s
20240222 19:26:24[INFO] gpu-6 is busy, wait 300s
20240222 19:26:25[INFO] gpu-5 is busy, wait 300s
20240222 19:26:37[INFO] gpu-4 is busy, wait 300s
20240222 19:27:06[INFO] gpu-3 is busy, wait 300s
20240222 19:27:34[INFO] gpu-2 is busy, wait 300s
20240222 19:30:32[INFO] gpu-7 is busy, wait 300s
20240222 19:31:26[INFO] gpu-5 is busy, wait 300s
20240222 19:31:38[INFO] gpu-4 is busy, wait 300s
20240222 19:31:41[INFO] gpu-6 is busy, wait 300s
20240222 19:32:07[INFO] gpu-3 is busy, wait 300s
20240222 19:32:35[INFO] gpu-2 is busy, wait 300s
20240222 19:35:33[INFO] gpu-7 is busy, wait 300s
20240222 19:36:27[INFO] gpu-5 is busy, wait 300s
20240222 19:36:39[INFO] gpu-4 is busy, wait 300s
20240222 19:36:48[INFO] gpu-6 is busy, wait 300s
20240222 19:37:07[INFO] gpu-3 is busy, wait 300s
20240222 19:37:35[INFO] gpu-2 is busy, wait 300s
20240222 19:42:06[INFO] gpu-6 is busy, wait 300s
20240222 19:42:36[INFO] gpu-2 is busy, wait 300s
20240222 19:47:14[INFO] gpu-6 is busy, wait 300s
20240222 19:52:29[INFO] gpu-6 is busy, wait 300s
20240222 19:57:44[INFO] gpu-6 is busy, wait 300s
20240222 20:03:11[INFO] gpu-6 is busy, wait 300s
20240222 20:08:25[INFO] gpu-6 is busy, wait 300s
20240222 20:13:46[INFO] gpu-6 is busy, wait 300s
20240222 20:19:09[INFO] gpu-6 is busy, wait 300s
20240222 20:24:59[INFO] gpu-6 is busy, wait 300s
20240222 20:30:04[INFO] gpu-6 is busy, wait 300s
20240222 20:35:35[INFO] gpu-6 is busy, wait 300s
20240222 20:40:49[INFO] gpu-6 is busy, wait 300s
20240222 20:45:56[INFO] gpu-6 is busy, wait 300s
20240222 20:51:12[INFO] gpu-6 is busy, wait 300s
20240222 20:57:02[INFO] gpu-6 is busy, wait 300s
20240222 21:02:15[INFO] gpu-6 is busy, wait 300s
20240222 21:07:30[INFO] gpu-6 is busy, wait 300s
20240222 21:12:42[INFO] gpu-6 is busy, wait 300s
20240222 21:18:02[INFO] gpu-6 is busy, wait 300s
20240222 21:23:16[INFO] gpu-6 is busy, wait 300s
20240222 21:28:34[INFO] gpu-6 is busy, wait 300s
20240222 21:33:59[INFO] gpu-6 is busy, wait 300s
20240222 21:39:11[INFO] gpu-6 is busy, wait 300s
20240222 21:46:06[INFO] gpu-6 is busy, wait 300s
20240222 21:51:15[INFO] gpu-6 is busy, wait 300s
20240222 21:56:24[INFO] gpu-6 is busy, wait 300s
20240222 22:02:31[INFO] gpu-6 is busy, wait 300s
20240222 22:07:39[INFO] gpu-6 is busy, wait 300s
20240222 22:12:45[INFO] gpu-6 is busy, wait 300s
20240222 22:19:08[INFO] gpu-6 is busy, wait 300s
20240222 22:24:33[INFO] gpu-6 is busy, wait 300s
20240222 22:32:00[INFO] gpu-6 is busy, wait 300s
20240222 22:37:23[INFO] gpu-6 is busy, wait 300s
20240222 22:42:34[INFO] gpu-6 is busy, wait 300s
20240222 22:47:59[INFO] gpu-6 is busy, wait 300s
20240222 22:54:17[INFO] gpu-6 is busy, wait 300s
20240222 22:59:25[INFO] gpu-6 is busy, wait 300s
20240222 23:04:31[INFO] gpu-6 is busy, wait 300s
20240222 23:09:41[INFO] gpu-6 is busy, wait 300s
20240222 23:14:49[INFO] gpu-6 is busy, wait 300s
20240222 23:20:02[INFO] gpu-6 is busy, wait 300s
20240222 23:25:42[INFO] gpu-6 is busy, wait 300s
20240222 23:31:05[INFO] gpu-6 is busy, wait 300s
20240222 23:37:18[INFO] gpu-6 is busy, wait 300s
20240222 23:42:42[INFO] gpu-6 is busy, wait 300s
20240222 23:48:13[INFO] gpu-6 is busy, wait 300s
20240222 23:53:51[INFO] gpu-6 is busy, wait 300s
20240222 23:59:25[INFO] gpu-6 is busy, wait 300s
20240223 00:04:43[INFO] gpu-6 is busy, wait 300s
20240223 00:09:51[INFO] gpu-6 is busy, wait 300s
20240223 00:15:16[INFO] gpu-6 is busy, wait 300s
20240223 00:20:28[INFO] gpu-6 is busy, wait 300s
20240223 00:25:49[INFO] gpu-6 is busy, wait 300s
20240223 00:30:54[INFO] gpu-6 is busy, wait 300s
20240223 00:36:26[INFO] gpu-6 is busy, wait 300s
20240223 00:42:04[INFO] gpu-6 is busy, wait 300s
20240223 00:48:37[INFO] gpu-6 is busy, wait 300s
20240223 00:53:44[INFO] gpu-6 is busy, wait 300s
20240223 00:58:53[INFO] gpu-6 is busy, wait 300s
20240223 01:04:55[INFO] gpu-6 is busy, wait 300s
20240223 01:10:03[INFO] gpu-6 is busy, wait 300s
20240223 01:16:07[INFO] gpu-6 is busy, wait 300s
20240223 01:21:15[INFO] gpu-6 is busy, wait 300s
20240223 01:26:23[INFO] gpu-6 is busy, wait 300s
20240223 01:31:30[INFO] gpu-6 is busy, wait 300s
20240223 01:36:39[INFO] gpu-6 is busy, wait 300s
20240223 01:42:42[INFO] gpu-6 is busy, wait 300s
20240223 01:49:02[INFO] gpu-6 is busy, wait 300s
20240223 01:54:07[INFO] gpu-6 is busy, wait 300s
20240223 01:59:12[INFO] gpu-6 is busy, wait 300s
20240223 02:04:34[INFO] gpu-6 is busy, wait 300s
20240223 02:09:39[INFO] gpu-6 is busy, wait 300s
20240223 02:15:12[INFO] gpu-6 is busy, wait 300s
20240223 02:20:18[INFO] gpu-6 is busy, wait 300s
20240223 02:25:37[INFO] gpu-6 is busy, wait 300s
20240223 02:31:01[INFO] gpu-6 is busy, wait 300s
20240223 02:37:22[INFO] gpu-6 is busy, wait 300s
20240223 02:42:28[INFO] gpu-6 is busy, wait 300s
20240223 02:47:51[INFO] gpu-6 is busy, wait 300s
20240223 02:53:02[INFO] gpu-6 is busy, wait 300s
20240223 02:58:19[INFO] gpu-6 is busy, wait 300s
20240223 03:03:47[INFO] gpu-6 is busy, wait 300s
20240223 03:09:05[INFO] gpu-6 is busy, wait 300s
20240223 03:14:55[INFO] gpu-6 is busy, wait 300s
20240223 03:20:42[INFO] gpu-6 is busy, wait 300s
20240223 03:26:12[INFO] gpu-6 is busy, wait 300s
20240223 03:31:17[INFO] gpu-6 is busy, wait 300s
20240223 03:36:29[INFO] gpu-6 is busy, wait 300s
20240223 03:41:43[INFO] gpu-6 is busy, wait 300s
20240223 03:47:02[INFO] gpu-6 is busy, wait 300s
20240223 03:52:28[INFO] gpu-6 is busy, wait 300s
20240223 03:57:53[INFO] gpu-6 is busy, wait 300s
20240223 04:04:06[INFO] gpu-6 is busy, wait 300s
20240223 04:09:31[INFO] gpu-6 is busy, wait 300s
20240223 04:14:47[INFO] gpu-6 is busy, wait 300s
20240223 04:20:11[INFO] gpu-6 is busy, wait 300s
20240223 04:25:21[INFO] gpu-6 is busy, wait 300s
20240223 04:30:27[INFO] gpu-6 is busy, wait 300s
20240223 04:35:48[INFO] gpu-6 is busy, wait 300s
20240223 04:41:15[INFO] gpu-6 is busy, wait 300s
20240223 04:46:31[INFO] gpu-6 is busy, wait 300s
20240223 04:51:37[INFO] gpu-6 is busy, wait 300s
20240223 04:56:57[INFO] gpu-6 is busy, wait 300s
20240223 05:02:02[INFO] gpu-6 is busy, wait 300s
20240223 05:07:15[INFO] gpu-6 is busy, wait 300s
20240223 05:12:33[INFO] gpu-6 is busy, wait 300s
20240223 05:17:50[INFO] gpu-6 is busy, wait 300s
20240223 05:23:11[INFO] gpu-6 is busy, wait 300s
20240223 05:28:37[INFO] gpu-6 is busy, wait 300s
20240223 05:34:08[INFO] gpu-6 is busy, wait 300s
20240223 05:39:34[INFO] gpu-6 is busy, wait 300s
20240223 05:44:59[INFO] gpu-6 is busy, wait 300s
20240223 05:50:30[INFO] gpu-6 is busy, wait 300s
20240223 05:56:32[INFO] gpu-6 is busy, wait 300s
20240223 06:01:39[INFO] gpu-6 is busy, wait 300s
20240223 06:07:07[INFO] gpu-6 is busy, wait 300s
20240223 06:12:14[INFO] gpu-6 is busy, wait 300s
20240223 06:17:25[INFO] gpu-6 is busy, wait 300s
20240223 06:22:37[INFO] gpu-6 is busy, wait 300s
20240223 06:27:49[INFO] gpu-6 is busy, wait 300s
20240223 06:32:57[INFO] gpu-6 is busy, wait 300s
20240223 06:38:13[INFO] gpu-6 is busy, wait 300s
20240223 06:43:29[INFO] gpu-6 is busy, wait 300s
20240223 06:48:37[INFO] gpu-6 is busy, wait 300s
20240223 06:53:49[INFO] gpu-6 is busy, wait 300s
20240223 06:59:29[INFO] gpu-6 is busy, wait 300s
20240223 07:04:37[INFO] gpu-6 is busy, wait 300s
20240223 07:10:02[INFO] gpu-6 is busy, wait 300s
20240223 07:15:44[INFO] gpu-6 is busy, wait 300s
20240223 07:20:51[INFO] gpu-6 is busy, wait 300s
20240223 07:26:47[INFO] gpu-6 is busy, wait 300s
20240223 07:31:53[INFO] gpu-6 is busy, wait 300s
20240223 07:37:44[INFO] gpu-6 is busy, wait 300s
20240223 07:43:18[INFO] gpu-6 is busy, wait 300s
20240223 07:48:33[INFO] gpu-6 is busy, wait 300s
20240223 07:53:41[INFO] gpu-6 is busy, wait 300s
20240223 07:58:47[INFO] gpu-6 is busy, wait 300s
20240223 08:03:54[INFO] gpu-6 is busy, wait 300s
20240223 08:09:02[INFO] gpu-6 is busy, wait 300s
20240223 08:14:11[INFO] gpu-6 is busy, wait 300s
20240223 08:19:41[INFO] gpu-6 is busy, wait 300s
20240223 08:24:47[INFO] gpu-6 is busy, wait 300s
20240223 08:30:39[INFO] gpu-6 is busy, wait 300s
20240223 08:36:31[INFO] gpu-6 is busy, wait 300s
20240223 08:41:51[INFO] gpu-6 is busy, wait 300s
20240223 08:47:16[INFO] gpu-6 is busy, wait 300s
20240223 08:52:52[INFO] gpu-6 is busy, wait 300s
20240223 08:58:08[INFO] gpu-6 is busy, wait 300s
20240223 09:03:33[INFO] gpu-6 is busy, wait 300s
20240223 09:08:40[INFO] gpu-6 is busy, wait 300s
20240223 09:15:08[INFO] gpu-6 is busy, wait 300s
20240223 09:20:19[INFO] gpu-6 is busy, wait 300s
20240223 09:25:32[INFO] gpu-6 is busy, wait 300s
20240223 09:30:46[INFO] gpu-6 is busy, wait 300s
20240223 09:35:54[INFO] gpu-6 is busy, wait 300s
20240223 09:41:40[INFO] gpu-6 is busy, wait 300s
20240223 09:47:24[INFO] gpu-6 is busy, wait 300s
20240223 09:52:58[INFO] gpu-6 is busy, wait 300s
20240223 09:58:12[INFO] gpu-6 is busy, wait 300s
20240223 10:03:18[INFO] gpu-6 is busy, wait 300s
20240223 10:08:31[INFO] gpu-6 is busy, wait 300s
20240223 10:14:12[INFO] gpu-6 is busy, wait 300s
20240223 10:19:18[INFO] gpu-6 is busy, wait 300s
20240223 10:24:27[INFO] gpu-6 is busy, wait 300s
20240223 10:29:41[INFO] gpu-6 is busy, wait 300s
20240223 10:34:49[INFO] gpu-6 is busy, wait 300s
20240223 10:40:05[INFO] gpu-6 is busy, wait 300s
20240223 10:45:13[INFO] gpu-6 is busy, wait 300s
20240223 10:50:28[INFO] gpu-6 is busy, wait 300s
20240223 10:55:52[INFO] gpu-6 is busy, wait 300s
20240223 11:00:33[INFO] gpu-1 is busy, wait 300s
20240223 11:01:01[INFO] gpu-6 is busy, wait 300s
20240223 11:01:13[INFO] gpu-0 is busy, wait 300s
20240223 11:05:34[INFO] gpu-1 is busy, wait 300s
20240223 11:06:20[INFO] gpu-0 is busy, wait 300s
20240223 11:06:25[INFO] gpu-6 is busy, wait 300s
20240223 11:10:35[INFO] gpu-1 is busy, wait 300s
20240223 11:10:45[INFO] gpu-2 is busy, wait 300s
Process Process-1:30385:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 308.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.22 GiB memory in use. Process 3456950 has 2.89 GiB memory in use. Of the allocated memory 2.43 GiB is allocated by PyTorch, and 14.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 11:11:32[INFO] gpu-0 is busy, wait 300s
20240223 11:11:48[INFO] gpu-6 is busy, wait 300s
20240223 11:15:35[INFO] gpu-1 is busy, wait 300s
20240223 11:15:46[INFO] gpu-2 is busy, wait 300s
20240223 11:16:43[INFO] gpu-0 is busy, wait 300s
20240223 11:17:32[INFO] gpu-6 is busy, wait 300s
20240223 11:20:36[INFO] gpu-1 is busy, wait 300s
20240223 11:20:46[INFO] gpu-2 is busy, wait 300s
20240223 11:21:48[INFO] gpu-0 is busy, wait 300s
20240223 11:22:41[INFO] gpu-6 is busy, wait 300s
20240223 11:25:37[INFO] gpu-1 is busy, wait 300s
20240223 11:25:47[INFO] gpu-2 is busy, wait 300s
Process Process-1:30390:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.22 GiB memory in use. Process 3680308 has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 19.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30391:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 358.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.22 GiB memory in use. Process 3681498 has 2.84 GiB memory in use. Of the allocated memory 2.07 GiB is allocated by PyTorch, and 337.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 11:26:53[INFO] gpu-0 is busy, wait 300s
20240223 11:28:05[INFO] gpu-6 is busy, wait 300s
20240223 11:30:38[INFO] gpu-1 is busy, wait 300s
20240223 11:30:48[INFO] gpu-2 is busy, wait 300s
Process Process-1:30392:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.22 GiB memory in use. Process 3835932 has 3.18 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 15.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 11:31:56[INFO] gpu-0 is busy, wait 300s
20240223 11:33:29[INFO] gpu-6 is busy, wait 300s
20240223 11:35:38[INFO] gpu-1 is busy, wait 300s
20240223 11:35:49[INFO] gpu-2 is busy, wait 300s
20240223 11:37:04[INFO] gpu-0 is busy, wait 300s
20240223 11:38:34[INFO] gpu-6 is busy, wait 300s
20240223 11:40:39[INFO] gpu-1 is busy, wait 300s
20240223 11:40:49[INFO] gpu-2 is busy, wait 300s
20240223 11:42:10[INFO] gpu-0 is busy, wait 300s
20240223 11:43:41[INFO] gpu-6 is busy, wait 300s
20240223 11:45:39[INFO] gpu-1 is busy, wait 300s
20240223 11:45:50[INFO] gpu-2 is busy, wait 300s
20240223 11:47:23[INFO] gpu-0 is busy, wait 300s
20240223 11:48:57[INFO] gpu-6 is busy, wait 300s
20240223 11:50:40[INFO] gpu-1 is busy, wait 300s
20240223 11:50:51[INFO] gpu-2 is busy, wait 300s
Process Process-1:30397:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 542.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 148141 has 2.92 GiB memory in use. Of the allocated memory 2.45 GiB is allocated by PyTorch, and 18.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30398:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 340.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 149004 has 2.63 GiB memory in use. Of the allocated memory 1.88 GiB is allocated by PyTorch, and 314.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 11:52:28[INFO] gpu-0 is busy, wait 300s
20240223 11:54:03[INFO] gpu-6 is busy, wait 300s
20240223 11:55:41[INFO] gpu-1 is busy, wait 300s
20240223 11:55:51[INFO] gpu-2 is busy, wait 300s
Process Process-1:30399:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 490.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 260045 has 2.49 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 15.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30401:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 578.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 520.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 262303 has 2.46 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30403:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 526.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 265930 has 2.85 GiB memory in use. Of the allocated memory 2.39 GiB is allocated by PyTorch, and 14.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 11:57:46[INFO] gpu-0 is busy, wait 300s
20240223 11:59:16[INFO] gpu-6 is busy, wait 300s
20240223 12:00:41[INFO] gpu-1 is busy, wait 300s
20240223 12:00:52[INFO] gpu-2 is busy, wait 300s
Process Process-1:30404:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 456.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 448.94 MiB is free. Process 3242257 has 10.22 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 381119 has 2.53 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 20.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 12:03:02[INFO] gpu-0 is busy, wait 300s
20240223 12:04:22[INFO] gpu-6 is busy, wait 300s
20240223 12:05:42[INFO] gpu-1 is busy, wait 300s
20240223 12:05:56[INFO] gpu-2 is busy, wait 300s
20240223 12:08:10[INFO] gpu-0 is busy, wait 300s
20240223 12:09:30[INFO] gpu-6 is busy, wait 300s
20240223 12:10:42[INFO] gpu-1 is busy, wait 300s
20240223 12:10:57[INFO] gpu-2 is busy, wait 300s
20240223 12:13:15[INFO] gpu-0 is busy, wait 300s
20240223 12:14:36[INFO] gpu-6 is busy, wait 300s
20240223 12:15:43[INFO] gpu-1 is busy, wait 300s
20240223 12:15:58[INFO] gpu-2 is busy, wait 300s
20240223 12:18:30[INFO] gpu-0 is busy, wait 300s
20240223 12:19:43[INFO] gpu-6 is busy, wait 300s
20240223 12:20:44[INFO] gpu-1 is busy, wait 300s
20240223 12:20:58[INFO] gpu-2 is busy, wait 300s
Process Process-1:30411:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 254.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 865855 has 2.49 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 289.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30412:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 468.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 158.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 866718 has 2.58 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 15.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30413:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 260.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 867252 has 2.48 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30416:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 526.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 478.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 873133 has 2.27 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 17.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30418:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 877039 has 2.71 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 18.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 12:24:02[INFO] gpu-0 is busy, wait 300s
20240223 12:25:41[INFO] gpu-6 is busy, wait 300s
20240223 12:25:45[INFO] gpu-1 is busy, wait 300s
20240223 12:25:59[INFO] gpu-2 is busy, wait 300s
20240223 12:29:19[INFO] gpu-0 is busy, wait 300s
20240223 12:30:46[INFO] gpu-1 is busy, wait 300s
20240223 12:30:52[INFO] gpu-6 is busy, wait 300s
20240223 12:30:59[INFO] gpu-2 is busy, wait 300s
Process Process-1:30421:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1064137 has 2.54 GiB memory in use. Of the allocated memory 1.80 GiB is allocated by PyTorch, and 302.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 12:34:21[INFO] gpu-0 is busy, wait 300s
20240223 12:35:47[INFO] gpu-1 is busy, wait 300s
20240223 12:35:57[INFO] gpu-6 is busy, wait 300s
20240223 12:36:00[INFO] gpu-2 is busy, wait 300s
Process Process-1:30422:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1107525 has 2.49 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30423:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 472.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1107854 has 2.60 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 20.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 12:39:25[INFO] gpu-0 is busy, wait 300s
20240223 12:40:47[INFO] gpu-1 is busy, wait 300s
20240223 12:41:01[INFO] gpu-2 is busy, wait 300s
20240223 12:41:34[INFO] gpu-6 is busy, wait 300s
20240223 12:44:32[INFO] gpu-0 is busy, wait 300s
20240223 12:45:48[INFO] gpu-1 is busy, wait 300s
20240223 12:46:01[INFO] gpu-2 is busy, wait 300s
20240223 12:46:41[INFO] gpu-6 is busy, wait 300s
Process Process-1:30425:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1193245 has 2.67 GiB memory in use. Of the allocated memory 2.21 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 12:49:47[INFO] gpu-0 is busy, wait 300s
20240223 12:50:48[INFO] gpu-1 is busy, wait 300s
20240223 12:51:02[INFO] gpu-2 is busy, wait 300s
20240223 12:51:58[INFO] gpu-6 is busy, wait 300s
20240223 12:54:52[INFO] gpu-0 is busy, wait 300s
20240223 12:55:49[INFO] gpu-1 is busy, wait 300s
20240223 12:56:12[INFO] gpu-2 is busy, wait 300s
20240223 12:57:05[INFO] gpu-6 is busy, wait 300s
Process Process-1:30429:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 232.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1318839 has 2.51 GiB memory in use. Of the allocated memory 1.78 GiB is allocated by PyTorch, and 292.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 12:59:54[INFO] gpu-0 is busy, wait 300s
20240223 13:00:49[INFO] gpu-1 is busy, wait 300s
20240223 13:01:12[INFO] gpu-2 is busy, wait 300s
20240223 13:02:18[INFO] gpu-6 is busy, wait 300s
20240223 13:04:59[INFO] gpu-0 is busy, wait 300s
20240223 13:05:50[INFO] gpu-1 is busy, wait 300s
20240223 13:06:21[INFO] gpu-2 is busy, wait 300s
20240223 13:07:26[INFO] gpu-6 is busy, wait 300s
Process Process-1:30431:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 558.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 362.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1554875 has 2.38 GiB memory in use. Of the allocated memory 1.98 GiB is allocated by PyTorch, and 17.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30433:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 612.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1559929 has 2.58 GiB memory in use. Of the allocated memory 2.18 GiB is allocated by PyTorch, and 15.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30435:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1564252 has 2.49 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 21.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 13:10:41[INFO] gpu-0 is busy, wait 300s
20240223 13:10:51[INFO] gpu-1 is busy, wait 300s
20240223 13:11:21[INFO] gpu-2 is busy, wait 300s
20240223 13:12:32[INFO] gpu-6 is busy, wait 300s
Process Process-1:30440:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.94 MiB is free. Process 3242257 has 10.45 GiB memory in use. Process 3450553 has 10.45 GiB memory in use. Process 1735862 has 2.62 GiB memory in use. Of the allocated memory 1.88 GiB is allocated by PyTorch, and 309.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 13:15:51[INFO] gpu-1 is busy, wait 300s
20240223 13:15:59[INFO] gpu-0 is busy, wait 300s
20240223 13:16:33[INFO] gpu-2 is busy, wait 300s
20240223 13:17:50[INFO] gpu-6 is busy, wait 300s
20240223 13:21:21[INFO] gpu-0 is busy, wait 300s
20240223 13:21:33[INFO] gpu-2 is busy, wait 300s
20240223 13:23:15[INFO] gpu-6 is busy, wait 300s
20240223 13:26:32[INFO] gpu-0 is busy, wait 300s
20240223 13:26:34[INFO] gpu-2 is busy, wait 300s
20240223 13:28:32[INFO] gpu-6 is busy, wait 300s
20240223 13:31:33[INFO] gpu-0 is busy, wait 300s
20240223 13:31:41[INFO] gpu-2 is busy, wait 300s
20240223 13:33:37[INFO] gpu-6 is busy, wait 300s
20240223 13:36:42[INFO] gpu-2 is busy, wait 300s
20240223 13:36:47[INFO] gpu-0 is busy, wait 300s
20240223 13:39:35[INFO] gpu-6 is busy, wait 300s
20240223 13:41:43[INFO] gpu-2 is busy, wait 300s
20240223 13:41:53[INFO] gpu-0 is busy, wait 300s
20240223 13:44:48[INFO] gpu-6 is busy, wait 300s
20240223 13:46:56[INFO] gpu-1 is busy, wait 300s
20240223 13:47:36[INFO] gpu-0 is busy, wait 300s
20240223 13:49:56[INFO] gpu-6 is busy, wait 300s
20240223 13:51:57[INFO] gpu-1 is busy, wait 300s
20240223 13:52:59[INFO] gpu-0 is busy, wait 300s
20240223 13:55:04[INFO] gpu-6 is busy, wait 300s
20240223 13:56:58[INFO] gpu-1 is busy, wait 300s
20240223 13:58:30[INFO] gpu-0 is busy, wait 300s
20240223 14:00:11[INFO] gpu-6 is busy, wait 300s
20240223 14:01:58[INFO] gpu-1 is busy, wait 300s
20240223 14:03:59[INFO] gpu-0 is busy, wait 300s
20240223 14:05:17[INFO] gpu-6 is busy, wait 300s
20240223 14:06:59[INFO] gpu-1 is busy, wait 300s
20240223 14:09:07[INFO] gpu-0 is busy, wait 300s
20240223 14:10:42[INFO] gpu-6 is busy, wait 300s
20240223 14:12:00[INFO] gpu-1 is busy, wait 300s
20240223 14:14:25[INFO] gpu-0 is busy, wait 300s
20240223 14:15:18[INFO] gpu-2 is busy, wait 300s
20240223 14:15:48[INFO] gpu-6 is busy, wait 300s
20240223 14:17:01[INFO] gpu-1 is busy, wait 300s
20240223 14:19:32[INFO] gpu-0 is busy, wait 300s
20240223 14:20:19[INFO] gpu-2 is busy, wait 300s
20240223 14:20:53[INFO] gpu-6 is busy, wait 300s
20240223 14:22:01[INFO] gpu-1 is busy, wait 300s
Process Process-1:30477:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 568.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3443348 has 3.03 GiB memory in use. Of the allocated memory 2.57 GiB is allocated by PyTorch, and 20.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 14:25:07[INFO] gpu-0 is busy, wait 300s
20240223 14:25:20[INFO] gpu-2 is busy, wait 300s
20240223 14:26:01[INFO] gpu-6 is busy, wait 300s
20240223 14:27:02[INFO] gpu-1 is busy, wait 300s
Process Process-1:30479:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 430.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 368.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3558061 has 2.83 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 337.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30480:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 422.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 414.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3558904 has 2.79 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 329.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 14:30:21[INFO] gpu-2 is busy, wait 300s
Process Process-1:30484:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3566190 has 3.07 GiB memory in use. Of the allocated memory 2.60 GiB is allocated by PyTorch, and 21.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 14:30:40[INFO] gpu-0 is busy, wait 300s
20240223 14:31:06[INFO] gpu-6 is busy, wait 300s
20240223 14:32:03[INFO] gpu-1 is busy, wait 300s
20240223 14:35:22[INFO] gpu-2 is busy, wait 300s
20240223 14:35:47[INFO] gpu-0 is busy, wait 300s
20240223 14:36:42[INFO] gpu-6 is busy, wait 300s
20240223 14:37:03[INFO] gpu-1 is busy, wait 300s
20240223 14:40:22[INFO] gpu-2 is busy, wait 300s
Process Process-1:30488:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 406.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3810999 has 3.10 GiB memory in use. Of the allocated memory 2.19 GiB is allocated by PyTorch, and 470.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30489:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 438.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 324.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3812387 has 2.88 GiB memory in use. Of the allocated memory 2.10 GiB is allocated by PyTorch, and 341.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 14:40:57[INFO] gpu-0 is busy, wait 300s
20240223 14:41:49[INFO] gpu-6 is busy, wait 300s
20240223 14:42:04[INFO] gpu-1 is busy, wait 300s
20240223 14:45:23[INFO] gpu-2 is busy, wait 300s
Process Process-1:30490:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 506.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 446.94 MiB is free. Process 2485445 has 10.22 GiB memory in use. Process 3206896 has 10.22 GiB memory in use. Process 3949331 has 2.76 GiB memory in use. Of the allocated memory 2.30 GiB is allocated by PyTorch, and 15.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 14:46:04[INFO] gpu-0 is busy, wait 300s
20240223 14:47:04[INFO] gpu-1 is busy, wait 300s
20240223 14:47:43[INFO] gpu-6 is busy, wait 300s
20240223 14:50:31[INFO] gpu-2 is busy, wait 300s
20240223 14:51:09[INFO] gpu-0 is busy, wait 300s
20240223 14:52:05[INFO] gpu-1 is busy, wait 300s
20240223 14:52:51[INFO] gpu-6 is busy, wait 300s
20240223 14:55:32[INFO] gpu-2 is busy, wait 300s
20240223 14:56:17[INFO] gpu-0 is busy, wait 300s
20240223 14:57:06[INFO] gpu-1 is busy, wait 300s
20240223 14:58:15[INFO] gpu-6 is busy, wait 300s
20240223 15:00:33[INFO] gpu-2 is busy, wait 300s
Process Process-1:30494:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 580.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 280.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 120750 has 2.46 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 15.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30495:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 550.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 390.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 121804 has 2.36 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 15:01:22[INFO] gpu-0 is busy, wait 300s
20240223 15:02:06[INFO] gpu-1 is busy, wait 300s
20240223 15:03:50[INFO] gpu-6 is busy, wait 300s
20240223 15:05:33[INFO] gpu-2 is busy, wait 300s
Process Process-1:30496:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 592.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 238.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 220697 has 2.51 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30497:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 104.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 221282 has 2.73 GiB memory in use. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 15:06:26[INFO] gpu-0 is busy, wait 300s
20240223 15:07:07[INFO] gpu-1 is busy, wait 300s
20240223 15:09:04[INFO] gpu-6 is busy, wait 300s
20240223 15:10:39[INFO] gpu-2 is busy, wait 300s
20240223 15:11:34[INFO] gpu-0 is busy, wait 300s
20240223 15:12:07[INFO] gpu-1 is busy, wait 300s
20240223 15:14:10[INFO] gpu-6 is busy, wait 300s
20240223 15:15:48[INFO] gpu-2 is busy, wait 300s
Process Process-1:30500:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 408086 has 2.42 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 19.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30501:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process Process-1:30502:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 562.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 346.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 409682 has 2.40 GiB memory in use. Of the allocated memory 2.00 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30503:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 554.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 376.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 410023 has 2.37 GiB memory in use. Of the allocated memory 1.97 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30504:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 460.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 196.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 410870 has 2.55 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 15:16:49[INFO] gpu-0 is busy, wait 300s
20240223 15:17:08[INFO] gpu-1 is busy, wait 300s
20240223 15:19:43[INFO] gpu-6 is busy, wait 300s
20240223 15:20:48[INFO] gpu-2 is busy, wait 300s
Process Process-1:30505:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 537297 has 2.63 GiB memory in use. Of the allocated memory 1.89 GiB is allocated by PyTorch, and 310.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 15:21:51[INFO] gpu-0 is busy, wait 300s
20240223 15:22:09[INFO] gpu-1 is busy, wait 300s
20240223 15:25:07[INFO] gpu-6 is busy, wait 300s
20240223 15:25:49[INFO] gpu-2 is busy, wait 300s
20240223 15:26:58[INFO] gpu-0 is busy, wait 300s
20240223 15:27:10[INFO] gpu-1 is busy, wait 300s
20240223 15:30:25[INFO] gpu-6 is busy, wait 300s
20240223 15:30:54[INFO] gpu-2 is busy, wait 300s
20240223 15:32:11[INFO] gpu-1 is busy, wait 300s
20240223 15:32:13[INFO] gpu-0 is busy, wait 300s
20240223 15:35:30[INFO] gpu-6 is busy, wait 300s
20240223 15:35:54[INFO] gpu-2 is busy, wait 300s
20240223 15:37:11[INFO] gpu-1 is busy, wait 300s
20240223 15:37:26[INFO] gpu-0 is busy, wait 300s
20240223 15:40:50[INFO] gpu-6 is busy, wait 300s
20240223 15:40:55[INFO] gpu-2 is busy, wait 300s
20240223 15:42:12[INFO] gpu-1 is busy, wait 300s
20240223 15:42:32[INFO] gpu-0 is busy, wait 300s
20240223 15:45:54[INFO] gpu-6 is busy, wait 300s
20240223 15:45:56[INFO] gpu-2 is busy, wait 300s
20240223 15:47:12[INFO] gpu-1 is busy, wait 300s
20240223 15:47:39[INFO] gpu-0 is busy, wait 300s
20240223 15:50:56[INFO] gpu-2 is busy, wait 300s
20240223 15:51:13[INFO] gpu-6 is busy, wait 300s
20240223 15:52:13[INFO] gpu-1 is busy, wait 300s
Process Process-1:30513:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1244980 has 2.54 GiB memory in use. Of the allocated memory 1.81 GiB is allocated by PyTorch, and 297.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30514:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1245752 has 2.55 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 18.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30516:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 142.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1249024 has 2.60 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 310.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 15:52:51[INFO] gpu-0 is busy, wait 300s
20240223 15:55:57[INFO] gpu-2 is busy, wait 300s
20240223 15:56:33[INFO] gpu-6 is busy, wait 300s
20240223 15:57:14[INFO] gpu-1 is busy, wait 300s
Process Process-1:30517:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1367684 has 2.49 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 18.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30520:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 440.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1372903 has 2.31 GiB memory in use. Of the allocated memory 1.90 GiB is allocated by PyTorch, and 19.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 15:58:15[INFO] gpu-0 is busy, wait 300s
20240223 16:00:58[INFO] gpu-2 is busy, wait 300s
20240223 16:01:42[INFO] gpu-6 is busy, wait 300s
20240223 16:02:14[INFO] gpu-1 is busy, wait 300s
Process Process-1:30522:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 516.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1449782 has 2.24 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:03:16[INFO] gpu-0 is busy, wait 300s
20240223 16:05:58[INFO] gpu-2 is busy, wait 300s
20240223 16:06:51[INFO] gpu-6 is busy, wait 300s
20240223 16:07:15[INFO] gpu-1 is busy, wait 300s
20240223 16:08:17[INFO] gpu-0 is busy, wait 300s
20240223 16:10:59[INFO] gpu-2 is busy, wait 300s
20240223 16:12:06[INFO] gpu-6 is busy, wait 300s
20240223 16:12:16[INFO] gpu-1 is busy, wait 300s
Process Process-1:30524:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 518.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 506.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1679498 has 2.24 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:13:38[INFO] gpu-0 is busy, wait 300s
20240223 16:16:00[INFO] gpu-2 is busy, wait 300s
20240223 16:17:14[INFO] gpu-6 is busy, wait 300s
20240223 16:17:16[INFO] gpu-1 is busy, wait 300s
20240223 16:18:46[INFO] gpu-0 is busy, wait 300s
20240223 16:21:01[INFO] gpu-2 is busy, wait 300s
20240223 16:22:18[INFO] gpu-1 is busy, wait 300s
20240223 16:22:28[INFO] gpu-6 is busy, wait 300s
Process Process-1:30528:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 416.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 400.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 1926980 has 2.35 GiB memory in use. Of the allocated memory 1.89 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:23:48[INFO] gpu-0 is busy, wait 300s
20240223 16:26:01[INFO] gpu-2 is busy, wait 300s
20240223 16:27:18[INFO] gpu-1 is busy, wait 300s
20240223 16:27:49[INFO] gpu-6 is busy, wait 300s
Process Process-1:30529:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 260.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2048829 has 2.48 GiB memory in use. Of the allocated memory 2.02 GiB is allocated by PyTorch, and 14.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:28:57[INFO] gpu-0 is busy, wait 300s
20240223 16:31:02[INFO] gpu-2 is busy, wait 300s
20240223 16:32:19[INFO] gpu-1 is busy, wait 300s
20240223 16:33:27[INFO] gpu-6 is busy, wait 300s
Process Process-1:30531:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 390.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2191863 has 2.61 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 306.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30533:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 230.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2195650 has 2.51 GiB memory in use. Of the allocated memory 1.78 GiB is allocated by PyTorch, and 294.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30536:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 438.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 298.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2202855 has 2.45 GiB memory in use. Of the allocated memory 1.98 GiB is allocated by PyTorch, and 20.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:34:23[INFO] gpu-0 is busy, wait 300s
20240223 16:36:03[INFO] gpu-2 is busy, wait 300s
20240223 16:37:19[INFO] gpu-1 is busy, wait 300s
20240223 16:38:56[INFO] gpu-6 is busy, wait 300s
Process Process-1:30537:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2332673 has 2.62 GiB memory in use. Of the allocated memory 1.88 GiB is allocated by PyTorch, and 308.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:39:24[INFO] gpu-0 is busy, wait 300s
20240223 16:41:04[INFO] gpu-2 is busy, wait 300s
20240223 16:42:20[INFO] gpu-1 is busy, wait 300s
20240223 16:44:08[INFO] gpu-6 is busy, wait 300s
20240223 16:44:40[INFO] gpu-0 is busy, wait 300s
20240223 16:46:04[INFO] gpu-2 is busy, wait 300s
20240223 16:47:21[INFO] gpu-1 is busy, wait 300s
20240223 16:49:23[INFO] gpu-6 is busy, wait 300s
Process Process-1:30540:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 356.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 322.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2574380 has 2.42 GiB memory in use. Of the allocated memory 1.70 GiB is allocated by PyTorch, and 285.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30541:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 242.94 MiB is free. Process 2485445 has 10.45 GiB memory in use. Process 3206896 has 10.45 GiB memory in use. Process 2575598 has 2.50 GiB memory in use. Of the allocated memory 1.77 GiB is allocated by PyTorch, and 290.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 16:49:45[INFO] gpu-0 is busy, wait 300s
20240223 16:51:05[INFO] gpu-2 is busy, wait 300s
20240223 16:52:22[INFO] gpu-1 is busy, wait 300s
20240223 16:54:29[INFO] gpu-6 is busy, wait 300s
20240223 16:54:53[INFO] gpu-0 is busy, wait 300s
20240223 16:57:23[INFO] gpu-1 is busy, wait 300s
20240223 16:59:36[INFO] gpu-6 is busy, wait 300s
20240223 17:00:00[INFO] gpu-0 is busy, wait 300s
20240223 17:02:24[INFO] gpu-1 is busy, wait 300s
20240223 17:04:44[INFO] gpu-4 is busy, wait 300s
20240223 17:04:51[INFO] gpu-6 is busy, wait 300s
20240223 17:05:39[INFO] gpu-0 is busy, wait 300s
20240223 17:07:24[INFO] gpu-1 is busy, wait 300s
20240223 17:09:45[INFO] gpu-4 is busy, wait 300s
20240223 17:10:11[INFO] gpu-6 is busy, wait 300s
20240223 17:10:47[INFO] gpu-0 is busy, wait 300s
20240223 17:12:25[INFO] gpu-1 is busy, wait 300s
20240223 17:15:31[INFO] gpu-6 is busy, wait 300s
20240223 17:15:59[INFO] gpu-2 is busy, wait 300s
20240223 17:16:03[INFO] gpu-0 is busy, wait 300s
20240223 17:17:26[INFO] gpu-1 is busy, wait 300s
20240223 17:20:52[INFO] gpu-6 is busy, wait 300s
20240223 17:20:59[INFO] gpu-2 is busy, wait 300s
20240223 17:21:17[INFO] gpu-0 is busy, wait 300s
20240223 17:22:26[INFO] gpu-1 is busy, wait 300s
20240223 17:25:49[INFO] gpu-4 is busy, wait 300s
20240223 17:26:00[INFO] gpu-2 is busy, wait 300s
20240223 17:26:20[INFO] gpu-6 is busy, wait 300s
20240223 17:26:23[INFO] gpu-0 is busy, wait 300s
20240223 17:27:27[INFO] gpu-1 is busy, wait 300s
20240223 17:30:31[INFO] gpu-3 is busy, wait 300s
20240223 17:30:50[INFO] gpu-4 is busy, wait 300s
20240223 17:31:01[INFO] gpu-2 is busy, wait 300s
Process Process-1:30556:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 454.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 234.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.22 GiB memory in use. Process 3544239 has 2.96 GiB memory in use. Of the allocated memory 2.16 GiB is allocated by PyTorch, and 362.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 17:31:25[INFO] gpu-0 is busy, wait 300s
20240223 17:31:39[INFO] gpu-6 is busy, wait 300s
20240223 17:32:27[INFO] gpu-1 is busy, wait 300s
20240223 17:35:55[INFO] gpu-4 is busy, wait 300s
20240223 17:36:01[INFO] gpu-2 is busy, wait 300s
20240223 17:36:32[INFO] gpu-0 is busy, wait 300s
20240223 17:36:49[INFO] gpu-6 is busy, wait 300s
20240223 17:37:28[INFO] gpu-1 is busy, wait 300s
20240223 17:37:47[INFO] gpu-3 is busy, wait 300s
20240223 17:41:02[INFO] gpu-2 is busy, wait 300s
Process Process-1:30558:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 596.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.22 GiB memory in use. Process 3794410 has 3.16 GiB memory in use. Of the allocated memory 2.70 GiB is allocated by PyTorch, and 18.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 17:41:34[INFO] gpu-0 is busy, wait 300s
20240223 17:42:03[INFO] gpu-6 is busy, wait 300s
20240223 17:42:29[INFO] gpu-1 is busy, wait 300s
20240223 17:46:08[INFO] gpu-2 is busy, wait 300s
Process Process-1:30562:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

20240223 17:46:52[INFO] gpu-0 is busy, wait 300s
20240223 17:47:18[INFO] gpu-6 is busy, wait 300s
20240223 17:47:29[INFO] gpu-1 is busy, wait 300s
20240223 17:51:09[INFO] gpu-2 is busy, wait 300s
20240223 17:52:00[INFO] gpu-0 is busy, wait 300s
20240223 17:52:30[INFO] gpu-1 is busy, wait 300s
20240223 17:53:12[INFO] gpu-6 is busy, wait 300s
20240223 17:56:09[INFO] gpu-2 is busy, wait 300s
Process Process-1:30564:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 4164266 has 2.72 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 14.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30565:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 534.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 86.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 4164952 has 2.88 GiB memory in use. Of the allocated memory 2.42 GiB is allocated by PyTorch, and 19.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30566:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 598.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 448.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 4165634 has 2.53 GiB memory in use. Of the allocated memory 2.13 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 17:57:06[INFO] gpu-0 is busy, wait 300s
20240223 17:57:31[INFO] gpu-1 is busy, wait 300s
20240223 17:58:19[INFO] gpu-6 is busy, wait 300s
20240223 18:01:10[INFO] gpu-2 is busy, wait 300s
20240223 18:02:12[INFO] gpu-0 is busy, wait 300s
20240223 18:02:32[INFO] gpu-1 is busy, wait 300s
20240223 18:03:31[INFO] gpu-6 is busy, wait 300s
20240223 18:06:11[INFO] gpu-2 is busy, wait 300s
Process Process-1:30568:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 460.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 428.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 178053 has 2.55 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:07:22[INFO] gpu-0 is busy, wait 300s
20240223 18:07:32[INFO] gpu-1 is busy, wait 300s
20240223 18:09:17[INFO] gpu-6 is busy, wait 300s
20240223 18:11:12[INFO] gpu-2 is busy, wait 300s
Process Process-1:30570:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 460.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 428.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 289021 has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 21.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:12:33[INFO] gpu-1 is busy, wait 300s
20240223 18:12:38[INFO] gpu-0 is busy, wait 300s
20240223 18:14:22[INFO] gpu-6 is busy, wait 300s
20240223 18:16:12[INFO] gpu-2 is busy, wait 300s
20240223 18:17:34[INFO] gpu-1 is busy, wait 300s
20240223 18:17:52[INFO] gpu-0 is busy, wait 300s
20240223 18:19:27[INFO] gpu-6 is busy, wait 300s
20240223 18:21:13[INFO] gpu-2 is busy, wait 300s
20240223 18:22:34[INFO] gpu-1 is busy, wait 300s
Process Process-1:30576:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 618.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 376.94 MiB is free. Process 3161911 has 10.22 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 532459 has 2.60 GiB memory in use. Of the allocated memory 2.19 GiB is allocated by PyTorch, and 17.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:23:00[INFO] gpu-0 is busy, wait 300s
20240223 18:24:35[INFO] gpu-6 is busy, wait 300s
20240223 18:26:13[INFO] gpu-2 is busy, wait 300s
20240223 18:27:35[INFO] gpu-1 is busy, wait 300s
Process Process-1:30578:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 152.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 654990 has 2.59 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 304.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:28:04[INFO] gpu-0 is busy, wait 300s
20240223 18:29:43[INFO] gpu-6 is busy, wait 300s
20240223 18:31:14[INFO] gpu-2 is busy, wait 300s
20240223 18:32:36[INFO] gpu-1 is busy, wait 300s
Process Process-1:30579:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 761771 has 2.42 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:33:14[INFO] gpu-0 is busy, wait 300s
20240223 18:35:07[INFO] gpu-6 is busy, wait 300s
20240223 18:36:15[INFO] gpu-2 is busy, wait 300s
20240223 18:37:36[INFO] gpu-1 is busy, wait 300s
20240223 18:38:18[INFO] gpu-0 is busy, wait 300s
20240223 18:40:20[INFO] gpu-6 is busy, wait 300s
20240223 18:41:16[INFO] gpu-2 is busy, wait 300s
20240223 18:42:37[INFO] gpu-1 is busy, wait 300s
Process Process-1:30582:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 342.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 978243 has 2.68 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 406.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30584:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 614.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 158.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 981875 has 2.58 GiB memory in use. Of the allocated memory 2.18 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:43:29[INFO] gpu-0 is busy, wait 300s
20240223 18:45:32[INFO] gpu-6 is busy, wait 300s
20240223 18:46:17[INFO] gpu-2 is busy, wait 300s
20240223 18:47:38[INFO] gpu-1 is busy, wait 300s
Process Process-1:30585:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 254.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1099185 has 2.49 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 289.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:48:39[INFO] gpu-0 is busy, wait 300s
20240223 18:50:49[INFO] gpu-6 is busy, wait 300s
20240223 18:51:17[INFO] gpu-2 is busy, wait 300s
20240223 18:52:38[INFO] gpu-1 is busy, wait 300s
20240223 18:53:58[INFO] gpu-0 is busy, wait 300s
20240223 18:56:06[INFO] gpu-6 is busy, wait 300s
20240223 18:56:18[INFO] gpu-2 is busy, wait 300s
20240223 18:57:39[INFO] gpu-1 is busy, wait 300s
Process Process-1:30591:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 438.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 298.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1343689 has 2.45 GiB memory in use. Of the allocated memory 1.98 GiB is allocated by PyTorch, and 19.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30592:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 614.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 158.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1344462 has 2.58 GiB memory in use. Of the allocated memory 2.18 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 18:59:10[INFO] gpu-0 is busy, wait 300s
20240223 19:01:13[INFO] gpu-6 is busy, wait 300s
20240223 19:01:19[INFO] gpu-2 is busy, wait 300s
20240223 19:02:40[INFO] gpu-1 is busy, wait 300s
20240223 19:04:14[INFO] gpu-0 is busy, wait 300s
20240223 19:06:20[INFO] gpu-2 is busy, wait 300s
20240223 19:06:35[INFO] gpu-6 is busy, wait 300s
20240223 19:07:40[INFO] gpu-1 is busy, wait 300s
Process Process-1:30595:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1531815 has 2.67 GiB memory in use. Of the allocated memory 2.21 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:09:24[INFO] gpu-0 is busy, wait 300s
20240223 19:11:20[INFO] gpu-2 is busy, wait 300s
20240223 19:12:12[INFO] gpu-6 is busy, wait 300s
20240223 19:12:42[INFO] gpu-1 is busy, wait 300s
Process Process-1:30597:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1646155 has 2.68 GiB memory in use. Of the allocated memory 2.28 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:14:30[INFO] gpu-0 is busy, wait 300s
20240223 19:16:21[INFO] gpu-2 is busy, wait 300s
20240223 19:17:19[INFO] gpu-6 is busy, wait 300s
20240223 19:17:43[INFO] gpu-1 is busy, wait 300s
20240223 19:19:38[INFO] gpu-0 is busy, wait 300s
20240223 19:21:21[INFO] gpu-2 is busy, wait 300s
20240223 19:22:25[INFO] gpu-6 is busy, wait 300s
20240223 19:22:43[INFO] gpu-1 is busy, wait 300s
Process Process-1:30599:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 424.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 362.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1889937 has 2.38 GiB memory in use. Of the allocated memory 1.92 GiB is allocated by PyTorch, and 20.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30600:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1890954 has 2.64 GiB memory in use. Of the allocated memory 2.19 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30601:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 208.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1891763 has 2.54 GiB memory in use. Of the allocated memory 1.80 GiB is allocated by PyTorch, and 297.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30602:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 1892465 has 2.73 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 20.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:24:45[INFO] gpu-0 is busy, wait 300s
20240223 19:26:22[INFO] gpu-2 is busy, wait 300s
20240223 19:27:32[INFO] gpu-6 is busy, wait 300s
20240223 19:27:44[INFO] gpu-1 is busy, wait 300s
Process Process-1:30603:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

20240223 19:29:47[INFO] gpu-0 is busy, wait 300s
20240223 19:31:23[INFO] gpu-2 is busy, wait 300s
20240223 19:32:40[INFO] gpu-6 is busy, wait 300s
20240223 19:32:45[INFO] gpu-1 is busy, wait 300s
Process Process-1:30604:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 602.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 200.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2145607 has 2.54 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 15.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:34:48[INFO] gpu-0 is busy, wait 300s
20240223 19:36:24[INFO] gpu-2 is busy, wait 300s
20240223 19:37:46[INFO] gpu-1 is busy, wait 300s
20240223 19:37:55[INFO] gpu-6 is busy, wait 300s
Process Process-1:30607:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 342.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2268528 has 2.68 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 398.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30608:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 466.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 168.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2269532 has 2.57 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30610:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 258.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2272783 has 2.49 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 15.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:40:27[INFO] gpu-0 is busy, wait 300s
20240223 19:41:25[INFO] gpu-2 is busy, wait 300s
20240223 19:42:47[INFO] gpu-1 is busy, wait 300s
20240223 19:43:08[INFO] gpu-6 is busy, wait 300s
Process Process-1:30613:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 242.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2395892 has 2.50 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 14.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30615:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2398048 has 2.42 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:45:35[INFO] gpu-0 is busy, wait 300s
20240223 19:46:25[INFO] gpu-2 is busy, wait 300s
20240223 19:47:47[INFO] gpu-1 is busy, wait 300s
20240223 19:48:52[INFO] gpu-6 is busy, wait 300s
Process Process-1:30616:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2515791 has 2.70 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 13.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Process Process-1:30617:
Traceback (most recent call last):
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 94, in use_gpu
    y = SA(x)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A100/team/mm/wangtao/projects/gpu_idle/main_g.py", line 78, in forward
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 362.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 288.94 MiB is free. Process 3161911 has 10.45 GiB memory in use. Process 3166468 has 10.45 GiB memory in use. Process 2516630 has 2.46 GiB memory in use. Of the allocated memory 1.73 GiB is allocated by PyTorch, and 285.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
20240223 19:50:40[INFO] gpu-0 is busy, wait 300s
20240223 19:51:27[INFO] gpu-2 is busy, wait 300s
20240223 19:52:48[INFO] gpu-1 is busy, wait 300s
20240223 19:54:08[INFO] gpu-6 is busy, wait 300s
20240223 19:55:46[INFO] gpu-0 is busy, wait 300s
20240223 19:56:32[INFO] gpu-2 is busy, wait 300s
20240223 19:57:49[INFO] gpu-1 is busy, wait 300s
20240223 19:59:16[INFO] gpu-6 is busy, wait 300s
20240223 20:01:00[INFO] gpu-0 is busy, wait 300s
20240223 20:01:37[INFO] gpu-2 is busy, wait 300s
20240223 20:02:50[INFO] gpu-1 is busy, wait 300s
20240223 20:04:22[INFO] gpu-6 is busy, wait 300s
20240223 20:06:07[INFO] gpu-0 is busy, wait 300s
20240223 20:06:56[INFO] gpu-2 is busy, wait 300s
20240223 20:07:51[INFO] gpu-1 is busy, wait 300s
20240223 20:09:26[INFO] gpu-6 is busy, wait 300s
20240223 20:11:14[INFO] gpu-0 is busy, wait 300s
20240223 20:12:09[INFO] gpu-2 is busy, wait 300s
20240223 20:12:52[INFO] gpu-1 is busy, wait 300s
20240223 20:14:35[INFO] gpu-6 is busy, wait 300s
20240223 20:16:21[INFO] gpu-0 is busy, wait 300s
20240223 20:17:23[INFO] gpu-2 is busy, wait 300s
20240223 20:17:53[INFO] gpu-1 is busy, wait 300s
20240223 20:19:59[INFO] gpu-6 is busy, wait 300s
20240223 20:21:38[INFO] gpu-0 is busy, wait 300s
20240223 20:22:32[INFO] gpu-2 is busy, wait 300s
20240223 20:22:53[INFO] gpu-1 is busy, wait 300s
20240223 20:25:19[INFO] gpu-6 is busy, wait 300s
20240223 20:26:54[INFO] gpu-0 is busy, wait 300s
20240223 20:27:44[INFO] gpu-2 is busy, wait 300s
20240223 20:27:54[INFO] gpu-1 is busy, wait 300s
20240223 20:30:23[INFO] gpu-6 is busy, wait 300s
20240223 20:32:00[INFO] gpu-0 is busy, wait 300s
20240223 20:32:55[INFO] gpu-1 is busy, wait 300s
20240223 20:33:03[INFO] gpu-2 is busy, wait 300s
20240223 20:35:31[INFO] gpu-6 is busy, wait 300s
20240223 20:37:08[INFO] gpu-0 is busy, wait 300s
20240223 20:37:56[INFO] gpu-1 is busy, wait 300s
20240223 20:38:08[INFO] gpu-2 is busy, wait 300s
20240223 20:40:39[INFO] gpu-6 is busy, wait 300s
20240223 20:42:14[INFO] gpu-0 is busy, wait 300s
20240223 20:43:13[INFO] gpu-2 is busy, wait 300s
20240223 20:46:07[INFO] gpu-6 is busy, wait 300s
20240223 20:48:21[INFO] gpu-2 is busy, wait 300s
20240223 20:51:14[INFO] gpu-6 is busy, wait 300s
20240223 20:53:36[INFO] gpu-2 is busy, wait 300s
20240223 20:56:47[INFO] gpu-6 is busy, wait 300s
20240223 20:59:26[INFO] gpu-2 is busy, wait 300s
20240223 21:02:03[INFO] gpu-6 is busy, wait 300s
20240223 21:04:31[INFO] gpu-2 is busy, wait 300s
20240223 21:07:14[INFO] gpu-6 is busy, wait 300s
20240223 21:09:41[INFO] gpu-2 is busy, wait 300s
20240223 21:12:19[INFO] gpu-6 is busy, wait 300s
20240223 21:14:46[INFO] gpu-2 is busy, wait 300s
20240223 21:17:58[INFO] gpu-6 is busy, wait 300s
20240223 21:19:54[INFO] gpu-2 is busy, wait 300s
20240223 21:23:24[INFO] gpu-6 is busy, wait 300s
20240223 21:25:08[INFO] gpu-2 is busy, wait 300s
20240223 21:29:02[INFO] gpu-6 is busy, wait 300s
20240223 21:30:54[INFO] gpu-2 is busy, wait 300s
20240223 21:34:26[INFO] gpu-6 is busy, wait 300s
20240223 21:36:13[INFO] gpu-2 is busy, wait 300s
20240223 21:39:39[INFO] gpu-6 is busy, wait 300s
20240223 21:41:36[INFO] gpu-2 is busy, wait 300s
20240223 21:44:57[INFO] gpu-6 is busy, wait 300s
20240223 21:46:49[INFO] gpu-2 is busy, wait 300s
20240223 21:50:15[INFO] gpu-6 is busy, wait 300s
20240223 21:52:05[INFO] gpu-2 is busy, wait 300s
20240223 21:55:22[INFO] gpu-6 is busy, wait 300s
20240223 21:57:43[INFO] gpu-2 is busy, wait 300s
20240223 22:00:38[INFO] gpu-6 is busy, wait 300s
20240223 22:02:48[INFO] gpu-2 is busy, wait 300s
20240223 22:05:52[INFO] gpu-6 is busy, wait 300s
20240223 22:08:16[INFO] gpu-2 is busy, wait 300s
20240223 22:11:37[INFO] gpu-6 is busy, wait 300s
20240223 22:14:21[INFO] gpu-2 is busy, wait 300s
20240223 22:16:42[INFO] gpu-6 is busy, wait 300s
20240223 22:19:37[INFO] gpu-2 is busy, wait 300s
20240223 22:21:48[INFO] gpu-6 is busy, wait 300s
20240223 22:24:50[INFO] gpu-2 is busy, wait 300s
20240223 22:27:14[INFO] gpu-6 is busy, wait 300s
20240223 22:30:05[INFO] gpu-2 is busy, wait 300s
20240223 22:32:37[INFO] gpu-6 is busy, wait 300s
20240223 22:35:12[INFO] gpu-2 is busy, wait 300s
20240223 22:37:45[INFO] gpu-6 is busy, wait 300s
20240223 22:40:33[INFO] gpu-2 is busy, wait 300s
20240223 22:42:52[INFO] gpu-6 is busy, wait 300s
20240223 22:48:30[INFO] gpu-6 is busy, wait 300s
20240223 22:53:52[INFO] gpu-6 is busy, wait 300s
20240223 22:59:04[INFO] gpu-6 is busy, wait 300s
20240223 23:04:17[INFO] gpu-6 is busy, wait 300s
20240223 23:09:34[INFO] gpu-6 is busy, wait 300s
20240223 23:14:39[INFO] gpu-6 is busy, wait 300s
20240223 23:19:53[INFO] gpu-6 is busy, wait 300s
20240223 23:24:59[INFO] gpu-6 is busy, wait 300s
20240223 23:30:05[INFO] gpu-6 is busy, wait 300s
20240223 23:36:17[INFO] gpu-6 is busy, wait 300s
20240223 23:41:45[INFO] gpu-6 is busy, wait 300s
20240223 23:47:05[INFO] gpu-6 is busy, wait 300s
20240223 23:52:20[INFO] gpu-6 is busy, wait 300s
20240223 23:57:27[INFO] gpu-6 is busy, wait 300s
20240224 00:02:33[INFO] gpu-6 is busy, wait 300s
20240224 00:07:59[INFO] gpu-6 is busy, wait 300s
20240224 00:13:06[INFO] gpu-6 is busy, wait 300s
20240224 00:18:17[INFO] gpu-6 is busy, wait 300s
20240224 00:24:00[INFO] gpu-6 is busy, wait 300s
20240224 00:29:36[INFO] gpu-6 is busy, wait 300s
20240224 00:34:42[INFO] gpu-6 is busy, wait 300s
20240224 00:39:58[INFO] gpu-6 is busy, wait 300s
20240224 00:45:09[INFO] gpu-6 is busy, wait 300s
20240224 00:50:55[INFO] gpu-6 is busy, wait 300s
20240224 00:56:08[INFO] gpu-6 is busy, wait 300s
20240224 01:01:16[INFO] gpu-6 is busy, wait 300s
20240224 01:06:30[INFO] gpu-6 is busy, wait 300s
20240224 01:12:02[INFO] gpu-6 is busy, wait 300s
20240224 01:17:29[INFO] gpu-6 is busy, wait 300s
20240224 01:23:05[INFO] gpu-6 is busy, wait 300s
20240224 01:28:30[INFO] gpu-6 is busy, wait 300s
20240224 01:33:36[INFO] gpu-6 is busy, wait 300s
20240224 01:38:44[INFO] gpu-6 is busy, wait 300s
20240224 01:44:04[INFO] gpu-6 is busy, wait 300s
20240224 01:49:12[INFO] gpu-6 is busy, wait 300s
20240224 01:55:11[INFO] gpu-6 is busy, wait 300s
20240224 02:00:44[INFO] gpu-6 is busy, wait 300s
20240224 02:06:41[INFO] gpu-6 is busy, wait 300s
20240224 02:12:19[INFO] gpu-6 is busy, wait 300s
20240224 02:17:27[INFO] gpu-6 is busy, wait 300s
20240224 02:22:34[INFO] gpu-6 is busy, wait 300s
20240224 02:27:49[INFO] gpu-6 is busy, wait 300s
20240224 02:33:31[INFO] gpu-6 is busy, wait 300s
20240224 02:38:54[INFO] gpu-6 is busy, wait 300s
20240224 02:45:00[INFO] gpu-6 is busy, wait 300s
20240224 02:50:16[INFO] gpu-6 is busy, wait 300s
20240224 02:55:25[INFO] gpu-6 is busy, wait 300s
20240224 03:00:32[INFO] gpu-6 is busy, wait 300s
20240224 03:05:37[INFO] gpu-6 is busy, wait 300s
20240224 03:10:59[INFO] gpu-6 is busy, wait 300s
20240224 03:16:07[INFO] gpu-6 is busy, wait 300s
20240224 03:21:14[INFO] gpu-6 is busy, wait 300s
20240224 03:26:26[INFO] gpu-6 is busy, wait 300s
20240224 03:31:39[INFO] gpu-6 is busy, wait 300s
20240224 03:37:43[INFO] gpu-6 is busy, wait 300s
20240224 03:42:50[INFO] gpu-6 is busy, wait 300s
20240224 03:48:05[INFO] gpu-6 is busy, wait 300s
20240224 03:53:36[INFO] gpu-6 is busy, wait 300s
20240224 03:58:59[INFO] gpu-6 is busy, wait 300s
20240224 04:04:08[INFO] gpu-6 is busy, wait 300s
20240224 04:09:14[INFO] gpu-6 is busy, wait 300s
20240224 04:14:21[INFO] gpu-6 is busy, wait 300s
20240224 04:20:08[INFO] gpu-6 is busy, wait 300s
20240224 04:25:15[INFO] gpu-6 is busy, wait 300s
20240224 04:30:33[INFO] gpu-6 is busy, wait 300s
20240224 04:35:41[INFO] gpu-6 is busy, wait 300s
20240224 04:41:05[INFO] gpu-6 is busy, wait 300s
20240224 04:46:15[INFO] gpu-6 is busy, wait 300s
20240224 04:51:54[INFO] gpu-6 is busy, wait 300s
20240224 04:57:01[INFO] gpu-6 is busy, wait 300s
20240224 05:02:25[INFO] gpu-6 is busy, wait 300s
20240224 05:07:37[INFO] gpu-6 is busy, wait 300s
20240224 05:12:57[INFO] gpu-6 is busy, wait 300s
20240224 05:18:40[INFO] gpu-6 is busy, wait 300s
20240224 05:24:05[INFO] gpu-6 is busy, wait 300s
20240224 05:29:20[INFO] gpu-6 is busy, wait 300s
20240224 05:34:35[INFO] gpu-6 is busy, wait 300s
20240224 05:39:57[INFO] gpu-6 is busy, wait 300s
20240224 05:45:27[INFO] gpu-6 is busy, wait 300s
20240224 05:50:35[INFO] gpu-6 is busy, wait 300s
20240224 05:55:41[INFO] gpu-6 is busy, wait 300s
20240224 06:01:52[INFO] gpu-6 is busy, wait 300s
20240224 06:06:59[INFO] gpu-6 is busy, wait 300s
20240224 06:12:06[INFO] gpu-6 is busy, wait 300s
20240224 06:17:12[INFO] gpu-6 is busy, wait 300s
20240224 06:22:20[INFO] gpu-6 is busy, wait 300s
20240224 06:27:37[INFO] gpu-6 is busy, wait 300s
20240224 06:32:50[INFO] gpu-6 is busy, wait 300s
20240224 06:38:51[INFO] gpu-6 is busy, wait 300s
20240224 06:44:14[INFO] gpu-6 is busy, wait 300s
20240224 06:49:22[INFO] gpu-6 is busy, wait 300s
20240224 06:54:42[INFO] gpu-6 is busy, wait 300s
20240224 07:00:49[INFO] gpu-6 is busy, wait 300s
20240224 07:06:53[INFO] gpu-6 is busy, wait 300s
20240224 07:11:58[INFO] gpu-6 is busy, wait 300s
20240224 07:17:12[INFO] gpu-6 is busy, wait 300s
20240224 07:22:17[INFO] gpu-6 is busy, wait 300s
20240224 07:28:55[INFO] gpu-6 is busy, wait 300s
20240224 07:34:01[INFO] gpu-6 is busy, wait 300s
20240224 07:39:51[INFO] gpu-6 is busy, wait 300s
20240224 07:45:07[INFO] gpu-6 is busy, wait 300s
20240224 07:50:16[INFO] gpu-6 is busy, wait 300s
20240224 07:55:30[INFO] gpu-6 is busy, wait 300s
20240224 08:01:00[INFO] gpu-6 is busy, wait 300s
20240224 08:06:15[INFO] gpu-6 is busy, wait 300s
20240224 08:11:37[INFO] gpu-6 is busy, wait 300s
20240224 08:17:03[INFO] gpu-6 is busy, wait 300s
20240224 08:22:09[INFO] gpu-6 is busy, wait 300s
20240224 08:28:30[INFO] gpu-6 is busy, wait 300s
20240224 08:33:38[INFO] gpu-6 is busy, wait 300s
20240224 08:38:43[INFO] gpu-6 is busy, wait 300s
20240224 08:43:56[INFO] gpu-6 is busy, wait 300s
20240224 08:49:06[INFO] gpu-6 is busy, wait 300s
20240224 08:54:33[INFO] gpu-6 is busy, wait 300s
20240224 08:59:38[INFO] gpu-6 is busy, wait 300s
20240224 09:04:53[INFO] gpu-6 is busy, wait 300s
20240224 09:10:08[INFO] gpu-6 is busy, wait 300s
20240224 09:15:43[INFO] gpu-6 is busy, wait 300s
20240224 09:21:15[INFO] gpu-6 is busy, wait 300s
20240224 09:26:21[INFO] gpu-6 is busy, wait 300s
20240224 09:31:39[INFO] gpu-6 is busy, wait 300s
20240224 09:37:14[INFO] gpu-6 is busy, wait 300s
20240224 09:42:30[INFO] gpu-6 is busy, wait 300s
20240224 09:47:48[INFO] gpu-6 is busy, wait 300s
20240224 09:53:10[INFO] gpu-6 is busy, wait 300s
20240224 09:58:17[INFO] gpu-6 is busy, wait 300s
20240224 10:04:10[INFO] gpu-6 is busy, wait 300s
20240224 10:10:09[INFO] gpu-6 is busy, wait 300s
20240224 10:15:25[INFO] gpu-6 is busy, wait 300s
20240224 10:20:32[INFO] gpu-6 is busy, wait 300s
20240224 10:25:39[INFO] gpu-6 is busy, wait 300s
20240224 10:31:06[INFO] gpu-6 is busy, wait 300s
20240224 10:36:20[INFO] gpu-6 is busy, wait 300s
20240224 10:41:42[INFO] gpu-6 is busy, wait 300s
20240224 10:47:07[INFO] gpu-6 is busy, wait 300s
20240224 10:52:17[INFO] gpu-6 is busy, wait 300s
20240224 10:57:22[INFO] gpu-6 is busy, wait 300s
20240224 11:02:39[INFO] gpu-6 is busy, wait 300s
20240224 11:08:47[INFO] gpu-6 is busy, wait 300s
20240224 11:14:23[INFO] gpu-6 is busy, wait 300s
20240224 11:19:29[INFO] gpu-6 is busy, wait 300s
20240224 11:24:34[INFO] gpu-6 is busy, wait 300s
20240224 11:29:53[INFO] gpu-6 is busy, wait 300s
20240224 11:34:59[INFO] gpu-6 is busy, wait 300s
20240224 11:40:47[INFO] gpu-6 is busy, wait 300s
20240224 11:45:53[INFO] gpu-6 is busy, wait 300s
20240224 11:51:22[INFO] gpu-6 is busy, wait 300s
20240224 11:56:38[INFO] gpu-6 is busy, wait 300s
20240224 12:01:54[INFO] gpu-6 is busy, wait 300s
20240224 12:07:25[INFO] gpu-6 is busy, wait 300s
20240224 12:12:55[INFO] gpu-6 is busy, wait 300s
20240224 12:18:10[INFO] gpu-6 is busy, wait 300s
20240224 12:23:43[INFO] gpu-6 is busy, wait 300s
20240224 12:29:05[INFO] gpu-6 is busy, wait 300s
20240224 12:34:31[INFO] gpu-6 is busy, wait 300s
20240224 12:39:43[INFO] gpu-6 is busy, wait 300s
20240224 12:45:14[INFO] gpu-6 is busy, wait 300s
20240224 12:50:26[INFO] gpu-6 is busy, wait 300s
20240224 12:55:34[INFO] gpu-6 is busy, wait 300s
20240224 13:00:40[INFO] gpu-6 is busy, wait 300s
20240224 13:05:45[INFO] gpu-6 is busy, wait 300s
20240224 13:10:59[INFO] gpu-6 is busy, wait 300s
20240224 13:16:07[INFO] gpu-6 is busy, wait 300s
20240224 13:21:13[INFO] gpu-6 is busy, wait 300s
20240224 13:26:47[INFO] gpu-6 is busy, wait 300s
20240224 13:32:26[INFO] gpu-6 is busy, wait 300s
20240224 13:38:02[INFO] gpu-6 is busy, wait 300s
20240224 13:43:15[INFO] gpu-6 is busy, wait 300s
20240224 13:48:20[INFO] gpu-6 is busy, wait 300s
20240224 13:53:50[INFO] gpu-6 is busy, wait 300s
20240224 13:58:55[INFO] gpu-6 is busy, wait 300s
20240224 14:04:03[INFO] gpu-6 is busy, wait 300s
20240224 14:09:11[INFO] gpu-6 is busy, wait 300s
20240224 14:14:18[INFO] gpu-6 is busy, wait 300s
20240224 14:19:31[INFO] gpu-6 is busy, wait 300s
20240224 14:25:04[INFO] gpu-6 is busy, wait 300s
20240224 14:30:29[INFO] gpu-6 is busy, wait 300s
20240224 14:35:43[INFO] gpu-6 is busy, wait 300s
20240224 14:40:51[INFO] gpu-6 is busy, wait 300s
20240224 14:45:58[INFO] gpu-6 is busy, wait 300s
20240224 14:52:11[INFO] gpu-6 is busy, wait 300s
20240224 14:57:28[INFO] gpu-6 is busy, wait 300s
20240224 15:02:36[INFO] gpu-6 is busy, wait 300s
20240224 15:08:45[INFO] gpu-6 is busy, wait 300s
20240224 15:13:51[INFO] gpu-6 is busy, wait 300s
20240224 15:19:09[INFO] gpu-6 is busy, wait 300s
20240224 15:24:29[INFO] gpu-6 is busy, wait 300s
20240224 15:29:37[INFO] gpu-6 is busy, wait 300s
20240224 15:34:51[INFO] gpu-6 is busy, wait 300s
20240224 15:40:04[INFO] gpu-6 is busy, wait 300s
20240224 15:45:35[INFO] gpu-6 is busy, wait 300s
20240224 15:51:10[INFO] gpu-6 is busy, wait 300s
20240224 15:56:26[INFO] gpu-6 is busy, wait 300s
20240224 16:01:31[INFO] gpu-6 is busy, wait 300s
20240224 16:06:38[INFO] gpu-6 is busy, wait 300s
20240224 16:11:45[INFO] gpu-6 is busy, wait 300s
20240224 16:17:11[INFO] gpu-6 is busy, wait 300s
20240224 16:22:30[INFO] gpu-6 is busy, wait 300s
20240224 16:27:37[INFO] gpu-6 is busy, wait 300s
20240224 16:32:57[INFO] gpu-6 is busy, wait 300s
20240224 16:38:03[INFO] gpu-6 is busy, wait 300s
20240224 16:43:19[INFO] gpu-6 is busy, wait 300s
20240224 16:48:31[INFO] gpu-6 is busy, wait 300s
20240224 16:53:38[INFO] gpu-6 is busy, wait 300s
20240224 16:58:59[INFO] gpu-6 is busy, wait 300s
20240224 17:04:36[INFO] gpu-6 is busy, wait 300s
20240224 17:10:07[INFO] gpu-6 is busy, wait 300s
20240224 17:15:13[INFO] gpu-6 is busy, wait 300s
20240224 17:21:23[INFO] gpu-6 is busy, wait 300s
20240224 17:26:44[INFO] gpu-6 is busy, wait 300s
20240224 17:32:20[INFO] gpu-6 is busy, wait 300s
20240224 17:37:31[INFO] gpu-6 is busy, wait 300s
20240224 17:40:58[INFO] gpu-0 is busy, wait 300s
20240224 17:41:00[INFO] gpu-4 is busy, wait 300s
20240224 17:41:05[INFO] gpu-1 is busy, wait 300s
20240224 17:41:11[INFO] gpu-2 is busy, wait 300s
20240224 17:41:13[INFO] gpu-5 is busy, wait 300s
20240224 17:41:17[INFO] gpu-7 is busy, wait 300s
20240224 17:42:36[INFO] gpu-6 is busy, wait 300s
20240224 17:46:10[INFO] gpu-1 is busy, wait 300s
20240224 17:46:11[INFO] gpu-0 is busy, wait 300s
20240224 17:46:16[INFO] gpu-2 is busy, wait 300s
20240224 17:46:24[INFO] gpu-4 is busy, wait 300s
20240224 17:46:36[INFO] gpu-7 is busy, wait 300s
20240224 17:46:36[INFO] gpu-5 is busy, wait 300s
20240224 17:47:48[INFO] gpu-6 is busy, wait 300s
20240224 17:50:55[INFO] gpu-3 is busy, wait 300s
20240224 17:51:10[INFO] gpu-1 is busy, wait 300s
20240224 17:51:12[INFO] gpu-0 is busy, wait 300s
20240224 17:51:17[INFO] gpu-2 is busy, wait 300s
20240224 17:51:25[INFO] gpu-4 is busy, wait 300s
20240224 17:51:36[INFO] gpu-7 is busy, wait 300s
20240224 17:51:37[INFO] gpu-5 is busy, wait 300s
20240224 17:52:48[INFO] gpu-6 is busy, wait 300s
20240224 17:56:03[INFO] gpu-3 is busy, wait 300s
20240224 17:56:11[INFO] gpu-1 is busy, wait 300s
20240224 17:56:12[INFO] gpu-0 is busy, wait 300s
20240224 17:56:17[INFO] gpu-2 is busy, wait 300s
20240224 17:56:25[INFO] gpu-4 is busy, wait 300s
20240224 17:56:37[INFO] gpu-7 is busy, wait 300s
20240224 17:56:38[INFO] gpu-5 is busy, wait 300s
20240224 17:58:01[INFO] gpu-6 is busy, wait 300s
20240224 18:01:50[INFO] gpu-5 is busy, wait 300s
20240224 18:02:21[INFO] gpu-0 is busy, wait 300s
20240224 18:02:33[INFO] gpu-4 is busy, wait 300s
20240224 18:03:08[INFO] gpu-3 is busy, wait 300s
20240224 18:03:09[INFO] gpu-6 is busy, wait 300s
20240224 18:03:16[INFO] gpu-2 is busy, wait 300s
20240224 18:03:47[INFO] gpu-1 is busy, wait 300s
20240224 18:06:11[INFO] gpu-7 is busy, wait 300s
20240224 18:06:50[INFO] gpu-5 is busy, wait 300s
20240224 18:07:22[INFO] gpu-0 is busy, wait 300s
20240224 18:07:34[INFO] gpu-4 is busy, wait 300s
20240224 18:08:09[INFO] gpu-3 is busy, wait 300s
20240224 18:08:10[INFO] gpu-6 is busy, wait 300s
20240224 18:08:17[INFO] gpu-2 is busy, wait 300s
20240224 18:08:48[INFO] gpu-1 is busy, wait 300s
20240224 18:11:12[INFO] gpu-7 is busy, wait 300s
20240224 18:11:51[INFO] gpu-5 is busy, wait 300s
20240224 18:13:22[INFO] gpu-6 is busy, wait 300s
20240224 18:18:43[INFO] gpu-6 is busy, wait 300s
20240224 18:24:18[INFO] gpu-6 is busy, wait 300s
20240224 18:29:27[INFO] gpu-6 is busy, wait 300s
20240224 18:34:36[INFO] gpu-6 is busy, wait 300s
20240224 18:36:11[INFO] gpu-1 is busy, wait 300s
20240224 18:36:24[INFO] gpu-0 is busy, wait 300s
20240224 18:36:51[INFO] gpu-4 is busy, wait 300s
20240224 18:37:02[INFO] gpu-2 is busy, wait 300s
20240224 18:37:20[INFO] gpu-7 is busy, wait 300s
20240224 18:37:31[INFO] gpu-5 is busy, wait 300s
20240224 18:39:42[INFO] gpu-3 is busy, wait 300s
20240224 18:39:51[INFO] gpu-6 is busy, wait 300s
20240224 18:41:12[INFO] gpu-1 is busy, wait 300s
20240224 18:41:24[INFO] gpu-0 is busy, wait 300s
20240224 18:41:52[INFO] gpu-4 is busy, wait 300s
20240224 18:42:02[INFO] gpu-2 is busy, wait 300s
20240224 18:42:20[INFO] gpu-7 is busy, wait 300s
20240224 18:42:32[INFO] gpu-5 is busy, wait 300s
20240224 18:44:43[INFO] gpu-3 is busy, wait 300s
20240224 18:44:51[INFO] gpu-6 is busy, wait 300s
20240224 18:46:13[INFO] gpu-1 is busy, wait 300s
20240224 18:46:25[INFO] gpu-0 is busy, wait 300s
20240224 18:46:52[INFO] gpu-4 is busy, wait 300s
20240224 18:47:03[INFO] gpu-2 is busy, wait 300s
20240224 18:47:21[INFO] gpu-7 is busy, wait 300s
20240224 18:47:32[INFO] gpu-5 is busy, wait 300s
20240224 18:50:09[INFO] gpu-6 is busy, wait 300s
20240224 18:55:15[INFO] gpu-6 is busy, wait 300s
20240224 19:00:23[INFO] gpu-6 is busy, wait 300s
20240224 19:05:56[INFO] gpu-6 is busy, wait 300s
20240224 19:11:11[INFO] gpu-6 is busy, wait 300s
20240224 19:16:19[INFO] gpu-6 is busy, wait 300s
20240224 19:20:56[INFO] gpu-3 is busy, wait 300s
20240224 19:20:57[INFO] gpu-0 is busy, wait 300s
20240224 19:21:07[INFO] gpu-2 is busy, wait 300s
20240224 19:21:29[INFO] gpu-6 is busy, wait 300s
20240224 19:21:46[INFO] gpu-7 is busy, wait 300s
20240224 19:22:00[INFO] gpu-1 is busy, wait 300s
20240224 19:22:01[INFO] gpu-5 is busy, wait 300s
20240224 19:26:01[INFO] gpu-3 is busy, wait 300s
20240224 19:26:04[INFO] gpu-0 is busy, wait 300s
20240224 19:26:21[INFO] gpu-2 is busy, wait 300s
20240224 19:26:51[INFO] gpu-7 is busy, wait 300s
20240224 19:26:53[INFO] gpu-6 is busy, wait 300s
20240224 19:27:13[INFO] gpu-5 is busy, wait 300s
20240224 19:27:18[INFO] gpu-1 is busy, wait 300s
20240224 19:30:02[INFO] gpu-4 is busy, wait 300s
20240224 19:31:02[INFO] gpu-3 is busy, wait 300s
20240224 19:31:04[INFO] gpu-0 is busy, wait 300s
20240224 19:31:22[INFO] gpu-2 is busy, wait 300s
20240224 19:31:52[INFO] gpu-7 is busy, wait 300s
20240224 19:31:54[INFO] gpu-6 is busy, wait 300s
20240224 19:32:13[INFO] gpu-5 is busy, wait 300s
20240224 19:32:19[INFO] gpu-1 is busy, wait 300s
20240224 19:36:59[INFO] gpu-6 is busy, wait 300s
20240224 19:37:05[INFO] gpu-7 is busy, wait 300s
20240224 19:42:17[INFO] gpu-6 is busy, wait 300s
20240224 19:45:46[INFO] gpu-2 is busy, wait 300s
20240224 19:45:53[INFO] gpu-5 is busy, wait 300s
20240224 19:46:02[INFO] gpu-4 is busy, wait 300s
20240224 19:46:53[INFO] gpu-3 is busy, wait 300s
20240224 19:46:54[INFO] gpu-0 is busy, wait 300s
20240224 19:47:22[INFO] gpu-6 is busy, wait 300s
20240224 19:47:24[INFO] gpu-1 is busy, wait 300s
20240224 19:51:05[INFO] gpu-2 is busy, wait 300s
20240224 19:51:17[INFO] gpu-4 is busy, wait 300s
20240224 19:51:19[INFO] gpu-5 is busy, wait 300s
20240224 19:51:57[INFO] gpu-3 is busy, wait 300s
20240224 19:52:06[INFO] gpu-0 is busy, wait 300s
20240224 19:52:37[INFO] gpu-1 is busy, wait 300s
20240224 19:52:41[INFO] gpu-6 is busy, wait 300s
20240224 19:54:52[INFO] gpu-7 is busy, wait 300s
20240224 19:56:06[INFO] gpu-2 is busy, wait 300s
20240224 19:56:18[INFO] gpu-4 is busy, wait 300s
20240224 19:56:20[INFO] gpu-5 is busy, wait 300s
20240224 19:56:58[INFO] gpu-3 is busy, wait 300s
20240224 19:57:06[INFO] gpu-0 is busy, wait 300s
20240224 19:57:38[INFO] gpu-1 is busy, wait 300s
20240224 19:57:46[INFO] gpu-6 is busy, wait 300s
20240224 19:59:52[INFO] gpu-7 is busy, wait 300s
20240224 20:01:06[INFO] gpu-2 is busy, wait 300s
20240224 20:01:19[INFO] gpu-4 is busy, wait 300s
20240224 20:01:20[INFO] gpu-5 is busy, wait 300s
20240224 20:01:58[INFO] gpu-3 is busy, wait 300s
20240224 20:02:07[INFO] gpu-0 is busy, wait 300s
20240224 20:02:38[INFO] gpu-1 is busy, wait 300s
20240224 20:02:46[INFO] gpu-6 is busy, wait 300s
20240224 20:07:52[INFO] gpu-6 is busy, wait 300s
20240224 20:12:59[INFO] gpu-6 is busy, wait 300s
20240224 20:14:32[INFO] gpu-0 is busy, wait 300s
20240224 20:14:47[INFO] gpu-3 is busy, wait 300s
20240224 20:15:07[INFO] gpu-7 is busy, wait 300s
20240224 20:15:47[INFO] gpu-5 is busy, wait 300s
20240224 20:16:06[INFO] gpu-1 is busy, wait 300s
20240224 20:16:26[INFO] gpu-2 is busy, wait 300s
20240224 20:18:05[INFO] gpu-6 is busy, wait 300s
20240224 20:19:53[INFO] gpu-3 is busy, wait 300s
20240224 20:19:55[INFO] gpu-0 is busy, wait 300s
20240224 20:20:26[INFO] gpu-7 is busy, wait 300s
20240224 20:20:58[INFO] gpu-5 is busy, wait 300s
20240224 20:21:15[INFO] gpu-1 is busy, wait 300s
20240224 20:21:33[INFO] gpu-2 is busy, wait 300s
20240224 20:23:23[INFO] gpu-6 is busy, wait 300s
20240224 20:24:10[INFO] gpu-4 is busy, wait 300s
20240224 20:24:54[INFO] gpu-3 is busy, wait 300s
20240224 20:24:56[INFO] gpu-0 is busy, wait 300s
20240224 20:25:26[INFO] gpu-7 is busy, wait 300s
20240224 20:25:58[INFO] gpu-5 is busy, wait 300s
20240224 20:26:15[INFO] gpu-1 is busy, wait 300s
20240224 20:26:33[INFO] gpu-2 is busy, wait 300s
20240224 20:28:23[INFO] gpu-6 is busy, wait 300s
20240224 20:29:11[INFO] gpu-4 is busy, wait 300s
20240224 20:29:54[INFO] gpu-3 is busy, wait 300s
20240224 20:29:56[INFO] gpu-0 is busy, wait 300s
20240224 20:30:27[INFO] gpu-7 is busy, wait 300s
20240224 20:30:59[INFO] gpu-5 is busy, wait 300s
20240224 20:31:16[INFO] gpu-1 is busy, wait 300s
20240224 20:33:37[INFO] gpu-6 is busy, wait 300s
20240224 20:38:33[INFO] gpu-5 is busy, wait 300s
20240224 20:38:51[INFO] gpu-6 is busy, wait 300s
20240224 20:39:12[INFO] gpu-3 is busy, wait 300s
20240224 20:39:30[INFO] gpu-4 is busy, wait 300s
20240224 20:39:59[INFO] gpu-1 is busy, wait 300s
20240224 20:40:22[INFO] gpu-2 is busy, wait 300s
20240224 20:40:23[INFO] gpu-0 is busy, wait 300s
20240224 20:43:39[INFO] gpu-5 is busy, wait 300s
20240224 20:44:06[INFO] gpu-6 is busy, wait 300s
20240224 20:44:29[INFO] gpu-3 is busy, wait 300s
20240224 20:44:41[INFO] gpu-4 is busy, wait 300s
20240224 20:45:14[INFO] gpu-1 is busy, wait 300s
20240224 20:45:43[INFO] gpu-2 is busy, wait 300s
20240224 20:45:46[INFO] gpu-0 is busy, wait 300s
20240224 20:47:58[INFO] gpu-7 is busy, wait 300s
20240224 20:48:40[INFO] gpu-5 is busy, wait 300s
20240224 20:49:07[INFO] gpu-6 is busy, wait 300s
20240224 20:49:29[INFO] gpu-3 is busy, wait 300s
20240224 20:49:42[INFO] gpu-4 is busy, wait 300s
20240224 20:50:14[INFO] gpu-1 is busy, wait 300s
20240224 20:50:44[INFO] gpu-2 is busy, wait 300s
20240224 20:50:55[INFO] gpu-0 is busy, wait 300s
20240224 20:53:06[INFO] gpu-7 is busy, wait 300s
20240224 20:53:40[INFO] gpu-5 is busy, wait 300s
20240224 20:54:07[INFO] gpu-6 is busy, wait 300s
20240224 20:54:30[INFO] gpu-3 is busy, wait 300s
20240224 20:54:43[INFO] gpu-4 is busy, wait 300s
20240224 20:55:15[INFO] gpu-1 is busy, wait 300s
20240224 20:55:45[INFO] gpu-2 is busy, wait 300s
20240224 20:55:55[INFO] gpu-0 is busy, wait 300s
20240224 20:59:14[INFO] gpu-6 is busy, wait 300s
20240224 21:02:21[INFO] gpu-5 is busy, wait 300s
20240224 21:02:36[INFO] gpu-0 is busy, wait 300s
20240224 21:03:10[INFO] gpu-4 is busy, wait 300s
20240224 21:03:25[INFO] gpu-3 is busy, wait 300s
20240224 21:03:42[INFO] gpu-1 is busy, wait 300s
20240224 21:03:43[INFO] gpu-2 is busy, wait 300s
20240224 21:04:27[INFO] gpu-6 is busy, wait 300s
20240224 21:07:38[INFO] gpu-5 is busy, wait 300s
20240224 21:07:48[INFO] gpu-0 is busy, wait 300s
20240224 21:08:14[INFO] gpu-4 is busy, wait 300s
20240224 21:08:30[INFO] gpu-3 is busy, wait 300s
20240224 21:08:57[INFO] gpu-2 is busy, wait 300s
20240224 21:09:32[INFO] gpu-6 is busy, wait 300s
20240224 21:09:47[INFO] gpu-1 is busy, wait 300s
20240224 21:11:42[INFO] gpu-7 is busy, wait 300s
20240224 21:12:39[INFO] gpu-5 is busy, wait 300s
20240224 21:12:49[INFO] gpu-0 is busy, wait 300s
20240224 21:13:15[INFO] gpu-4 is busy, wait 300s
20240224 21:13:30[INFO] gpu-3 is busy, wait 300s
20240224 21:13:57[INFO] gpu-2 is busy, wait 300s
20240224 21:14:33[INFO] gpu-6 is busy, wait 300s
20240224 21:14:48[INFO] gpu-1 is busy, wait 300s
20240224 21:16:42[INFO] gpu-7 is busy, wait 300s
20240224 21:17:39[INFO] gpu-5 is busy, wait 300s
20240224 21:17:49[INFO] gpu-0 is busy, wait 300s
20240224 21:18:15[INFO] gpu-4 is busy, wait 300s
20240224 21:18:31[INFO] gpu-3 is busy, wait 300s
20240224 21:18:58[INFO] gpu-2 is busy, wait 300s
20240224 21:19:33[INFO] gpu-6 is busy, wait 300s
20240224 21:19:48[INFO] gpu-1 is busy, wait 300s
20240224 21:25:01[INFO] gpu-6 is busy, wait 300s
20240224 21:30:05[INFO] gpu-0 is busy, wait 300s
20240224 21:30:15[INFO] gpu-6 is busy, wait 300s
20240224 21:30:28[INFO] gpu-4 is busy, wait 300s
20240224 21:30:40[INFO] gpu-1 is busy, wait 300s
20240224 21:31:14[INFO] gpu-7 is busy, wait 300s
20240224 21:31:23[INFO] gpu-5 is busy, wait 300s
20240224 21:31:38[INFO] gpu-2 is busy, wait 300s
20240224 21:35:12[INFO] gpu-0 is busy, wait 300s
20240224 21:35:22[INFO] gpu-6 is busy, wait 300s
20240224 21:35:33[INFO] gpu-4 is busy, wait 300s
20240224 21:35:44[INFO] gpu-1 is busy, wait 300s
20240224 21:36:36[INFO] gpu-5 is busy, wait 300s
20240224 21:36:50[INFO] gpu-2 is busy, wait 300s
20240224 21:36:54[INFO] gpu-7 is busy, wait 300s
20240224 21:39:27[INFO] gpu-3 is busy, wait 300s
20240224 21:40:13[INFO] gpu-0 is busy, wait 300s
20240224 21:40:22[INFO] gpu-6 is busy, wait 300s
20240224 21:40:34[INFO] gpu-4 is busy, wait 300s
20240224 21:40:45[INFO] gpu-1 is busy, wait 300s
20240224 21:41:37[INFO] gpu-5 is busy, wait 300s
20240224 21:41:50[INFO] gpu-2 is busy, wait 300s
20240224 21:41:54[INFO] gpu-7 is busy, wait 300s
20240224 21:44:28[INFO] gpu-3 is busy, wait 300s
20240224 21:45:14[INFO] gpu-0 is busy, wait 300s
20240224 21:45:23[INFO] gpu-6 is busy, wait 300s
20240224 21:45:34[INFO] gpu-4 is busy, wait 300s
20240224 21:45:45[INFO] gpu-1 is busy, wait 300s
20240224 21:46:37[INFO] gpu-5 is busy, wait 300s
20240224 21:46:51[INFO] gpu-2 is busy, wait 300s
20240224 21:46:55[INFO] gpu-7 is busy, wait 300s
20240224 21:49:28[INFO] gpu-3 is busy, wait 300s
20240224 21:50:14[INFO] gpu-0 is busy, wait 300s
20240224 21:50:24[INFO] gpu-6 is busy, wait 300s
20240224 21:50:35[INFO] gpu-4 is busy, wait 300s
20240224 21:50:46[INFO] gpu-1 is busy, wait 300s
20240224 21:51:38[INFO] gpu-5 is busy, wait 300s
20240224 21:55:41[INFO] gpu-6 is busy, wait 300s
20240224 21:56:49[INFO] gpu-7 is busy, wait 300s
20240224 21:57:04[INFO] gpu-4 is busy, wait 300s
20240224 21:57:06[INFO] gpu-3 is busy, wait 300s
20240224 21:57:44[INFO] gpu-2 is busy, wait 300s
20240224 21:58:03[INFO] gpu-1 is busy, wait 300s
20240224 21:58:05[INFO] gpu-0 is busy, wait 300s
20240224 22:00:47[INFO] gpu-6 is busy, wait 300s
20240224 22:02:09[INFO] gpu-4 is busy, wait 300s
20240224 22:02:10[INFO] gpu-7 is busy, wait 300s
20240224 22:02:16[INFO] gpu-3 is busy, wait 300s
20240224 22:03:06[INFO] gpu-2 is busy, wait 300s
20240224 22:03:14[INFO] gpu-1 is busy, wait 300s
20240224 22:03:33[INFO] gpu-0 is busy, wait 300s
20240224 22:06:06[INFO] gpu-5 is busy, wait 300s
20240224 22:06:13[INFO] gpu-6 is busy, wait 300s
20240224 22:07:09[INFO] gpu-4 is busy, wait 300s
20240224 22:07:10[INFO] gpu-7 is busy, wait 300s
20240224 22:07:16[INFO] gpu-3 is busy, wait 300s
20240224 22:08:06[INFO] gpu-2 is busy, wait 300s
20240224 22:08:14[INFO] gpu-1 is busy, wait 300s
20240224 22:08:33[INFO] gpu-0 is busy, wait 300s
20240224 22:11:06[INFO] gpu-5 is busy, wait 300s
20240224 22:11:14[INFO] gpu-6 is busy, wait 300s
20240224 22:12:10[INFO] gpu-4 is busy, wait 300s
20240224 22:12:11[INFO] gpu-7 is busy, wait 300s
20240224 22:12:17[INFO] gpu-3 is busy, wait 300s
20240224 22:13:07[INFO] gpu-2 is busy, wait 300s
20240224 22:13:15[INFO] gpu-1 is busy, wait 300s
20240224 22:13:34[INFO] gpu-0 is busy, wait 300s
20240224 22:16:07[INFO] gpu-5 is busy, wait 300s
20240224 22:16:15[INFO] gpu-6 is busy, wait 300s
20240224 22:17:10[INFO] gpu-4 is busy, wait 300s
20240224 22:17:11[INFO] gpu-7 is busy, wait 300s
20240224 22:17:17[INFO] gpu-3 is busy, wait 300s
20240224 22:18:07[INFO] gpu-2 is busy, wait 300s
20240224 22:18:15[INFO] gpu-1 is busy, wait 300s
20240224 22:18:34[INFO] gpu-0 is busy, wait 300s
20240224 22:21:15[INFO] gpu-6 is busy, wait 300s
20240224 22:21:19[INFO] gpu-5 is busy, wait 300s
20240224 22:22:11[INFO] gpu-4 is busy, wait 300s
20240224 22:22:12[INFO] gpu-7 is busy, wait 300s
20240224 22:22:18[INFO] gpu-3 is busy, wait 300s
20240224 22:26:01[INFO] gpu-1 is busy, wait 300s
20240224 22:26:07[INFO] gpu-0 is busy, wait 300s
20240224 22:26:20[INFO] gpu-6 is busy, wait 300s
20240224 22:26:20[INFO] gpu-2 is busy, wait 300s
20240224 22:26:26[INFO] gpu-5 is busy, wait 300s
20240224 22:27:18[INFO] gpu-4 is busy, wait 300s
20240224 22:27:23[INFO] gpu-3 is busy, wait 300s
20240224 22:31:06[INFO] gpu-1 is busy, wait 300s
20240224 22:31:13[INFO] gpu-0 is busy, wait 300s
20240224 22:31:29[INFO] gpu-2 is busy, wait 300s
20240224 22:31:31[INFO] gpu-5 is busy, wait 300s
20240224 22:31:35[INFO] gpu-6 is busy, wait 300s
20240224 22:32:29[INFO] gpu-4 is busy, wait 300s
20240224 22:32:44[INFO] gpu-3 is busy, wait 300s
20240224 22:35:26[INFO] gpu-7 is busy, wait 300s
20240224 22:36:06[INFO] gpu-1 is busy, wait 300s
20240224 22:36:13[INFO] gpu-0 is busy, wait 300s
20240224 22:36:29[INFO] gpu-2 is busy, wait 300s
20240224 22:36:31[INFO] gpu-5 is busy, wait 300s
20240224 22:36:36[INFO] gpu-6 is busy, wait 300s
20240224 22:37:30[INFO] gpu-4 is busy, wait 300s
20240224 22:37:44[INFO] gpu-3 is busy, wait 300s
20240224 22:40:27[INFO] gpu-7 is busy, wait 300s
20240224 22:41:07[INFO] gpu-1 is busy, wait 300s
20240224 22:41:41[INFO] gpu-6 is busy, wait 300s
20240224 22:44:37[INFO] gpu-0 is busy, wait 300s
20240224 22:44:47[INFO] gpu-3 is busy, wait 300s
20240224 22:44:54[INFO] gpu-2 is busy, wait 300s
20240224 22:44:57[INFO] gpu-4 is busy, wait 300s
20240224 22:45:01[INFO] gpu-5 is busy, wait 300s
20240224 22:46:24[INFO] gpu-1 is busy, wait 300s
20240224 22:46:46[INFO] gpu-6 is busy, wait 300s
20240224 22:49:44[INFO] gpu-0 is busy, wait 300s
20240224 22:49:53[INFO] gpu-3 is busy, wait 300s
20240224 22:50:04[INFO] gpu-2 is busy, wait 300s
20240224 22:50:07[INFO] gpu-5 is busy, wait 300s
20240224 22:50:08[INFO] gpu-4 is busy, wait 300s
20240224 22:51:38[INFO] gpu-1 is busy, wait 300s
20240224 22:51:53[INFO] gpu-6 is busy, wait 300s
20240224 22:53:58[INFO] gpu-7 is busy, wait 300s
20240224 22:54:45[INFO] gpu-0 is busy, wait 300s
20240224 22:54:54[INFO] gpu-3 is busy, wait 300s
20240224 22:55:05[INFO] gpu-2 is busy, wait 300s
20240224 22:55:07[INFO] gpu-5 is busy, wait 300s
20240224 22:55:08[INFO] gpu-4 is busy, wait 300s
20240224 22:56:38[INFO] gpu-1 is busy, wait 300s
20240224 22:56:53[INFO] gpu-6 is busy, wait 300s
20240224 23:01:59[INFO] gpu-6 is busy, wait 300s
20240224 23:02:00[INFO] gpu-1 is busy, wait 300s
20240224 23:02:01[INFO] gpu-0 is busy, wait 300s
20240224 23:02:02[INFO] gpu-7 is busy, wait 300s
20240224 23:02:05[INFO] gpu-4 is busy, wait 300s
20240224 23:02:11[INFO] gpu-5 is busy, wait 300s
20240224 23:02:15[INFO] gpu-3 is busy, wait 300s
20240224 23:02:54[INFO] gpu-2 is busy, wait 300s
20240224 23:07:07[INFO] gpu-6 is busy, wait 300s
20240224 23:07:09[INFO] gpu-0 is busy, wait 300s
20240224 23:07:09[INFO] gpu-4 is busy, wait 300s
20240224 23:07:15[INFO] gpu-5 is busy, wait 300s
20240224 23:07:36[INFO] gpu-3 is busy, wait 300s
20240224 23:08:05[INFO] gpu-1 is busy, wait 300s
20240224 23:08:23[INFO] gpu-2 is busy, wait 300s
20240224 23:12:14[INFO] gpu-6 is busy, wait 300s
20240224 23:12:16[INFO] gpu-4 is busy, wait 300s
20240224 23:12:20[INFO] gpu-5 is busy, wait 300s
20240224 23:12:21[INFO] gpu-0 is busy, wait 300s
20240224 23:12:40[INFO] gpu-3 is busy, wait 300s
20240224 23:12:54[INFO] gpu-7 is busy, wait 300s
20240224 23:13:06[INFO] gpu-1 is busy, wait 300s
20240224 23:13:24[INFO] gpu-2 is busy, wait 300s
20240224 23:18:16[INFO] gpu-6 is busy, wait 300s
20240224 23:23:23[INFO] gpu-6 is busy, wait 300s
20240224 23:28:39[INFO] gpu-6 is busy, wait 300s
20240224 23:34:00[INFO] gpu-6 is busy, wait 300s
20240224 23:39:21[INFO] gpu-6 is busy, wait 300s
20240224 23:44:36[INFO] gpu-6 is busy, wait 300s
20240224 23:49:56[INFO] gpu-6 is busy, wait 300s
20240224 23:55:16[INFO] gpu-6 is busy, wait 300s
20240225 00:01:26[INFO] gpu-6 is busy, wait 300s
20240225 00:06:45[INFO] gpu-6 is busy, wait 300s
20240225 00:11:53[INFO] gpu-6 is busy, wait 300s
20240225 00:16:59[INFO] gpu-6 is busy, wait 300s
20240225 00:22:06[INFO] gpu-6 is busy, wait 300s
20240225 00:27:47[INFO] gpu-6 is busy, wait 300s
20240225 00:33:01[INFO] gpu-6 is busy, wait 300s
20240225 00:38:23[INFO] gpu-6 is busy, wait 300s
20240225 00:43:30[INFO] gpu-6 is busy, wait 300s
20240225 00:48:44[INFO] gpu-6 is busy, wait 300s
20240225 00:53:51[INFO] gpu-6 is busy, wait 300s
20240225 00:58:58[INFO] gpu-6 is busy, wait 300s
20240225 01:04:23[INFO] gpu-6 is busy, wait 300s
20240225 01:09:39[INFO] gpu-6 is busy, wait 300s
20240225 01:14:47[INFO] gpu-6 is busy, wait 300s
20240225 01:20:24[INFO] gpu-6 is busy, wait 300s
20240225 01:25:45[INFO] gpu-6 is busy, wait 300s
20240225 01:31:14[INFO] gpu-6 is busy, wait 300s
20240225 01:36:34[INFO] gpu-6 is busy, wait 300s
20240225 01:41:40[INFO] gpu-6 is busy, wait 300s
20240225 01:47:06[INFO] gpu-6 is busy, wait 300s
20240225 01:52:15[INFO] gpu-6 is busy, wait 300s
20240225 01:57:28[INFO] gpu-6 is busy, wait 300s
20240225 02:02:40[INFO] gpu-6 is busy, wait 300s
20240225 02:07:47[INFO] gpu-6 is busy, wait 300s
20240225 02:13:23[INFO] gpu-6 is busy, wait 300s
20240225 02:18:58[INFO] gpu-6 is busy, wait 300s
20240225 02:24:05[INFO] gpu-6 is busy, wait 300s
20240225 02:29:14[INFO] gpu-6 is busy, wait 300s
20240225 02:34:21[INFO] gpu-6 is busy, wait 300s
20240225 02:39:27[INFO] gpu-6 is busy, wait 300s
20240225 02:44:46[INFO] gpu-6 is busy, wait 300s
20240225 02:50:48[INFO] gpu-6 is busy, wait 300s
20240225 02:55:56[INFO] gpu-6 is busy, wait 300s
20240225 03:01:03[INFO] gpu-6 is busy, wait 300s
20240225 03:06:39[INFO] gpu-6 is busy, wait 300s
20240225 03:11:45[INFO] gpu-6 is busy, wait 300s
20240225 03:17:05[INFO] gpu-6 is busy, wait 300s
20240225 03:22:14[INFO] gpu-6 is busy, wait 300s
20240225 03:27:19[INFO] gpu-6 is busy, wait 300s
20240225 03:32:25[INFO] gpu-6 is busy, wait 300s
20240225 03:37:41[INFO] gpu-6 is busy, wait 300s
20240225 03:42:48[INFO] gpu-6 is busy, wait 300s
20240225 03:48:03[INFO] gpu-6 is busy, wait 300s
20240225 03:53:47[INFO] gpu-6 is busy, wait 300s
20240225 03:59:03[INFO] gpu-6 is busy, wait 300s
20240225 04:04:11[INFO] gpu-6 is busy, wait 300s
20240225 04:09:20[INFO] gpu-6 is busy, wait 300s
20240225 04:14:34[INFO] gpu-6 is busy, wait 300s
20240225 04:19:44[INFO] gpu-6 is busy, wait 300s
20240225 04:24:51[INFO] gpu-6 is busy, wait 300s
20240225 04:31:17[INFO] gpu-6 is busy, wait 300s
20240225 04:36:30[INFO] gpu-6 is busy, wait 300s
20240225 04:41:37[INFO] gpu-6 is busy, wait 300s
20240225 04:47:03[INFO] gpu-6 is busy, wait 300s
20240225 04:52:10[INFO] gpu-6 is busy, wait 300s
20240225 04:58:26[INFO] gpu-6 is busy, wait 300s
20240225 05:03:53[INFO] gpu-6 is busy, wait 300s
20240225 05:08:59[INFO] gpu-6 is busy, wait 300s
20240225 05:14:07[INFO] gpu-6 is busy, wait 300s
20240225 05:19:30[INFO] gpu-6 is busy, wait 300s
20240225 05:24:46[INFO] gpu-6 is busy, wait 300s
20240225 05:30:29[INFO] gpu-6 is busy, wait 300s
20240225 05:35:37[INFO] gpu-6 is busy, wait 300s
20240225 05:41:12[INFO] gpu-6 is busy, wait 300s
20240225 05:46:20[INFO] gpu-6 is busy, wait 300s
20240225 05:52:02[INFO] gpu-6 is busy, wait 300s
20240225 05:57:09[INFO] gpu-6 is busy, wait 300s
20240225 06:02:18[INFO] gpu-6 is busy, wait 300s
20240225 06:07:35[INFO] gpu-6 is busy, wait 300s
20240225 06:12:43[INFO] gpu-6 is busy, wait 300s
20240225 06:17:57[INFO] gpu-6 is busy, wait 300s
20240225 06:23:03[INFO] gpu-6 is busy, wait 300s
20240225 06:28:23[INFO] gpu-6 is busy, wait 300s
20240225 06:33:42[INFO] gpu-6 is busy, wait 300s
20240225 06:38:49[INFO] gpu-6 is busy, wait 300s
20240225 06:43:58[INFO] gpu-6 is busy, wait 300s
20240225 06:49:11[INFO] gpu-6 is busy, wait 300s
20240225 06:54:20[INFO] gpu-6 is busy, wait 300s
20240225 07:00:11[INFO] gpu-6 is busy, wait 300s
20240225 07:05:17[INFO] gpu-6 is busy, wait 300s
20240225 07:10:24[INFO] gpu-6 is busy, wait 300s
20240225 07:15:56[INFO] gpu-6 is busy, wait 300s
20240225 07:21:11[INFO] gpu-6 is busy, wait 300s
20240225 07:26:30[INFO] gpu-6 is busy, wait 300s
20240225 07:31:36[INFO] gpu-6 is busy, wait 300s
20240225 07:36:44[INFO] gpu-6 is busy, wait 300s
20240225 07:41:52[INFO] gpu-6 is busy, wait 300s
20240225 07:47:07[INFO] gpu-6 is busy, wait 300s
20240225 07:52:14[INFO] gpu-6 is busy, wait 300s
20240225 07:57:27[INFO] gpu-6 is busy, wait 300s
20240225 08:02:34[INFO] gpu-6 is busy, wait 300s
20240225 08:07:41[INFO] gpu-6 is busy, wait 300s
20240225 08:13:16[INFO] gpu-6 is busy, wait 300s
20240225 08:18:36[INFO] gpu-6 is busy, wait 300s
20240225 08:24:56[INFO] gpu-6 is busy, wait 300s
20240225 08:30:57[INFO] gpu-6 is busy, wait 300s
20240225 08:36:51[INFO] gpu-6 is busy, wait 300s
20240225 08:42:12[INFO] gpu-6 is busy, wait 300s
20240225 08:48:01[INFO] gpu-6 is busy, wait 300s
20240225 08:54:13[INFO] gpu-6 is busy, wait 300s
20240225 08:59:58[INFO] gpu-6 is busy, wait 300s
20240225 09:05:19[INFO] gpu-6 is busy, wait 300s
20240225 09:10:27[INFO] gpu-6 is busy, wait 300s
20240225 09:15:42[INFO] gpu-6 is busy, wait 300s
20240225 09:20:51[INFO] gpu-6 is busy, wait 300s
20240225 09:25:57[INFO] gpu-6 is busy, wait 300s
20240225 09:31:05[INFO] gpu-6 is busy, wait 300s
20240225 09:36:13[INFO] gpu-6 is busy, wait 300s
20240225 09:41:22[INFO] gpu-6 is busy, wait 300s
20240225 09:46:42[INFO] gpu-6 is busy, wait 300s
20240225 09:51:53[INFO] gpu-6 is busy, wait 300s
20240225 09:57:10[INFO] gpu-6 is busy, wait 300s
20240225 10:02:17[INFO] gpu-6 is busy, wait 300s
20240225 10:07:25[INFO] gpu-6 is busy, wait 300s
20240225 10:12:39[INFO] gpu-6 is busy, wait 300s
20240225 10:18:27[INFO] gpu-6 is busy, wait 300s
20240225 10:23:33[INFO] gpu-6 is busy, wait 300s
20240225 10:28:42[INFO] gpu-6 is busy, wait 300s
20240225 10:33:57[INFO] gpu-6 is busy, wait 300s
20240225 10:39:04[INFO] gpu-6 is busy, wait 300s
20240225 10:44:47[INFO] gpu-6 is busy, wait 300s
20240225 10:50:03[INFO] gpu-6 is busy, wait 300s
20240225 10:55:07[INFO] gpu-6 is busy, wait 300s
20240225 11:00:30[INFO] gpu-6 is busy, wait 300s
20240225 11:05:45[INFO] gpu-6 is busy, wait 300s
20240225 11:11:06[INFO] gpu-6 is busy, wait 300s
20240225 11:16:19[INFO] gpu-6 is busy, wait 300s
20240225 11:21:25[INFO] gpu-6 is busy, wait 300s
20240225 11:26:34[INFO] gpu-6 is busy, wait 300s
20240225 11:31:40[INFO] gpu-6 is busy, wait 300s
20240225 11:37:00[INFO] gpu-6 is busy, wait 300s
20240225 11:42:52[INFO] gpu-6 is busy, wait 300s
20240225 11:48:16[INFO] gpu-6 is busy, wait 300s
20240225 11:53:22[INFO] gpu-6 is busy, wait 300s
20240225 11:58:29[INFO] gpu-6 is busy, wait 300s
20240225 12:03:36[INFO] gpu-6 is busy, wait 300s
20240225 12:09:00[INFO] gpu-6 is busy, wait 300s
20240225 12:14:06[INFO] gpu-6 is busy, wait 300s
20240225 12:19:20[INFO] gpu-6 is busy, wait 300s
20240225 12:24:55[INFO] gpu-6 is busy, wait 300s
20240225 12:30:28[INFO] gpu-6 is busy, wait 300s
20240225 12:35:34[INFO] gpu-6 is busy, wait 300s
20240225 12:40:57[INFO] gpu-6 is busy, wait 300s
20240225 12:46:10[INFO] gpu-6 is busy, wait 300s
20240225 12:51:18[INFO] gpu-6 is busy, wait 300s
20240225 12:56:25[INFO] gpu-6 is busy, wait 300s
20240225 13:01:38[INFO] gpu-6 is busy, wait 300s
20240225 13:06:56[INFO] gpu-6 is busy, wait 300s
20240225 13:12:11[INFO] gpu-6 is busy, wait 300s
20240225 13:18:44[INFO] gpu-6 is busy, wait 300s
20240225 13:24:06[INFO] gpu-6 is busy, wait 300s
20240225 13:29:31[INFO] gpu-6 is busy, wait 300s
20240225 13:34:56[INFO] gpu-6 is busy, wait 300s
20240225 13:40:18[INFO] gpu-6 is busy, wait 300s
20240225 13:45:32[INFO] gpu-6 is busy, wait 300s
20240225 13:51:29[INFO] gpu-6 is busy, wait 300s
20240225 13:56:50[INFO] gpu-6 is busy, wait 300s
20240225 14:01:57[INFO] gpu-6 is busy, wait 300s
20240225 14:07:47[INFO] gpu-6 is busy, wait 300s
20240225 14:12:54[INFO] gpu-6 is busy, wait 300s
20240225 14:18:01[INFO] gpu-6 is busy, wait 300s
20240225 14:23:30[INFO] gpu-6 is busy, wait 300s
20240225 14:29:14[INFO] gpu-6 is busy, wait 300s
20240225 14:34:31[INFO] gpu-6 is busy, wait 300s
20240225 14:39:49[INFO] gpu-6 is busy, wait 300s
20240225 14:45:06[INFO] gpu-6 is busy, wait 300s
20240225 14:50:17[INFO] gpu-6 is busy, wait 300s
20240225 14:55:59[INFO] gpu-6 is busy, wait 300s
20240225 15:02:11[INFO] gpu-6 is busy, wait 300s
20240225 15:07:24[INFO] gpu-6 is busy, wait 300s
20240225 15:12:57[INFO] gpu-6 is busy, wait 300s
20240225 15:18:19[INFO] gpu-6 is busy, wait 300s
20240225 15:23:36[INFO] gpu-6 is busy, wait 300s
20240225 15:29:11[INFO] gpu-6 is busy, wait 300s
20240225 15:35:04[INFO] gpu-6 is busy, wait 300s
20240225 15:40:18[INFO] gpu-6 is busy, wait 300s
20240225 15:45:33[INFO] gpu-6 is busy, wait 300s
20240225 15:52:08[INFO] gpu-6 is busy, wait 300s
20240225 15:58:29[INFO] gpu-6 is busy, wait 300s
20240225 16:03:35[INFO] gpu-6 is busy, wait 300s
20240225 16:08:43[INFO] gpu-6 is busy, wait 300s
20240225 16:14:30[INFO] gpu-6 is busy, wait 300s
20240225 16:21:35[INFO] gpu-6 is busy, wait 300s
20240225 16:27:08[INFO] gpu-6 is busy, wait 300s
20240225 16:32:17[INFO] gpu-6 is busy, wait 300s
20240225 16:38:19[INFO] gpu-6 is busy, wait 300s
20240225 16:44:44[INFO] gpu-6 is busy, wait 300s
20240225 16:50:05[INFO] gpu-6 is busy, wait 300s
20240225 16:55:19[INFO] gpu-6 is busy, wait 300s
20240225 17:01:16[INFO] gpu-6 is busy, wait 300s
20240225 17:06:21[INFO] gpu-6 is busy, wait 300s
20240225 17:11:27[INFO] gpu-6 is busy, wait 300s
20240225 17:17:02[INFO] gpu-6 is busy, wait 300s
20240225 17:23:39[INFO] gpu-6 is busy, wait 300s
20240225 17:29:26[INFO] gpu-6 is busy, wait 300s
20240225 17:34:41[INFO] gpu-6 is busy, wait 300s
20240225 17:39:59[INFO] gpu-6 is busy, wait 300s
20240225 17:45:07[INFO] gpu-6 is busy, wait 300s
20240225 17:50:16[INFO] gpu-6 is busy, wait 300s
20240225 17:55:38[INFO] gpu-6 is busy, wait 300s
20240225 18:02:28[INFO] gpu-6 is busy, wait 300s
20240225 18:07:35[INFO] gpu-6 is busy, wait 300s
20240225 18:14:20[INFO] gpu-6 is busy, wait 300s
20240225 18:19:44[INFO] gpu-6 is busy, wait 300s
20240225 18:25:26[INFO] gpu-6 is busy, wait 300s
20240225 18:30:37[INFO] gpu-6 is busy, wait 300s
20240225 18:36:08[INFO] gpu-6 is busy, wait 300s
20240225 18:41:35[INFO] gpu-6 is busy, wait 300s
20240225 18:48:02[INFO] gpu-6 is busy, wait 300s
20240225 18:53:21[INFO] gpu-6 is busy, wait 300s
20240225 18:58:35[INFO] gpu-6 is busy, wait 300s
20240225 19:04:15[INFO] gpu-6 is busy, wait 300s
20240225 19:09:20[INFO] gpu-6 is busy, wait 300s
20240225 19:14:55[INFO] gpu-6 is busy, wait 300s
20240225 19:20:01[INFO] gpu-6 is busy, wait 300s
20240225 19:25:10[INFO] gpu-6 is busy, wait 300s
20240225 19:31:47[INFO] gpu-6 is busy, wait 300s
20240225 19:36:55[INFO] gpu-6 is busy, wait 300s
20240225 19:42:01[INFO] gpu-6 is busy, wait 300s
20240225 19:47:06[INFO] gpu-6 is busy, wait 300s
20240225 19:52:31[INFO] gpu-6 is busy, wait 300s
20240225 19:57:58[INFO] gpu-6 is busy, wait 300s
20240225 20:03:06[INFO] gpu-6 is busy, wait 300s
20240225 20:08:45[INFO] gpu-6 is busy, wait 300s
20240225 20:14:08[INFO] gpu-6 is busy, wait 300s
20240225 20:20:05[INFO] gpu-6 is busy, wait 300s
20240225 20:25:15[INFO] gpu-6 is busy, wait 300s
20240225 20:30:46[INFO] gpu-6 is busy, wait 300s
20240225 20:36:10[INFO] gpu-6 is busy, wait 300s
20240225 20:41:18[INFO] gpu-6 is busy, wait 300s
20240225 20:46:26[INFO] gpu-6 is busy, wait 300s
20240225 20:51:32[INFO] gpu-6 is busy, wait 300s
20240225 20:56:41[INFO] gpu-6 is busy, wait 300s
20240225 21:02:09[INFO] gpu-6 is busy, wait 300s
20240225 21:07:31[INFO] gpu-6 is busy, wait 300s
20240225 21:12:45[INFO] gpu-6 is busy, wait 300s
20240225 21:17:56[INFO] gpu-6 is busy, wait 300s
20240225 21:23:39[INFO] gpu-6 is busy, wait 300s
20240225 21:28:46[INFO] gpu-6 is busy, wait 300s
20240225 21:33:53[INFO] gpu-6 is busy, wait 300s
20240225 21:39:33[INFO] gpu-6 is busy, wait 300s
20240225 21:44:57[INFO] gpu-6 is busy, wait 300s
20240225 21:50:12[INFO] gpu-6 is busy, wait 300s
20240225 21:55:51[INFO] gpu-6 is busy, wait 300s
20240225 22:01:04[INFO] gpu-6 is busy, wait 300s
20240225 22:06:11[INFO] gpu-6 is busy, wait 300s
20240225 22:11:18[INFO] gpu-6 is busy, wait 300s
20240225 22:16:32[INFO] gpu-6 is busy, wait 300s
20240225 22:22:19[INFO] gpu-6 is busy, wait 300s
20240225 22:27:54[INFO] gpu-6 is busy, wait 300s
20240225 22:33:12[INFO] gpu-6 is busy, wait 300s
20240225 22:38:41[INFO] gpu-6 is busy, wait 300s
20240225 22:44:27[INFO] gpu-6 is busy, wait 300s
20240225 22:49:57[INFO] gpu-6 is busy, wait 300s
20240225 22:55:36[INFO] gpu-6 is busy, wait 300s
20240225 23:00:52[INFO] gpu-6 is busy, wait 300s
20240225 23:06:30[INFO] gpu-6 is busy, wait 300s
20240225 23:11:37[INFO] gpu-6 is busy, wait 300s
20240225 23:17:22[INFO] gpu-6 is busy, wait 300s
20240225 23:22:30[INFO] gpu-6 is busy, wait 300s
20240225 23:27:36[INFO] gpu-6 is busy, wait 300s
20240225 23:32:44[INFO] gpu-6 is busy, wait 300s
20240225 23:37:52[INFO] gpu-6 is busy, wait 300s
20240225 23:43:04[INFO] gpu-6 is busy, wait 300s
20240225 23:48:32[INFO] gpu-6 is busy, wait 300s
20240225 23:54:36[INFO] gpu-6 is busy, wait 300s
20240225 23:59:53[INFO] gpu-6 is busy, wait 300s
20240226 00:05:07[INFO] gpu-6 is busy, wait 300s
20240226 00:10:29[INFO] gpu-6 is busy, wait 300s
20240226 00:15:43[INFO] gpu-6 is busy, wait 300s
20240226 00:20:51[INFO] gpu-6 is busy, wait 300s
20240226 00:26:13[INFO] gpu-6 is busy, wait 300s
20240226 00:31:52[INFO] gpu-6 is busy, wait 300s
20240226 00:36:58[INFO] gpu-6 is busy, wait 300s
20240226 00:42:32[INFO] gpu-6 is busy, wait 300s
20240226 00:47:38[INFO] gpu-6 is busy, wait 300s
20240226 00:52:44[INFO] gpu-6 is busy, wait 300s
20240226 00:57:51[INFO] gpu-6 is busy, wait 300s
20240226 01:03:18[INFO] gpu-6 is busy, wait 300s
20240226 01:08:47[INFO] gpu-6 is busy, wait 300s
20240226 01:14:01[INFO] gpu-6 is busy, wait 300s
20240226 01:19:26[INFO] gpu-6 is busy, wait 300s
20240226 01:25:30[INFO] gpu-6 is busy, wait 300s
20240226 01:31:09[INFO] gpu-6 is busy, wait 300s
20240226 01:36:30[INFO] gpu-6 is busy, wait 300s
20240226 01:42:01[INFO] gpu-6 is busy, wait 300s
20240226 01:48:01[INFO] gpu-6 is busy, wait 300s
20240226 01:53:32[INFO] gpu-6 is busy, wait 300s
20240226 01:59:17[INFO] gpu-6 is busy, wait 300s
20240226 02:04:23[INFO] gpu-6 is busy, wait 300s
20240226 02:09:56[INFO] gpu-6 is busy, wait 300s
20240226 02:15:14[INFO] gpu-6 is busy, wait 300s
20240226 02:20:28[INFO] gpu-6 is busy, wait 300s
20240226 02:26:13[INFO] gpu-6 is busy, wait 300s
20240226 02:31:34[INFO] gpu-6 is busy, wait 300s
20240226 02:36:54[INFO] gpu-6 is busy, wait 300s
20240226 02:42:20[INFO] gpu-6 is busy, wait 300s
20240226 02:47:27[INFO] gpu-6 is busy, wait 300s
20240226 02:52:41[INFO] gpu-6 is busy, wait 300s
20240226 02:57:47[INFO] gpu-6 is busy, wait 300s
20240226 03:02:57[INFO] gpu-6 is busy, wait 300s
20240226 03:08:12[INFO] gpu-6 is busy, wait 300s
20240226 03:13:36[INFO] gpu-6 is busy, wait 300s
20240226 03:18:42[INFO] gpu-6 is busy, wait 300s
20240226 03:23:49[INFO] gpu-6 is busy, wait 300s
20240226 03:28:55[INFO] gpu-6 is busy, wait 300s
20240226 03:34:12[INFO] gpu-6 is busy, wait 300s
20240226 03:39:34[INFO] gpu-6 is busy, wait 300s
20240226 03:45:25[INFO] gpu-6 is busy, wait 300s
20240226 03:50:32[INFO] gpu-6 is busy, wait 300s
20240226 03:56:00[INFO] gpu-6 is busy, wait 300s
20240226 04:01:26[INFO] gpu-6 is busy, wait 300s
20240226 04:06:32[INFO] gpu-6 is busy, wait 300s
20240226 04:11:39[INFO] gpu-6 is busy, wait 300s
20240226 04:16:47[INFO] gpu-6 is busy, wait 300s
20240226 04:21:58[INFO] gpu-6 is busy, wait 300s
20240226 04:27:18[INFO] gpu-6 is busy, wait 300s
20240226 04:32:25[INFO] gpu-6 is busy, wait 300s
20240226 04:37:32[INFO] gpu-6 is busy, wait 300s
20240226 04:42:37[INFO] gpu-6 is busy, wait 300s
20240226 04:48:03[INFO] gpu-6 is busy, wait 300s
20240226 04:53:09[INFO] gpu-6 is busy, wait 300s
20240226 04:58:16[INFO] gpu-6 is busy, wait 300s
20240226 05:03:24[INFO] gpu-6 is busy, wait 300s
20240226 05:08:46[INFO] gpu-6 is busy, wait 300s
20240226 05:14:00[INFO] gpu-6 is busy, wait 300s
20240226 05:19:07[INFO] gpu-6 is busy, wait 300s
20240226 05:24:53[INFO] gpu-6 is busy, wait 300s
20240226 05:29:59[INFO] gpu-6 is busy, wait 300s
20240226 05:35:20[INFO] gpu-6 is busy, wait 300s
20240226 05:40:46[INFO] gpu-6 is busy, wait 300s
20240226 05:46:22[INFO] gpu-6 is busy, wait 300s
20240226 05:51:28[INFO] gpu-6 is busy, wait 300s
20240226 05:56:50[INFO] gpu-6 is busy, wait 300s
20240226 06:01:57[INFO] gpu-6 is busy, wait 300s
20240226 06:07:06[INFO] gpu-6 is busy, wait 300s
20240226 06:12:19[INFO] gpu-6 is busy, wait 300s
20240226 06:17:42[INFO] gpu-6 is busy, wait 300s
20240226 06:22:55[INFO] gpu-6 is busy, wait 300s
20240226 06:28:17[INFO] gpu-6 is busy, wait 300s
20240226 06:34:14[INFO] gpu-6 is busy, wait 300s
20240226 06:39:35[INFO] gpu-6 is busy, wait 300s
20240226 06:44:43[INFO] gpu-6 is busy, wait 300s
20240226 06:50:07[INFO] gpu-6 is busy, wait 300s
20240226 06:55:20[INFO] gpu-6 is busy, wait 300s
20240226 07:00:48[INFO] gpu-6 is busy, wait 300s
20240226 07:05:55[INFO] gpu-6 is busy, wait 300s
20240226 07:11:07[INFO] gpu-6 is busy, wait 300s
20240226 07:16:15[INFO] gpu-6 is busy, wait 300s
20240226 07:21:21[INFO] gpu-6 is busy, wait 300s
20240226 07:26:31[INFO] gpu-6 is busy, wait 300s
20240226 07:31:53[INFO] gpu-6 is busy, wait 300s
20240226 07:37:18[INFO] gpu-6 is busy, wait 300s
20240226 07:43:05[INFO] gpu-6 is busy, wait 300s
20240226 07:49:07[INFO] gpu-6 is busy, wait 300s
20240226 07:54:18[INFO] gpu-6 is busy, wait 300s
20240226 07:59:38[INFO] gpu-6 is busy, wait 300s
20240226 08:04:44[INFO] gpu-6 is busy, wait 300s
20240226 08:11:28[INFO] gpu-6 is busy, wait 300s
20240226 08:16:37[INFO] gpu-6 is busy, wait 300s
20240226 08:22:06[INFO] gpu-6 is busy, wait 300s
20240226 08:27:11[INFO] gpu-6 is busy, wait 300s
20240226 08:32:19[INFO] gpu-6 is busy, wait 300s
20240226 08:37:40[INFO] gpu-6 is busy, wait 300s
20240226 08:42:47[INFO] gpu-6 is busy, wait 300s
20240226 08:48:29[INFO] gpu-6 is busy, wait 300s
20240226 08:53:36[INFO] gpu-6 is busy, wait 300s
20240226 08:58:47[INFO] gpu-6 is busy, wait 300s
20240226 09:04:01[INFO] gpu-6 is busy, wait 300s
20240226 09:09:19[INFO] gpu-6 is busy, wait 300s
20240226 09:14:23[INFO] gpu-6 is busy, wait 300s
20240226 09:19:36[INFO] gpu-6 is busy, wait 300s
20240226 09:24:53[INFO] gpu-6 is busy, wait 300s
20240226 09:30:24[INFO] gpu-6 is busy, wait 300s
20240226 09:36:09[INFO] gpu-6 is busy, wait 300s
20240226 09:41:47[INFO] gpu-6 is busy, wait 300s
20240226 09:47:00[INFO] gpu-6 is busy, wait 300s
20240226 09:52:49[INFO] gpu-6 is busy, wait 300s
20240226 09:58:13[INFO] gpu-6 is busy, wait 300s
20240226 10:03:20[INFO] gpu-6 is busy, wait 300s
20240226 10:08:28[INFO] gpu-6 is busy, wait 300s
20240226 10:13:41[INFO] gpu-6 is busy, wait 300s
20240226 10:17:19[INFO] gpu-0 is busy, wait 300s
20240226 10:19:03[INFO] gpu-6 is busy, wait 300s
20240226 10:22:26[INFO] gpu-0 is busy, wait 300s
20240226 10:24:11[INFO] gpu-6 is busy, wait 300s
20240226 10:27:27[INFO] gpu-0 is busy, wait 300s
20240226 10:29:23[INFO] gpu-6 is busy, wait 300s
20240226 10:33:07[INFO] gpu-0 is busy, wait 300s
20240226 10:34:46[INFO] gpu-6 is busy, wait 300s
20240226 10:38:16[INFO] gpu-0 is busy, wait 300s
20240226 10:40:17[INFO] gpu-6 is busy, wait 300s
20240226 10:43:25[INFO] gpu-0 is busy, wait 300s
20240226 10:45:45[INFO] gpu-6 is busy, wait 300s
20240226 10:48:26[INFO] gpu-0 is busy, wait 300s
20240226 10:50:51[INFO] gpu-6 is busy, wait 300s
20240226 10:53:34[INFO] gpu-0 is busy, wait 300s
20240226 10:56:19[INFO] gpu-6 is busy, wait 300s
20240226 10:58:49[INFO] gpu-0 is busy, wait 300s
20240226 11:01:34[INFO] gpu-6 is busy, wait 300s
20240226 11:04:01[INFO] gpu-0 is busy, wait 300s
20240226 11:06:41[INFO] gpu-6 is busy, wait 300s
20240226 11:09:02[INFO] gpu-0 is busy, wait 300s
20240226 11:11:57[INFO] gpu-6 is busy, wait 300s
20240226 11:14:10[INFO] gpu-0 is busy, wait 300s
20240226 11:17:42[INFO] gpu-6 is busy, wait 300s
20240226 11:22:53[INFO] gpu-6 is busy, wait 300s
20240226 11:28:01[INFO] gpu-6 is busy, wait 300s
20240226 11:33:26[INFO] gpu-6 is busy, wait 300s
20240226 11:38:35[INFO] gpu-6 is busy, wait 300s
20240226 11:43:49[INFO] gpu-6 is busy, wait 300s
20240226 11:44:22[INFO] gpu-0 is busy, wait 300s
20240226 11:49:35[INFO] gpu-0 is busy, wait 300s
20240226 11:49:46[INFO] gpu-6 is busy, wait 300s
20240226 11:54:36[INFO] gpu-0 is busy, wait 300s
20240226 11:55:00[INFO] gpu-6 is busy, wait 300s
20240226 12:00:04[INFO] gpu-0 is busy, wait 300s
20240226 12:00:06[INFO] gpu-6 is busy, wait 300s
20240226 12:05:13[INFO] gpu-0 is busy, wait 300s
20240226 12:05:36[INFO] gpu-6 is busy, wait 300s
20240226 12:10:13[INFO] gpu-0 is busy, wait 300s
20240226 12:10:53[INFO] gpu-6 is busy, wait 300s
20240226 12:15:53[INFO] gpu-0 is busy, wait 300s
20240226 12:16:01[INFO] gpu-6 is busy, wait 300s
20240226 12:21:08[INFO] gpu-0 is busy, wait 300s
20240226 12:21:25[INFO] gpu-6 is busy, wait 300s
20240226 12:26:45[INFO] gpu-0 is busy, wait 300s
20240226 12:26:55[INFO] gpu-6 is busy, wait 300s
20240226 12:32:03[INFO] gpu-6 is busy, wait 300s
20240226 12:32:11[INFO] gpu-0 is busy, wait 300s
20240226 12:37:11[INFO] gpu-0 is busy, wait 300s
20240226 12:37:24[INFO] gpu-6 is busy, wait 300s
20240226 12:42:33[INFO] gpu-6 is busy, wait 300s
20240226 12:47:39[INFO] gpu-6 is busy, wait 300s
20240226 12:52:48[INFO] gpu-6 is busy, wait 300s
20240226 12:58:22[INFO] gpu-6 is busy, wait 300s
20240226 13:03:30[INFO] gpu-6 is busy, wait 300s
20240226 13:08:54[INFO] gpu-6 is busy, wait 300s
20240226 13:13:59[INFO] gpu-6 is busy, wait 300s
20240226 13:19:05[INFO] gpu-6 is busy, wait 300s
20240226 13:24:10[INFO] gpu-6 is busy, wait 300s
20240226 13:29:16[INFO] gpu-6 is busy, wait 300s
20240226 13:34:22[INFO] gpu-6 is busy, wait 300s
20240226 13:40:08[INFO] gpu-6 is busy, wait 300s
20240226 13:43:52[INFO] gpu-0 is busy, wait 300s
20240226 13:45:25[INFO] gpu-6 is busy, wait 300s
20240226 13:48:53[INFO] gpu-0 is busy, wait 300s
20240226 13:50:43[INFO] gpu-6 is busy, wait 300s
20240226 13:55:49[INFO] gpu-6 is busy, wait 300s
20240226 13:59:37[INFO] gpu-0 is busy, wait 300s
20240226 14:00:56[INFO] gpu-6 is busy, wait 300s
20240226 14:05:22[INFO] gpu-0 is busy, wait 300s
20240226 14:06:24[INFO] gpu-6 is busy, wait 300s
20240226 14:10:22[INFO] gpu-0 is busy, wait 300s
20240226 14:11:36[INFO] gpu-6 is busy, wait 300s
20240226 14:16:50[INFO] gpu-6 is busy, wait 300s
20240226 14:21:58[INFO] gpu-6 is busy, wait 300s
20240226 14:27:07[INFO] gpu-6 is busy, wait 300s
20240226 14:32:16[INFO] gpu-6 is busy, wait 300s
20240226 14:37:40[INFO] gpu-6 is busy, wait 300s
20240226 14:43:01[INFO] gpu-6 is busy, wait 300s
20240226 14:49:03[INFO] gpu-6 is busy, wait 300s
20240226 14:54:12[INFO] gpu-6 is busy, wait 300s
20240226 14:59:26[INFO] gpu-6 is busy, wait 300s
20240226 15:03:11[INFO] gpu-0 is busy, wait 300s
20240226 15:04:48[INFO] gpu-6 is busy, wait 300s
20240226 15:10:03[INFO] gpu-6 is busy, wait 300s
20240226 15:15:13[INFO] gpu-6 is busy, wait 300s
20240226 15:16:47[INFO] gpu-0 is busy, wait 300s
20240226 15:20:27[INFO] gpu-6 is busy, wait 300s
20240226 15:22:01[INFO] gpu-0 is busy, wait 300s
20240226 15:25:40[INFO] gpu-6 is busy, wait 300s
20240226 15:27:26[INFO] gpu-0 is busy, wait 300s
20240226 15:31:00[INFO] gpu-6 is busy, wait 300s
20240226 15:32:35[INFO] gpu-0 is busy, wait 300s
20240226 15:36:16[INFO] gpu-6 is busy, wait 300s
20240226 15:37:50[INFO] gpu-0 is busy, wait 300s
20240226 15:41:48[INFO] gpu-6 is busy, wait 300s
20240226 15:43:03[INFO] gpu-0 is busy, wait 300s
20240226 15:47:20[INFO] gpu-6 is busy, wait 300s
20240226 15:50:48[INFO] gpu-0 is busy, wait 300s
20240226 15:53:34[INFO] gpu-6 is busy, wait 300s
20240226 15:56:04[INFO] gpu-0 is busy, wait 300s
20240226 15:58:40[INFO] gpu-6 is busy, wait 300s
20240226 16:01:09[INFO] gpu-0 is busy, wait 300s
20240226 16:04:02[INFO] gpu-6 is busy, wait 300s
20240226 16:06:18[INFO] gpu-0 is busy, wait 300s
20240226 16:09:08[INFO] gpu-6 is busy, wait 300s
20240226 16:11:25[INFO] gpu-0 is busy, wait 300s
20240226 16:14:16[INFO] gpu-6 is busy, wait 300s
20240226 16:16:34[INFO] gpu-0 is busy, wait 300s
20240226 16:19:29[INFO] gpu-6 is busy, wait 300s
20240226 16:21:43[INFO] gpu-0 is busy, wait 300s
20240226 16:25:08[INFO] gpu-6 is busy, wait 300s
20240226 16:27:24[INFO] gpu-0 is busy, wait 300s
20240226 16:30:16[INFO] gpu-6 is busy, wait 300s
20240226 16:32:38[INFO] gpu-0 is busy, wait 300s
20240226 16:35:32[INFO] gpu-6 is busy, wait 300s
20240226 16:37:46[INFO] gpu-0 is busy, wait 300s
20240226 16:40:38[INFO] gpu-6 is busy, wait 300s
20240226 16:43:14[INFO] gpu-0 is busy, wait 300s
20240226 16:46:13[INFO] gpu-6 is busy, wait 300s
20240226 16:48:28[INFO] gpu-0 is busy, wait 300s
20240226 16:51:39[INFO] gpu-6 is busy, wait 300s
20240226 16:53:35[INFO] gpu-0 is busy, wait 300s
20240226 16:56:52[INFO] gpu-6 is busy, wait 300s
20240226 16:58:50[INFO] gpu-0 is busy, wait 300s
20240226 17:01:58[INFO] gpu-6 is busy, wait 300s
20240226 17:02:15[INFO] gpu-1 is busy, wait 300s
20240226 17:04:06[INFO] gpu-0 is busy, wait 300s
20240226 17:07:14[INFO] gpu-6 is busy, wait 300s
20240226 17:07:24[INFO] gpu-1 is busy, wait 300s
20240226 17:09:25[INFO] gpu-0 is busy, wait 300s
20240226 17:10:09[INFO] gpu-4 is busy, wait 300s
20240226 17:12:19[INFO] gpu-6 is busy, wait 300s
20240226 17:15:05[INFO] gpu-0 is busy, wait 300s
20240226 17:17:33[INFO] gpu-6 is busy, wait 300s
20240226 17:20:55[INFO] gpu-0 is busy, wait 300s
20240226 17:22:52[INFO] gpu-6 is busy, wait 300s
20240226 17:25:59[INFO] gpu-0 is busy, wait 300s
20240226 17:28:12[INFO] gpu-6 is busy, wait 300s
20240226 17:31:14[INFO] gpu-0 is busy, wait 300s
20240226 17:33:19[INFO] gpu-6 is busy, wait 300s
20240226 17:36:21[INFO] gpu-0 is busy, wait 300s
20240226 17:38:37[INFO] gpu-6 is busy, wait 300s
20240226 17:42:01[INFO] gpu-0 is busy, wait 300s
20240226 17:43:44[INFO] gpu-6 is busy, wait 300s
20240226 17:47:09[INFO] gpu-0 is busy, wait 300s
20240226 17:50:34[INFO] gpu-6 is busy, wait 300s
20240226 17:52:18[INFO] gpu-0 is busy, wait 300s
20240226 17:55:54[INFO] gpu-6 is busy, wait 300s
20240226 17:57:25[INFO] gpu-0 is busy, wait 300s
20240226 18:01:00[INFO] gpu-6 is busy, wait 300s
20240226 18:03:18[INFO] gpu-0 is busy, wait 300s
20240226 18:06:15[INFO] gpu-6 is busy, wait 300s
20240226 18:08:25[INFO] gpu-0 is busy, wait 300s
20240226 18:11:35[INFO] gpu-6 is busy, wait 300s
20240226 18:13:40[INFO] gpu-0 is busy, wait 300s
20240226 18:16:50[INFO] gpu-6 is busy, wait 300s
20240226 18:18:55[INFO] gpu-0 is busy, wait 300s
20240226 18:22:10[INFO] gpu-6 is busy, wait 300s
20240226 18:27:23[INFO] gpu-6 is busy, wait 300s
20240226 18:32:32[INFO] gpu-6 is busy, wait 300s
20240226 18:37:41[INFO] gpu-6 is busy, wait 300s
20240226 18:43:06[INFO] gpu-6 is busy, wait 300s
20240226 18:48:29[INFO] gpu-6 is busy, wait 300s
20240226 18:51:48[INFO] gpu-1 is busy, wait 300s
20240226 18:54:16[INFO] gpu-6 is busy, wait 300s
20240226 18:57:34[INFO] gpu-1 is busy, wait 300s
20240226 18:59:48[INFO] gpu-6 is busy, wait 300s
20240226 19:02:41[INFO] gpu-1 is busy, wait 300s
20240226 19:04:56[INFO] gpu-6 is busy, wait 300s
20240226 19:07:46[INFO] gpu-1 is busy, wait 300s
20240226 19:10:04[INFO] gpu-6 is busy, wait 300s
20240226 19:13:05[INFO] gpu-1 is busy, wait 300s
20240226 19:15:18[INFO] gpu-6 is busy, wait 300s
20240226 19:18:22[INFO] gpu-1 is busy, wait 300s
20240226 19:20:33[INFO] gpu-6 is busy, wait 300s
20240226 19:23:37[INFO] gpu-1 is busy, wait 300s
20240226 19:26:20[INFO] gpu-6 is busy, wait 300s
20240226 19:28:58[INFO] gpu-1 is busy, wait 300s
20240226 19:31:33[INFO] gpu-6 is busy, wait 300s
20240226 19:36:58[INFO] gpu-6 is busy, wait 300s
20240226 19:42:05[INFO] gpu-6 is busy, wait 300s
20240226 19:48:00[INFO] gpu-6 is busy, wait 300s
20240226 19:53:33[INFO] gpu-6 is busy, wait 300s
20240226 19:58:48[INFO] gpu-6 is busy, wait 300s
20240226 20:04:12[INFO] gpu-6 is busy, wait 300s
20240226 20:09:27[INFO] gpu-6 is busy, wait 300s
20240226 20:15:00[INFO] gpu-6 is busy, wait 300s
20240226 20:20:34[INFO] gpu-6 is busy, wait 300s
20240226 20:25:38[INFO] gpu-6 is busy, wait 300s
20240226 20:30:52[INFO] gpu-6 is busy, wait 300s
20240226 20:36:21[INFO] gpu-6 is busy, wait 300s
20240226 20:41:36[INFO] gpu-6 is busy, wait 300s
20240226 20:46:44[INFO] gpu-6 is busy, wait 300s
20240226 20:51:58[INFO] gpu-6 is busy, wait 300s
20240226 20:57:03[INFO] gpu-6 is busy, wait 300s
20240226 21:02:11[INFO] gpu-6 is busy, wait 300s
20240226 21:07:20[INFO] gpu-6 is busy, wait 300s
20240226 21:12:31[INFO] gpu-6 is busy, wait 300s
20240226 21:17:51[INFO] gpu-6 is busy, wait 300s
20240226 21:23:06[INFO] gpu-6 is busy, wait 300s
20240226 21:25:41[INFO] gpu-7 is busy, wait 300s
20240226 21:25:43[INFO] gpu-1 is busy, wait 300s
20240226 21:25:44[INFO] gpu-2 is busy, wait 300s
20240226 21:25:45[INFO] gpu-3 is busy, wait 300s
20240226 21:25:52[INFO] gpu-4 is busy, wait 300s
20240226 21:25:55[INFO] gpu-0 is busy, wait 300s
20240226 21:28:11[INFO] gpu-6 is busy, wait 300s
20240226 21:30:50[INFO] gpu-1 is busy, wait 300s
20240226 21:31:00[INFO] gpu-3 is busy, wait 300s
20240226 21:31:04[INFO] gpu-2 is busy, wait 300s
20240226 21:31:05[INFO] gpu-7 is busy, wait 300s
20240226 21:31:06[INFO] gpu-0 is busy, wait 300s
20240226 21:31:35[INFO] gpu-4 is busy, wait 300s
20240226 21:33:19[INFO] gpu-6 is busy, wait 300s
20240226 21:35:21[INFO] gpu-5 is busy, wait 300s
20240226 21:35:56[INFO] gpu-1 is busy, wait 300s
20240226 21:36:06[INFO] gpu-3 is busy, wait 300s
20240226 21:36:12[INFO] gpu-0 is busy, wait 300s
20240226 21:36:21[INFO] gpu-2 is busy, wait 300s
20240226 21:37:04[INFO] gpu-4 is busy, wait 300s
20240226 21:38:24[INFO] gpu-6 is busy, wait 300s
20240226 21:40:25[INFO] gpu-5 is busy, wait 300s
20240226 21:41:17[INFO] gpu-0 is busy, wait 300s
20240226 21:41:20[INFO] gpu-3 is busy, wait 300s
20240226 21:41:22[INFO] gpu-1 is busy, wait 300s
20240226 21:41:27[INFO] gpu-2 is busy, wait 300s
20240226 21:42:10[INFO] gpu-4 is busy, wait 300s
20240226 21:43:29[INFO] gpu-6 is busy, wait 300s
20240226 21:45:49[INFO] gpu-5 is busy, wait 300s
20240226 21:46:28[INFO] gpu-1 is busy, wait 300s
20240226 21:46:31[INFO] gpu-0 is busy, wait 300s
20240226 21:46:33[INFO] gpu-3 is busy, wait 300s
20240226 21:46:35[INFO] gpu-2 is busy, wait 300s
20240226 21:47:22[INFO] gpu-4 is busy, wait 300s
20240226 21:49:06[INFO] gpu-6 is busy, wait 300s
20240226 21:49:41[INFO] gpu-7 is busy, wait 300s
20240226 21:51:42[INFO] gpu-0 is busy, wait 300s
20240226 21:51:52[INFO] gpu-2 is busy, wait 300s
20240226 21:51:54[INFO] gpu-3 is busy, wait 300s
20240226 21:51:58[INFO] gpu-1 is busy, wait 300s
20240226 21:52:50[INFO] gpu-4 is busy, wait 300s
20240226 21:54:14[INFO] gpu-6 is busy, wait 300s
20240226 21:54:52[INFO] gpu-7 is busy, wait 300s
20240226 21:56:52[INFO] gpu-0 is busy, wait 300s
20240226 21:57:06[INFO] gpu-2 is busy, wait 300s
20240226 21:57:08[INFO] gpu-1 is busy, wait 300s
20240226 21:57:20[INFO] gpu-3 is busy, wait 300s
20240226 21:57:54[INFO] gpu-4 is busy, wait 300s
20240226 21:59:13[INFO] gpu-5 is busy, wait 300s
20240226 21:59:27[INFO] gpu-6 is busy, wait 300s
20240226 22:00:02[INFO] gpu-7 is busy, wait 300s
20240226 22:02:13[INFO] gpu-2 is busy, wait 300s
20240226 22:02:31[INFO] gpu-3 is busy, wait 300s
20240226 22:02:33[INFO] gpu-1 is busy, wait 300s
20240226 22:02:55[INFO] gpu-0 is busy, wait 300s
20240226 22:03:11[INFO] gpu-4 is busy, wait 300s
20240226 22:04:35[INFO] gpu-6 is busy, wait 300s
20240226 22:05:08[INFO] gpu-7 is busy, wait 300s
20240226 22:10:03[INFO] gpu-6 is busy, wait 300s
20240226 22:12:16[INFO] gpu-2 is busy, wait 300s
20240226 22:15:11[INFO] gpu-6 is busy, wait 300s
20240226 22:18:02[INFO] gpu-4 is busy, wait 300s
20240226 22:19:01[INFO] gpu-0 is busy, wait 300s
20240226 22:20:17[INFO] gpu-6 is busy, wait 300s
20240226 22:25:21[INFO] gpu-6 is busy, wait 300s
20240226 22:30:49[INFO] gpu-6 is busy, wait 300s
20240226 22:35:56[INFO] gpu-6 is busy, wait 300s
20240226 22:41:28[INFO] gpu-6 is busy, wait 300s
20240226 22:47:00[INFO] gpu-6 is busy, wait 300s
20240226 22:52:37[INFO] gpu-6 is busy, wait 300s
20240226 22:58:54[INFO] gpu-6 is busy, wait 300s
20240226 23:04:38[INFO] gpu-6 is busy, wait 300s
20240226 23:10:00[INFO] gpu-6 is busy, wait 300s
20240226 23:15:31[INFO] gpu-6 is busy, wait 300s
20240226 23:20:45[INFO] gpu-6 is busy, wait 300s
20240226 23:25:58[INFO] gpu-6 is busy, wait 300s
20240226 23:31:34[INFO] gpu-6 is busy, wait 300s
20240226 23:37:01[INFO] gpu-6 is busy, wait 300s
20240226 23:42:07[INFO] gpu-6 is busy, wait 300s
20240226 23:47:22[INFO] gpu-6 is busy, wait 300s
20240226 23:52:29[INFO] gpu-6 is busy, wait 300s
20240226 23:57:36[INFO] gpu-6 is busy, wait 300s
20240227 00:02:52[INFO] gpu-6 is busy, wait 300s
20240227 00:07:58[INFO] gpu-6 is busy, wait 300s
20240227 00:13:12[INFO] gpu-6 is busy, wait 300s
20240227 00:18:17[INFO] gpu-6 is busy, wait 300s
20240227 00:23:24[INFO] gpu-6 is busy, wait 300s
20240227 00:28:30[INFO] gpu-6 is busy, wait 300s
20240227 00:33:46[INFO] gpu-6 is busy, wait 300s
20240227 00:39:01[INFO] gpu-6 is busy, wait 300s
20240227 00:44:13[INFO] gpu-6 is busy, wait 300s
20240227 00:49:27[INFO] gpu-6 is busy, wait 300s
20240227 00:54:34[INFO] gpu-6 is busy, wait 300s
20240227 00:59:57[INFO] gpu-6 is busy, wait 300s
20240227 01:05:03[INFO] gpu-6 is busy, wait 300s
20240227 01:10:35[INFO] gpu-6 is busy, wait 300s
20240227 01:15:45[INFO] gpu-6 is busy, wait 300s
20240227 01:21:05[INFO] gpu-6 is busy, wait 300s
20240227 01:26:11[INFO] gpu-6 is busy, wait 300s
20240227 01:31:19[INFO] gpu-6 is busy, wait 300s
20240227 01:36:42[INFO] gpu-6 is busy, wait 300s
20240227 01:42:34[INFO] gpu-6 is busy, wait 300s
20240227 01:47:40[INFO] gpu-6 is busy, wait 300s
20240227 01:52:45[INFO] gpu-6 is busy, wait 300s
20240227 01:57:59[INFO] gpu-6 is busy, wait 300s
20240227 02:03:13[INFO] gpu-6 is busy, wait 300s
20240227 02:08:28[INFO] gpu-6 is busy, wait 300s
20240227 02:13:45[INFO] gpu-6 is busy, wait 300s
20240227 02:18:52[INFO] gpu-6 is busy, wait 300s
20240227 02:24:03[INFO] gpu-6 is busy, wait 300s
20240227 02:29:50[INFO] gpu-6 is busy, wait 300s
20240227 02:35:21[INFO] gpu-6 is busy, wait 300s
20240227 02:41:08[INFO] gpu-6 is busy, wait 300s
20240227 02:46:25[INFO] gpu-6 is busy, wait 300s
20240227 02:51:39[INFO] gpu-6 is busy, wait 300s
20240227 02:56:56[INFO] gpu-6 is busy, wait 300s
20240227 03:02:28[INFO] gpu-6 is busy, wait 300s
20240227 03:07:54[INFO] gpu-6 is busy, wait 300s
20240227 03:14:16[INFO] gpu-6 is busy, wait 300s
20240227 03:19:24[INFO] gpu-6 is busy, wait 300s
20240227 03:24:31[INFO] gpu-6 is busy, wait 300s
20240227 03:30:58[INFO] gpu-6 is busy, wait 300s
20240227 03:36:25[INFO] gpu-6 is busy, wait 300s
20240227 03:42:04[INFO] gpu-6 is busy, wait 300s
20240227 03:47:45[INFO] gpu-6 is busy, wait 300s
20240227 03:53:04[INFO] gpu-6 is busy, wait 300s
20240227 03:58:11[INFO] gpu-6 is busy, wait 300s
20240227 04:03:25[INFO] gpu-6 is busy, wait 300s
20240227 04:09:01[INFO] gpu-6 is busy, wait 300s
20240227 04:14:07[INFO] gpu-6 is busy, wait 300s
20240227 04:19:30[INFO] gpu-6 is busy, wait 300s
20240227 04:24:39[INFO] gpu-6 is busy, wait 300s
20240227 04:30:02[INFO] gpu-6 is busy, wait 300s
20240227 04:35:09[INFO] gpu-6 is busy, wait 300s
20240227 04:40:15[INFO] gpu-6 is busy, wait 300s
20240227 04:45:24[INFO] gpu-6 is busy, wait 300s
20240227 04:50:39[INFO] gpu-6 is busy, wait 300s
20240227 04:55:44[INFO] gpu-6 is busy, wait 300s
20240227 05:00:51[INFO] gpu-6 is busy, wait 300s
20240227 05:06:06[INFO] gpu-6 is busy, wait 300s
20240227 05:11:44[INFO] gpu-6 is busy, wait 300s
20240227 05:17:07[INFO] gpu-6 is busy, wait 300s
20240227 05:22:12[INFO] gpu-6 is busy, wait 300s
20240227 05:28:09[INFO] gpu-6 is busy, wait 300s
20240227 05:33:55[INFO] gpu-6 is busy, wait 300s
20240227 05:39:00[INFO] gpu-6 is busy, wait 300s
20240227 05:44:30[INFO] gpu-6 is busy, wait 300s
20240227 05:49:39[INFO] gpu-6 is busy, wait 300s
20240227 05:54:47[INFO] gpu-6 is busy, wait 300s
20240227 05:59:59[INFO] gpu-6 is busy, wait 300s
20240227 06:05:06[INFO] gpu-6 is busy, wait 300s
20240227 06:10:12[INFO] gpu-6 is busy, wait 300s
20240227 06:15:42[INFO] gpu-6 is busy, wait 300s
20240227 06:20:49[INFO] gpu-6 is busy, wait 300s
20240227 06:26:10[INFO] gpu-6 is busy, wait 300s
20240227 06:31:23[INFO] gpu-6 is busy, wait 300s
20240227 06:37:02[INFO] gpu-6 is busy, wait 300s
20240227 06:42:30[INFO] gpu-6 is busy, wait 300s
20240227 06:47:36[INFO] gpu-6 is busy, wait 300s
20240227 06:53:24[INFO] gpu-6 is busy, wait 300s
20240227 06:58:31[INFO] gpu-6 is busy, wait 300s
20240227 07:03:57[INFO] gpu-6 is busy, wait 300s
20240227 07:09:09[INFO] gpu-6 is busy, wait 300s
20240227 07:14:29[INFO] gpu-6 is busy, wait 300s
20240227 07:19:52[INFO] gpu-6 is busy, wait 300s
20240227 07:25:46[INFO] gpu-6 is busy, wait 300s
20240227 07:31:11[INFO] gpu-6 is busy, wait 300s
20240227 07:36:30[INFO] gpu-6 is busy, wait 300s
20240227 07:41:39[INFO] gpu-6 is busy, wait 300s
20240227 07:46:58[INFO] gpu-6 is busy, wait 300s
20240227 07:52:22[INFO] gpu-6 is busy, wait 300s
20240227 07:57:38[INFO] gpu-6 is busy, wait 300s
20240227 08:03:39[INFO] gpu-6 is busy, wait 300s
20240227 08:09:04[INFO] gpu-6 is busy, wait 300s
20240227 08:14:18[INFO] gpu-6 is busy, wait 300s
20240227 08:19:31[INFO] gpu-6 is busy, wait 300s
20240227 08:25:18[INFO] gpu-6 is busy, wait 300s
20240227 08:30:50[INFO] gpu-6 is busy, wait 300s
20240227 08:36:04[INFO] gpu-6 is busy, wait 300s
20240227 08:41:09[INFO] gpu-6 is busy, wait 300s
20240227 08:46:14[INFO] gpu-6 is busy, wait 300s
20240227 08:51:23[INFO] gpu-6 is busy, wait 300s
20240227 08:56:30[INFO] gpu-6 is busy, wait 300s
20240227 09:01:35[INFO] gpu-6 is busy, wait 300s
20240227 09:06:41[INFO] gpu-6 is busy, wait 300s
20240227 09:11:49[INFO] gpu-6 is busy, wait 300s
20240227 09:17:22[INFO] gpu-6 is busy, wait 300s
20240227 09:22:58[INFO] gpu-6 is busy, wait 300s
20240227 09:28:31[INFO] gpu-6 is busy, wait 300s
20240227 09:33:48[INFO] gpu-6 is busy, wait 300s
20240227 09:38:56[INFO] gpu-6 is busy, wait 300s
20240227 09:44:10[INFO] gpu-6 is busy, wait 300s
20240227 09:49:17[INFO] gpu-6 is busy, wait 300s
20240227 09:54:30[INFO] gpu-6 is busy, wait 300s
20240227 09:59:38[INFO] gpu-6 is busy, wait 300s
20240227 10:04:45[INFO] gpu-6 is busy, wait 300s
20240227 10:09:52[INFO] gpu-6 is busy, wait 300s
20240227 10:15:31[INFO] gpu-6 is busy, wait 300s
20240227 10:21:09[INFO] gpu-6 is busy, wait 300s
20240227 10:26:15[INFO] gpu-6 is busy, wait 300s
20240227 10:31:37[INFO] gpu-6 is busy, wait 300s
20240227 10:36:45[INFO] gpu-6 is busy, wait 300s
20240227 10:42:03[INFO] gpu-6 is busy, wait 300s
20240227 10:47:22[INFO] gpu-6 is busy, wait 300s
20240227 10:52:30[INFO] gpu-6 is busy, wait 300s
20240227 10:57:55[INFO] gpu-6 is busy, wait 300s
20240227 11:03:04[INFO] gpu-6 is busy, wait 300s
20240227 11:08:28[INFO] gpu-6 is busy, wait 300s
20240227 11:14:10[INFO] gpu-6 is busy, wait 300s
20240227 11:19:17[INFO] gpu-6 is busy, wait 300s
20240227 11:21:03[INFO] gpu-7 is busy, wait 300s
20240227 11:22:38[INFO] gpu-3 is busy, wait 300s
20240227 11:24:24[INFO] gpu-6 is busy, wait 300s
20240227 11:25:28[INFO] gpu-2 is busy, wait 300s
20240227 11:25:30[INFO] gpu-1 is busy, wait 300s
20240227 11:25:31[INFO] gpu-0 is busy, wait 300s
20240227 11:25:34[INFO] gpu-4 is busy, wait 300s
20240227 11:25:35[INFO] gpu-5 is busy, wait 300s
20240227 11:26:17[INFO] gpu-7 is busy, wait 300s
20240227 11:27:39[INFO] gpu-3 is busy, wait 300s
20240227 11:29:32[INFO] gpu-6 is busy, wait 300s
20240227 11:30:29[INFO] gpu-2 is busy, wait 300s
20240227 11:30:31[INFO] gpu-1 is busy, wait 300s
20240227 11:30:31[INFO] gpu-0 is busy, wait 300s
20240227 11:30:35[INFO] gpu-4 is busy, wait 300s
20240227 11:30:36[INFO] gpu-5 is busy, wait 300s
20240227 11:31:25[INFO] gpu-7 is busy, wait 300s
20240227 11:32:40[INFO] gpu-3 is busy, wait 300s
20240227 11:34:46[INFO] gpu-6 is busy, wait 300s
20240227 11:35:29[INFO] gpu-2 is busy, wait 300s
20240227 11:35:31[INFO] gpu-1 is busy, wait 300s
20240227 11:35:32[INFO] gpu-0 is busy, wait 300s
20240227 11:40:07[INFO] gpu-6 is busy, wait 300s
20240227 11:46:20[INFO] gpu-6 is busy, wait 300s
20240227 11:51:37[INFO] gpu-6 is busy, wait 300s
20240227 11:56:59[INFO] gpu-6 is busy, wait 300s
20240227 12:02:29[INFO] gpu-6 is busy, wait 300s
20240227 12:07:36[INFO] gpu-6 is busy, wait 300s
20240227 12:12:52[INFO] gpu-6 is busy, wait 300s
20240227 12:18:00[INFO] gpu-6 is busy, wait 300s
20240227 12:21:42[INFO] gpu-2 is busy, wait 300s
20240227 12:22:43[INFO] gpu-1 is busy, wait 300s
20240227 12:22:44[INFO] gpu-3 is busy, wait 300s
20240227 12:22:47[INFO] gpu-0 is busy, wait 300s
20240227 12:23:06[INFO] gpu-6 is busy, wait 300s
20240227 12:28:41[INFO] gpu-6 is busy, wait 300s
20240227 12:33:47[INFO] gpu-6 is busy, wait 300s
20240227 12:39:03[INFO] gpu-6 is busy, wait 300s
20240227 12:44:15[INFO] gpu-6 is busy, wait 300s
20240227 12:49:22[INFO] gpu-6 is busy, wait 300s
20240227 12:55:12[INFO] gpu-6 is busy, wait 300s
20240227 13:00:26[INFO] gpu-6 is busy, wait 300s
20240227 13:05:31[INFO] gpu-6 is busy, wait 300s
20240227 13:10:38[INFO] gpu-6 is busy, wait 300s
20240227 13:15:43[INFO] gpu-6 is busy, wait 300s
20240227 13:21:04[INFO] gpu-6 is busy, wait 300s
20240227 13:26:12[INFO] gpu-6 is busy, wait 300s
20240227 13:31:55[INFO] gpu-6 is busy, wait 300s
20240227 13:37:22[INFO] gpu-6 is busy, wait 300s
20240227 13:42:28[INFO] gpu-6 is busy, wait 300s
20240227 13:47:52[INFO] gpu-6 is busy, wait 300s
20240227 13:53:00[INFO] gpu-6 is busy, wait 300s
20240227 13:59:15[INFO] gpu-6 is busy, wait 300s
20240227 14:04:38[INFO] gpu-6 is busy, wait 300s
20240227 14:09:55[INFO] gpu-6 is busy, wait 300s
20240227 14:15:21[INFO] gpu-6 is busy, wait 300s
20240227 14:20:29[INFO] gpu-6 is busy, wait 300s
20240227 14:25:48[INFO] gpu-6 is busy, wait 300s
20240227 14:30:54[INFO] gpu-6 is busy, wait 300s
20240227 14:36:03[INFO] gpu-6 is busy, wait 300s
20240227 14:39:58[INFO] gpu-2 is busy, wait 300s
20240227 14:40:01[INFO] gpu-1 is busy, wait 300s
20240227 14:40:02[INFO] gpu-0 is busy, wait 300s
20240227 14:40:14[INFO] gpu-3 is busy, wait 300s
20240227 14:41:18[INFO] gpu-6 is busy, wait 300s
20240227 14:42:05[INFO] gpu-4 is busy, wait 300s
20240227 14:42:06[INFO] gpu-5 is busy, wait 300s
20240227 14:44:58[INFO] gpu-2 is busy, wait 300s
20240227 14:45:02[INFO] gpu-1 is busy, wait 300s
20240227 14:45:03[INFO] gpu-0 is busy, wait 300s
20240227 14:45:14[INFO] gpu-3 is busy, wait 300s
20240227 14:46:23[INFO] gpu-6 is busy, wait 300s
20240227 14:51:48[INFO] gpu-6 is busy, wait 300s
20240227 14:56:56[INFO] gpu-6 is busy, wait 300s
20240227 15:02:22[INFO] gpu-6 is busy, wait 300s
20240227 15:07:51[INFO] gpu-6 is busy, wait 300s
20240227 15:11:40[INFO] gpu-0 is busy, wait 300s
20240227 15:13:02[INFO] gpu-6 is busy, wait 300s
20240227 15:16:51[INFO] gpu-0 is busy, wait 300s
20240227 15:18:16[INFO] gpu-6 is busy, wait 300s
20240227 15:23:32[INFO] gpu-6 is busy, wait 300s
20240227 15:28:41[INFO] gpu-6 is busy, wait 300s
20240227 15:33:55[INFO] gpu-6 is busy, wait 300s
20240227 15:39:01[INFO] gpu-6 is busy, wait 300s
20240227 15:44:14[INFO] gpu-6 is busy, wait 300s
20240227 15:49:27[INFO] gpu-6 is busy, wait 300s
20240227 15:54:59[INFO] gpu-6 is busy, wait 300s
20240227 16:00:08[INFO] gpu-6 is busy, wait 300s
20240227 16:05:55[INFO] gpu-6 is busy, wait 300s
20240227 16:11:10[INFO] gpu-6 is busy, wait 300s
20240227 16:11:55[INFO] gpu-3 is busy, wait 300s
20240227 16:11:59[INFO] gpu-1 is busy, wait 300s
20240227 16:12:00[INFO] gpu-4 is busy, wait 300s
20240227 16:12:00[INFO] gpu-7 is busy, wait 300s
20240227 16:12:02[INFO] gpu-5 is busy, wait 300s
20240227 16:12:02[INFO] gpu-2 is busy, wait 300s
20240227 16:12:04[INFO] gpu-0 is busy, wait 300s
20240227 16:16:17[INFO] gpu-6 is busy, wait 300s
20240227 16:22:09[INFO] gpu-6 is busy, wait 300s
20240227 16:24:27[INFO] gpu-2 is busy, wait 300s
20240227 16:24:30[INFO] gpu-4 is busy, wait 300s
20240227 16:24:31[INFO] gpu-3 is busy, wait 300s
20240227 16:24:32[INFO] gpu-0 is busy, wait 300s
20240227 16:24:35[INFO] gpu-7 is busy, wait 300s
20240227 16:24:36[INFO] gpu-5 is busy, wait 300s
20240227 16:24:47[INFO] gpu-1 is busy, wait 300s
20240227 16:27:17[INFO] gpu-6 is busy, wait 300s
20240227 16:32:20[INFO] gpu-1 is busy, wait 300s
20240227 16:32:25[INFO] gpu-6 is busy, wait 300s
20240227 16:32:30[INFO] gpu-2 is busy, wait 300s
20240227 16:32:38[INFO] gpu-4 is busy, wait 300s
20240227 16:32:45[INFO] gpu-3 is busy, wait 300s
20240227 16:32:49[INFO] gpu-5 is busy, wait 300s
20240227 16:35:29[INFO] gpu-0 is busy, wait 300s
20240227 16:35:31[INFO] gpu-7 is busy, wait 300s
20240227 16:37:33[INFO] gpu-6 is busy, wait 300s
20240227 16:37:43[INFO] gpu-2 is busy, wait 300s
20240227 16:37:44[INFO] gpu-1 is busy, wait 300s
20240227 16:37:45[INFO] gpu-4 is busy, wait 300s
20240227 16:37:50[INFO] gpu-3 is busy, wait 300s
20240227 16:37:54[INFO] gpu-5 is busy, wait 300s
20240227 16:40:30[INFO] gpu-0 is busy, wait 300s
20240227 16:40:49[INFO] gpu-7 is busy, wait 300s
20240227 16:42:34[INFO] gpu-6 is busy, wait 300s
20240227 16:42:44[INFO] gpu-2 is busy, wait 300s
20240227 16:42:45[INFO] gpu-1 is busy, wait 300s
20240227 16:42:46[INFO] gpu-4 is busy, wait 300s
20240227 16:42:51[INFO] gpu-3 is busy, wait 300s
20240227 16:42:55[INFO] gpu-5 is busy, wait 300s
20240227 16:45:31[INFO] gpu-0 is busy, wait 300s
20240227 16:46:16[INFO] gpu-7 is busy, wait 300s
20240227 16:47:34[INFO] gpu-6 is busy, wait 300s
20240227 16:47:44[INFO] gpu-2 is busy, wait 300s
20240227 16:47:46[INFO] gpu-1 is busy, wait 300s
20240227 16:47:47[INFO] gpu-4 is busy, wait 300s
20240227 16:47:52[INFO] gpu-3 is busy, wait 300s
20240227 16:47:56[INFO] gpu-5 is busy, wait 300s
20240227 16:50:32[INFO] gpu-0 is busy, wait 300s
20240227 16:51:17[INFO] gpu-7 is busy, wait 300s
20240227 16:52:35[INFO] gpu-6 is busy, wait 300s
20240227 16:52:45[INFO] gpu-2 is busy, wait 300s
20240227 16:52:47[INFO] gpu-1 is busy, wait 300s
20240227 16:52:48[INFO] gpu-4 is busy, wait 300s
20240227 16:52:53[INFO] gpu-3 is busy, wait 300s
20240227 16:52:56[INFO] gpu-5 is busy, wait 300s
20240227 16:55:32[INFO] gpu-0 is busy, wait 300s
20240227 16:56:40[INFO] gpu-7 is busy, wait 300s
20240227 16:57:36[INFO] gpu-6 is busy, wait 300s
20240227 16:57:46[INFO] gpu-2 is busy, wait 300s
20240227 16:57:47[INFO] gpu-1 is busy, wait 300s
20240227 16:57:48[INFO] gpu-4 is busy, wait 300s
20240227 16:57:53[INFO] gpu-3 is busy, wait 300s
20240227 16:57:57[INFO] gpu-5 is busy, wait 300s
20240227 17:00:33[INFO] gpu-0 is busy, wait 300s
20240227 17:01:45[INFO] gpu-7 is busy, wait 300s
20240227 17:02:36[INFO] gpu-6 is busy, wait 300s
20240227 17:02:46[INFO] gpu-2 is busy, wait 300s
20240227 17:02:48[INFO] gpu-1 is busy, wait 300s
20240227 17:02:49[INFO] gpu-4 is busy, wait 300s
20240227 17:02:54[INFO] gpu-3 is busy, wait 300s
20240227 17:02:57[INFO] gpu-5 is busy, wait 300s
20240227 17:05:33[INFO] gpu-0 is busy, wait 300s
20240227 17:06:52[INFO] gpu-7 is busy, wait 300s
20240227 17:07:37[INFO] gpu-6 is busy, wait 300s
20240227 17:07:47[INFO] gpu-2 is busy, wait 300s
20240227 17:07:48[INFO] gpu-1 is busy, wait 300s
20240227 17:07:49[INFO] gpu-4 is busy, wait 300s
20240227 17:07:55[INFO] gpu-3 is busy, wait 300s
20240227 17:07:58[INFO] gpu-5 is busy, wait 300s
20240227 17:10:34[INFO] gpu-0 is busy, wait 300s
20240227 17:11:59[INFO] gpu-7 is busy, wait 300s
20240227 17:12:37[INFO] gpu-6 is busy, wait 300s
20240227 17:12:47[INFO] gpu-2 is busy, wait 300s
20240227 17:12:49[INFO] gpu-1 is busy, wait 300s
20240227 17:12:50[INFO] gpu-4 is busy, wait 300s
20240227 17:12:55[INFO] gpu-3 is busy, wait 300s
20240227 17:12:59[INFO] gpu-5 is busy, wait 300s
20240227 17:15:35[INFO] gpu-0 is busy, wait 300s
20240227 17:17:22[INFO] gpu-7 is busy, wait 300s
20240227 17:17:38[INFO] gpu-6 is busy, wait 300s
20240227 17:17:48[INFO] gpu-2 is busy, wait 300s
20240227 17:17:50[INFO] gpu-1 is busy, wait 300s
20240227 17:17:51[INFO] gpu-4 is busy, wait 300s
20240227 17:17:56[INFO] gpu-3 is busy, wait 300s
20240227 17:17:59[INFO] gpu-5 is busy, wait 300s
20240227 17:20:35[INFO] gpu-0 is busy, wait 300s
20240227 17:22:27[INFO] gpu-7 is busy, wait 300s
20240227 17:22:39[INFO] gpu-6 is busy, wait 300s
20240227 17:22:49[INFO] gpu-2 is busy, wait 300s
20240227 17:22:50[INFO] gpu-1 is busy, wait 300s
20240227 17:22:51[INFO] gpu-4 is busy, wait 300s
20240227 17:22:56[INFO] gpu-3 is busy, wait 300s
20240227 17:23:00[INFO] gpu-5 is busy, wait 300s
20240227 17:25:36[INFO] gpu-0 is busy, wait 300s
20240227 17:27:35[INFO] gpu-7 is busy, wait 300s
20240227 17:27:39[INFO] gpu-6 is busy, wait 300s
20240227 17:27:49[INFO] gpu-2 is busy, wait 300s
20240227 17:27:51[INFO] gpu-1 is busy, wait 300s
20240227 17:27:52[INFO] gpu-4 is busy, wait 300s
20240227 17:27:57[INFO] gpu-3 is busy, wait 300s
20240227 17:28:00[INFO] gpu-5 is busy, wait 300s
20240227 17:30:36[INFO] gpu-0 is busy, wait 300s
20240227 17:32:40[INFO] gpu-6 is busy, wait 300s
20240227 17:32:50[INFO] gpu-2 is busy, wait 300s
20240227 17:32:51[INFO] gpu-1 is busy, wait 300s
20240227 17:32:52[INFO] gpu-4 is busy, wait 300s
20240227 17:32:53[INFO] gpu-7 is busy, wait 300s
20240227 17:32:58[INFO] gpu-3 is busy, wait 300s
20240227 17:33:01[INFO] gpu-5 is busy, wait 300s
20240227 17:37:03[INFO] gpu-0 is busy, wait 300s
20240227 17:37:46[INFO] gpu-6 is busy, wait 300s
20240227 17:38:38[INFO] gpu-5 is busy, wait 300s
20240227 17:38:41[INFO] gpu-3 is busy, wait 300s
20240227 17:38:43[INFO] gpu-1 is busy, wait 300s
20240227 17:38:47[INFO] gpu-7 is busy, wait 300s
20240227 17:38:54[INFO] gpu-4 is busy, wait 300s
20240227 17:39:02[INFO] gpu-2 is busy, wait 300s
20240227 17:42:37[INFO] gpu-0 is busy, wait 300s
20240227 17:42:54[INFO] gpu-6 is busy, wait 300s
20240227 17:43:58[INFO] gpu-1 is busy, wait 300s
20240227 17:43:58[INFO] gpu-4 is busy, wait 300s
20240227 17:43:59[INFO] gpu-3 is busy, wait 300s
20240227 17:44:01[INFO] gpu-5 is busy, wait 300s
20240227 17:44:10[INFO] gpu-2 is busy, wait 300s
20240227 17:44:16[INFO] gpu-7 is busy, wait 300s
20240227 17:47:54[INFO] gpu-0 is busy, wait 300s
20240227 17:48:03[INFO] gpu-6 is busy, wait 300s
20240227 17:49:09[INFO] gpu-3 is busy, wait 300s
20240227 17:49:16[INFO] gpu-4 is busy, wait 300s
20240227 17:49:17[INFO] gpu-2 is busy, wait 300s
20240227 17:49:31[INFO] gpu-7 is busy, wait 300s
20240227 17:49:31[INFO] gpu-1 is busy, wait 300s
20240227 17:49:33[INFO] gpu-5 is busy, wait 300s
20240227 17:53:03[INFO] gpu-0 is busy, wait 300s
20240227 17:53:08[INFO] gpu-6 is busy, wait 300s
20240227 17:54:16[INFO] gpu-3 is busy, wait 300s
20240227 17:54:20[INFO] gpu-4 is busy, wait 300s
20240227 17:54:36[INFO] gpu-1 is busy, wait 300s
20240227 17:54:41[INFO] gpu-2 is busy, wait 300s
20240227 17:54:44[INFO] gpu-7 is busy, wait 300s
20240227 17:54:44[INFO] gpu-5 is busy, wait 300s
20240227 17:58:04[INFO] gpu-0 is busy, wait 300s
20240227 17:58:23[INFO] gpu-6 is busy, wait 300s
20240227 17:59:20[INFO] gpu-3 is busy, wait 300s
20240227 17:59:35[INFO] gpu-4 is busy, wait 300s
20240227 17:59:42[INFO] gpu-1 is busy, wait 300s
20240227 17:59:55[INFO] gpu-2 is busy, wait 300s
20240227 17:59:55[INFO] gpu-7 is busy, wait 300s
20240227 18:00:27[INFO] gpu-5 is busy, wait 300s
20240227 18:03:22[INFO] gpu-0 is busy, wait 300s
20240227 18:03:39[INFO] gpu-6 is busy, wait 300s
20240227 18:04:26[INFO] gpu-3 is busy, wait 300s
20240227 18:04:39[INFO] gpu-4 is busy, wait 300s
20240227 18:04:57[INFO] gpu-1 is busy, wait 300s
20240227 18:05:08[INFO] gpu-7 is busy, wait 300s
20240227 18:05:19[INFO] gpu-2 is busy, wait 300s
20240227 18:05:32[INFO] gpu-5 is busy, wait 300s
20240227 18:08:34[INFO] gpu-0 is busy, wait 300s
20240227 18:08:45[INFO] gpu-6 is busy, wait 300s
20240227 18:09:36[INFO] gpu-3 is busy, wait 300s
20240227 18:09:49[INFO] gpu-4 is busy, wait 300s
20240227 18:10:16[INFO] gpu-7 is busy, wait 300s
20240227 18:10:39[INFO] gpu-5 is busy, wait 300s
20240227 18:10:40[INFO] gpu-1 is busy, wait 300s
20240227 18:10:42[INFO] gpu-2 is busy, wait 300s
20240227 18:13:35[INFO] gpu-0 is busy, wait 300s
20240227 18:13:57[INFO] gpu-6 is busy, wait 300s
20240227 18:14:41[INFO] gpu-3 is busy, wait 300s
20240227 18:14:54[INFO] gpu-4 is busy, wait 300s
20240227 18:15:29[INFO] gpu-7 is busy, wait 300s
20240227 18:15:44[INFO] gpu-5 is busy, wait 300s
20240227 18:15:52[INFO] gpu-1 is busy, wait 300s
20240227 18:16:19[INFO] gpu-2 is busy, wait 300s
20240227 18:19:04[INFO] gpu-6 is busy, wait 300s
20240227 18:19:10[INFO] gpu-0 is busy, wait 300s
20240227 18:19:45[INFO] gpu-3 is busy, wait 300s
20240227 18:20:04[INFO] gpu-4 is busy, wait 300s
20240227 18:20:35[INFO] gpu-7 is busy, wait 300s
20240227 18:21:03[INFO] gpu-5 is busy, wait 300s
20240227 18:21:04[INFO] gpu-1 is busy, wait 300s
20240227 18:21:24[INFO] gpu-2 is busy, wait 300s
20240227 18:24:12[INFO] gpu-6 is busy, wait 300s
20240227 18:24:35[INFO] gpu-0 is busy, wait 300s
20240227 18:24:51[INFO] gpu-3 is busy, wait 300s
20240227 18:25:16[INFO] gpu-4 is busy, wait 300s
20240227 18:25:51[INFO] gpu-7 is busy, wait 300s
20240227 18:26:16[INFO] gpu-1 is busy, wait 300s
20240227 18:26:30[INFO] gpu-5 is busy, wait 300s
20240227 18:26:36[INFO] gpu-2 is busy, wait 300s
20240227 18:29:24[INFO] gpu-6 is busy, wait 300s
20240227 18:29:54[INFO] gpu-0 is busy, wait 300s
20240227 18:29:58[INFO] gpu-3 is busy, wait 300s
20240227 18:30:41[INFO] gpu-4 is busy, wait 300s
20240227 18:31:17[INFO] gpu-7 is busy, wait 300s
20240227 18:31:24[INFO] gpu-1 is busy, wait 300s
20240227 18:31:37[INFO] gpu-5 is busy, wait 300s
20240227 18:32:01[INFO] gpu-2 is busy, wait 300s
20240227 18:34:30[INFO] gpu-6 is busy, wait 300s
20240227 18:35:05[INFO] gpu-0 is busy, wait 300s
20240227 18:35:09[INFO] gpu-3 is busy, wait 300s
20240227 18:35:57[INFO] gpu-4 is busy, wait 300s
20240227 18:36:29[INFO] gpu-1 is busy, wait 300s
20240227 18:36:40[INFO] gpu-7 is busy, wait 300s
20240227 18:36:48[INFO] gpu-5 is busy, wait 300s
20240227 18:37:05[INFO] gpu-2 is busy, wait 300s
20240227 18:40:08[INFO] gpu-6 is busy, wait 300s
20240227 18:40:13[INFO] gpu-3 is busy, wait 300s
20240227 18:40:25[INFO] gpu-0 is busy, wait 300s
20240227 18:41:03[INFO] gpu-4 is busy, wait 300s
20240227 18:41:54[INFO] gpu-1 is busy, wait 300s
20240227 18:42:07[INFO] gpu-7 is busy, wait 300s
20240227 18:42:08[INFO] gpu-5 is busy, wait 300s
20240227 18:42:18[INFO] gpu-2 is busy, wait 300s
20240227 18:45:33[INFO] gpu-6 is busy, wait 300s
20240227 18:45:53[INFO] gpu-3 is busy, wait 300s
20240227 18:46:01[INFO] gpu-0 is busy, wait 300s
20240227 18:46:07[INFO] gpu-4 is busy, wait 300s
20240227 18:47:32[INFO] gpu-1 is busy, wait 300s
20240227 18:47:32[INFO] gpu-7 is busy, wait 300s
20240227 18:47:39[INFO] gpu-5 is busy, wait 300s
20240227 18:47:41[INFO] gpu-2 is busy, wait 300s
20240227 18:50:58[INFO] gpu-3 is busy, wait 300s
20240227 18:51:01[INFO] gpu-6 is busy, wait 300s
20240227 18:51:12[INFO] gpu-4 is busy, wait 300s
20240227 18:51:22[INFO] gpu-0 is busy, wait 300s
20240227 18:52:39[INFO] gpu-1 is busy, wait 300s
20240227 18:52:54[INFO] gpu-2 is busy, wait 300s
20240227 18:53:00[INFO] gpu-5 is busy, wait 300s
20240227 18:53:38[INFO] gpu-7 is busy, wait 300s
20240227 18:56:15[INFO] gpu-6 is busy, wait 300s
20240227 18:56:16[INFO] gpu-3 is busy, wait 300s
20240227 18:56:26[INFO] gpu-4 is busy, wait 300s
20240227 18:56:28[INFO] gpu-0 is busy, wait 300s
20240227 18:58:09[INFO] gpu-1 is busy, wait 300s
20240227 18:58:35[INFO] gpu-2 is busy, wait 300s
20240227 18:58:43[INFO] gpu-5 is busy, wait 300s
20240227 18:58:49[INFO] gpu-7 is busy, wait 300s
20240227 19:01:20[INFO] gpu-6 is busy, wait 300s
20240227 19:01:36[INFO] gpu-0 is busy, wait 300s
20240227 19:01:49[INFO] gpu-3 is busy, wait 300s
20240227 19:01:56[INFO] gpu-4 is busy, wait 300s
20240227 19:03:13[INFO] gpu-1 is busy, wait 300s
20240227 19:03:41[INFO] gpu-2 is busy, wait 300s
20240227 19:04:07[INFO] gpu-5 is busy, wait 300s
20240227 19:04:11[INFO] gpu-7 is busy, wait 300s
20240227 19:06:44[INFO] gpu-0 is busy, wait 300s
20240227 19:06:47[INFO] gpu-6 is busy, wait 300s
20240227 19:06:57[INFO] gpu-3 is busy, wait 300s
20240227 19:07:20[INFO] gpu-4 is busy, wait 300s
20240227 19:08:28[INFO] gpu-1 is busy, wait 300s
20240227 19:09:03[INFO] gpu-2 is busy, wait 300s
20240227 19:09:20[INFO] gpu-5 is busy, wait 300s
20240227 19:09:23[INFO] gpu-7 is busy, wait 300s
20240227 19:11:50[INFO] gpu-0 is busy, wait 300s
20240227 19:11:56[INFO] gpu-6 is busy, wait 300s
20240227 19:12:07[INFO] gpu-3 is busy, wait 300s
20240227 19:12:45[INFO] gpu-4 is busy, wait 300s
20240227 19:13:34[INFO] gpu-1 is busy, wait 300s
20240227 19:14:11[INFO] gpu-2 is busy, wait 300s
20240227 19:14:25[INFO] gpu-5 is busy, wait 300s
20240227 19:14:36[INFO] gpu-7 is busy, wait 300s
20240227 19:17:05[INFO] gpu-0 is busy, wait 300s
20240227 19:17:13[INFO] gpu-6 is busy, wait 300s
20240227 19:17:14[INFO] gpu-3 is busy, wait 300s
20240227 19:17:51[INFO] gpu-4 is busy, wait 300s
20240227 19:18:38[INFO] gpu-1 is busy, wait 300s
20240227 19:19:19[INFO] gpu-2 is busy, wait 300s
20240227 19:19:30[INFO] gpu-5 is busy, wait 300s
20240227 19:19:50[INFO] gpu-7 is busy, wait 300s
20240227 19:22:11[INFO] gpu-0 is busy, wait 300s
20240227 19:22:29[INFO] gpu-6 is busy, wait 300s
20240227 19:22:32[INFO] gpu-3 is busy, wait 300s
20240227 19:23:15[INFO] gpu-4 is busy, wait 300s
20240227 19:23:43[INFO] gpu-1 is busy, wait 300s
20240227 19:24:46[INFO] gpu-2 is busy, wait 300s
20240227 19:24:50[INFO] gpu-5 is busy, wait 300s
20240227 19:25:07[INFO] gpu-7 is busy, wait 300s
20240227 19:27:17[INFO] gpu-0 is busy, wait 300s
20240227 19:27:37[INFO] gpu-3 is busy, wait 300s
20240227 19:28:05[INFO] gpu-6 is busy, wait 300s
20240227 19:28:23[INFO] gpu-4 is busy, wait 300s
20240227 19:28:56[INFO] gpu-1 is busy, wait 300s
20240227 19:29:51[INFO] gpu-2 is busy, wait 300s
20240227 19:30:02[INFO] gpu-5 is busy, wait 300s
20240227 19:30:13[INFO] gpu-7 is busy, wait 300s
20240227 19:32:23[INFO] gpu-0 is busy, wait 300s
20240227 19:32:44[INFO] gpu-3 is busy, wait 300s
20240227 19:33:53[INFO] gpu-4 is busy, wait 300s
20240227 19:33:55[INFO] gpu-6 is busy, wait 300s
20240227 19:34:09[INFO] gpu-1 is busy, wait 300s
20240227 19:35:07[INFO] gpu-5 is busy, wait 300s
20240227 19:35:10[INFO] gpu-2 is busy, wait 300s
20240227 19:35:26[INFO] gpu-7 is busy, wait 300s
20240227 19:37:28[INFO] gpu-0 is busy, wait 300s
20240227 19:37:56[INFO] gpu-3 is busy, wait 300s
20240227 19:39:00[INFO] gpu-6 is busy, wait 300s
20240227 19:39:15[INFO] gpu-1 is busy, wait 300s
20240227 19:39:23[INFO] gpu-4 is busy, wait 300s
20240227 19:40:20[INFO] gpu-5 is busy, wait 300s
20240227 19:40:26[INFO] gpu-2 is busy, wait 300s
20240227 19:40:34[INFO] gpu-7 is busy, wait 300s
20240227 19:42:42[INFO] gpu-0 is busy, wait 300s
20240227 19:43:02[INFO] gpu-3 is busy, wait 300s
20240227 19:44:48[INFO] gpu-6 is busy, wait 300s
20240227 19:44:58[INFO] gpu-4 is busy, wait 300s
20240227 19:44:59[INFO] gpu-1 is busy, wait 300s
20240227 19:45:47[INFO] gpu-7 is busy, wait 300s
20240227 19:45:59[INFO] gpu-2 is busy, wait 300s
20240227 19:46:02[INFO] gpu-5 is busy, wait 300s
20240227 19:47:47[INFO] gpu-0 is busy, wait 300s
20240227 19:48:10[INFO] gpu-3 is busy, wait 300s
20240227 19:50:05[INFO] gpu-6 is busy, wait 300s
20240227 19:50:14[INFO] gpu-1 is busy, wait 300s
20240227 19:50:16[INFO] gpu-4 is busy, wait 300s
20240227 19:51:16[INFO] gpu-2 is busy, wait 300s
20240227 19:51:18[INFO] gpu-5 is busy, wait 300s
20240227 19:51:23[INFO] gpu-7 is busy, wait 300s
20240227 19:53:06[INFO] gpu-0 is busy, wait 300s
20240227 19:53:33[INFO] gpu-3 is busy, wait 300s
20240227 19:55:13[INFO] gpu-6 is busy, wait 300s
20240227 19:55:26[INFO] gpu-1 is busy, wait 300s
20240227 19:55:30[INFO] gpu-4 is busy, wait 300s
20240227 19:56:22[INFO] gpu-2 is busy, wait 300s
20240227 19:56:29[INFO] gpu-7 is busy, wait 300s
20240227 19:56:30[INFO] gpu-5 is busy, wait 300s
20240227 19:58:15[INFO] gpu-0 is busy, wait 300s
20240227 19:59:02[INFO] gpu-3 is busy, wait 300s
20240227 20:00:19[INFO] gpu-6 is busy, wait 300s
20240227 20:00:33[INFO] gpu-1 is busy, wait 300s
20240227 20:00:44[INFO] gpu-4 is busy, wait 300s
20240227 20:01:36[INFO] gpu-7 is busy, wait 300s
20240227 20:01:37[INFO] gpu-5 is busy, wait 300s
20240227 20:01:52[INFO] gpu-2 is busy, wait 300s
20240227 20:03:27[INFO] gpu-0 is busy, wait 300s
20240227 20:04:07[INFO] gpu-3 is busy, wait 300s
20240227 20:05:26[INFO] gpu-6 is busy, wait 300s
20240227 20:05:37[INFO] gpu-1 is busy, wait 300s
20240227 20:05:49[INFO] gpu-4 is busy, wait 300s
20240227 20:06:43[INFO] gpu-7 is busy, wait 300s
20240227 20:06:48[INFO] gpu-5 is busy, wait 300s
20240227 20:07:03[INFO] gpu-2 is busy, wait 300s
20240227 20:08:38[INFO] gpu-0 is busy, wait 300s
20240227 20:09:15[INFO] gpu-3 is busy, wait 300s
20240227 20:10:44[INFO] gpu-1 is busy, wait 300s
20240227 20:10:50[INFO] gpu-6 is busy, wait 300s
20240227 20:10:56[INFO] gpu-4 is busy, wait 300s
20240227 20:12:05[INFO] gpu-5 is busy, wait 300s
20240227 20:12:09[INFO] gpu-7 is busy, wait 300s
20240227 20:12:19[INFO] gpu-2 is busy, wait 300s
20240227 20:13:46[INFO] gpu-0 is busy, wait 300s
20240227 20:14:27[INFO] gpu-3 is busy, wait 300s
20240227 20:15:56[INFO] gpu-6 is busy, wait 300s
20240227 20:16:09[INFO] gpu-4 is busy, wait 300s
20240227 20:16:09[INFO] gpu-1 is busy, wait 300s
20240227 20:17:13[INFO] gpu-5 is busy, wait 300s
20240227 20:17:20[INFO] gpu-7 is busy, wait 300s
20240227 20:17:26[INFO] gpu-2 is busy, wait 300s
20240227 20:19:01[INFO] gpu-0 is busy, wait 300s
20240227 20:19:38[INFO] gpu-3 is busy, wait 300s
20240227 20:21:02[INFO] gpu-6 is busy, wait 300s
20240227 20:21:13[INFO] gpu-4 is busy, wait 300s
20240227 20:21:21[INFO] gpu-1 is busy, wait 300s
20240227 20:22:18[INFO] gpu-5 is busy, wait 300s
20240227 20:22:38[INFO] gpu-2 is busy, wait 300s
20240227 20:22:46[INFO] gpu-7 is busy, wait 300s
20240227 20:24:22[INFO] gpu-0 is busy, wait 300s
20240227 20:24:45[INFO] gpu-3 is busy, wait 300s
20240227 20:26:19[INFO] gpu-4 is busy, wait 300s
20240227 20:26:26[INFO] gpu-6 is busy, wait 300s
20240227 20:26:28[INFO] gpu-1 is busy, wait 300s
20240227 20:27:24[INFO] gpu-5 is busy, wait 300s
20240227 20:27:42[INFO] gpu-2 is busy, wait 300s
20240227 20:27:52[INFO] gpu-7 is busy, wait 300s
20240227 20:29:29[INFO] gpu-0 is busy, wait 300s
20240227 20:30:16[INFO] gpu-3 is busy, wait 300s
20240227 20:31:29[INFO] gpu-4 is busy, wait 300s
20240227 20:31:32[INFO] gpu-6 is busy, wait 300s
20240227 20:32:02[INFO] gpu-1 is busy, wait 300s
20240227 20:32:48[INFO] gpu-2 is busy, wait 300s
20240227 20:33:06[INFO] gpu-5 is busy, wait 300s
20240227 20:33:13[INFO] gpu-7 is busy, wait 300s
20240227 20:34:34[INFO] gpu-0 is busy, wait 300s
20240227 20:35:39[INFO] gpu-3 is busy, wait 300s
20240227 20:36:33[INFO] gpu-4 is busy, wait 300s
20240227 20:37:07[INFO] gpu-6 is busy, wait 300s
20240227 20:37:08[INFO] gpu-1 is busy, wait 300s
20240227 20:37:55[INFO] gpu-2 is busy, wait 300s
20240227 20:38:18[INFO] gpu-5 is busy, wait 300s
20240227 20:38:44[INFO] gpu-7 is busy, wait 300s
20240227 20:39:46[INFO] gpu-0 is busy, wait 300s
20240227 20:40:44[INFO] gpu-3 is busy, wait 300s
20240227 20:41:39[INFO] gpu-4 is busy, wait 300s
20240227 20:42:11[INFO] gpu-6 is busy, wait 300s
20240227 20:42:19[INFO] gpu-1 is busy, wait 300s
20240227 20:43:09[INFO] gpu-2 is busy, wait 300s
20240227 20:43:36[INFO] gpu-5 is busy, wait 300s
20240227 20:43:48[INFO] gpu-7 is busy, wait 300s
20240227 20:44:54[INFO] gpu-0 is busy, wait 300s
20240227 20:46:04[INFO] gpu-3 is busy, wait 300s
20240227 20:46:45[INFO] gpu-4 is busy, wait 300s
20240227 20:47:25[INFO] gpu-6 is busy, wait 300s
20240227 20:47:38[INFO] gpu-1 is busy, wait 300s
20240227 20:48:17[INFO] gpu-2 is busy, wait 300s
20240227 20:48:49[INFO] gpu-5 is busy, wait 300s
20240227 20:49:05[INFO] gpu-7 is busy, wait 300s
20240227 20:49:58[INFO] gpu-0 is busy, wait 300s
20240227 20:51:08[INFO] gpu-3 is busy, wait 300s
20240227 20:51:58[INFO] gpu-4 is busy, wait 300s
20240227 20:52:48[INFO] gpu-1 is busy, wait 300s
20240227 20:52:57[INFO] gpu-6 is busy, wait 300s
20240227 20:53:21[INFO] gpu-2 is busy, wait 300s
20240227 20:53:55[INFO] gpu-5 is busy, wait 300s
20240227 20:54:15[INFO] gpu-7 is busy, wait 300s
20240227 20:55:03[INFO] gpu-0 is busy, wait 300s
20240227 20:56:24[INFO] gpu-3 is busy, wait 300s
20240227 20:57:03[INFO] gpu-4 is busy, wait 300s
20240227 20:58:01[INFO] gpu-1 is busy, wait 300s
20240227 20:58:26[INFO] gpu-2 is busy, wait 300s
20240227 20:58:42[INFO] gpu-6 is busy, wait 300s
20240227 20:59:00[INFO] gpu-5 is busy, wait 300s
20240227 20:59:21[INFO] gpu-7 is busy, wait 300s
20240227 21:00:10[INFO] gpu-0 is busy, wait 300s
20240227 21:01:57[INFO] gpu-3 is busy, wait 300s
20240227 21:02:17[INFO] gpu-4 is busy, wait 300s
20240227 21:03:08[INFO] gpu-1 is busy, wait 300s
20240227 21:03:40[INFO] gpu-2 is busy, wait 300s
20240227 21:03:55[INFO] gpu-6 is busy, wait 300s
20240227 21:04:05[INFO] gpu-5 is busy, wait 300s
20240227 21:04:27[INFO] gpu-7 is busy, wait 300s
20240227 21:05:25[INFO] gpu-0 is busy, wait 300s
20240227 21:07:04[INFO] gpu-3 is busy, wait 300s
20240227 21:07:28[INFO] gpu-4 is busy, wait 300s
20240227 21:08:15[INFO] gpu-1 is busy, wait 300s
20240227 21:08:45[INFO] gpu-2 is busy, wait 300s
20240227 21:09:01[INFO] gpu-6 is busy, wait 300s
20240227 21:09:10[INFO] gpu-5 is busy, wait 300s
20240227 21:09:38[INFO] gpu-7 is busy, wait 300s
20240227 21:10:30[INFO] gpu-0 is busy, wait 300s
20240227 21:12:32[INFO] gpu-3 is busy, wait 300s
20240227 21:13:01[INFO] gpu-4 is busy, wait 300s
20240227 21:13:20[INFO] gpu-1 is busy, wait 300s
20240227 21:13:55[INFO] gpu-2 is busy, wait 300s
20240227 21:14:08[INFO] gpu-6 is busy, wait 300s
20240227 21:14:39[INFO] gpu-5 is busy, wait 300s
20240227 21:14:51[INFO] gpu-7 is busy, wait 300s
20240227 21:15:36[INFO] gpu-0 is busy, wait 300s
20240227 21:17:40[INFO] gpu-3 is busy, wait 300s
20240227 21:18:07[INFO] gpu-4 is busy, wait 300s
20240227 21:18:47[INFO] gpu-1 is busy, wait 300s
20240227 21:19:00[INFO] gpu-2 is busy, wait 300s
20240227 21:19:15[INFO] gpu-6 is busy, wait 300s
20240227 21:19:43[INFO] gpu-5 is busy, wait 300s
20240227 21:20:09[INFO] gpu-7 is busy, wait 300s
20240227 21:21:12[INFO] gpu-0 is busy, wait 300s
20240227 21:22:45[INFO] gpu-3 is busy, wait 300s
20240227 21:23:11[INFO] gpu-4 is busy, wait 300s
20240227 21:24:03[INFO] gpu-1 is busy, wait 300s
20240227 21:24:13[INFO] gpu-2 is busy, wait 300s
20240227 21:24:19[INFO] gpu-6 is busy, wait 300s
20240227 21:24:57[INFO] gpu-5 is busy, wait 300s
20240227 21:25:29[INFO] gpu-7 is busy, wait 300s
20240227 21:26:16[INFO] gpu-0 is busy, wait 300s
20240227 21:27:57[INFO] gpu-3 is busy, wait 300s
20240227 21:28:36[INFO] gpu-4 is busy, wait 300s
20240227 21:29:07[INFO] gpu-1 is busy, wait 300s
20240227 21:29:32[INFO] gpu-2 is busy, wait 300s
20240227 21:29:33[INFO] gpu-6 is busy, wait 300s
20240227 21:30:03[INFO] gpu-5 is busy, wait 300s
20240227 21:30:33[INFO] gpu-7 is busy, wait 300s
20240227 21:31:29[INFO] gpu-0 is busy, wait 300s
20240227 21:33:08[INFO] gpu-3 is busy, wait 300s
20240227 21:33:58[INFO] gpu-4 is busy, wait 300s
20240227 21:34:24[INFO] gpu-1 is busy, wait 300s
20240227 21:34:36[INFO] gpu-2 is busy, wait 300s
20240227 21:34:38[INFO] gpu-6 is busy, wait 300s
20240227 21:35:17[INFO] gpu-5 is busy, wait 300s
20240227 21:35:37[INFO] gpu-7 is busy, wait 300s
20240227 21:36:33[INFO] gpu-0 is busy, wait 300s
20240227 21:38:21[INFO] gpu-3 is busy, wait 300s
20240227 21:39:19[INFO] gpu-4 is busy, wait 300s
20240227 21:39:44[INFO] gpu-6 is busy, wait 300s
20240227 21:39:45[INFO] gpu-1 is busy, wait 300s
20240227 21:40:09[INFO] gpu-2 is busy, wait 300s
20240227 21:40:29[INFO] gpu-5 is busy, wait 300s
20240227 21:40:57[INFO] gpu-7 is busy, wait 300s
20240227 21:41:38[INFO] gpu-0 is busy, wait 300s
20240227 21:43:36[INFO] gpu-3 is busy, wait 300s
20240227 21:44:30[INFO] gpu-4 is busy, wait 300s
20240227 21:44:51[INFO] gpu-6 is busy, wait 300s
20240227 21:45:02[INFO] gpu-1 is busy, wait 300s
20240227 21:45:19[INFO] gpu-2 is busy, wait 300s
20240227 21:45:36[INFO] gpu-5 is busy, wait 300s
20240227 21:46:07[INFO] gpu-7 is busy, wait 300s
20240227 21:46:44[INFO] gpu-0 is busy, wait 300s
20240227 21:48:57[INFO] gpu-3 is busy, wait 300s
20240227 21:49:37[INFO] gpu-4 is busy, wait 300s
20240227 21:50:23[INFO] gpu-1 is busy, wait 300s
20240227 21:50:29[INFO] gpu-2 is busy, wait 300s
20240227 21:50:32[INFO] gpu-6 is busy, wait 300s
20240227 21:50:47[INFO] gpu-5 is busy, wait 300s
20240227 21:51:44[INFO] gpu-7 is busy, wait 300s
20240227 21:51:50[INFO] gpu-0 is busy, wait 300s
20240227 21:54:31[INFO] gpu-3 is busy, wait 300s
20240227 21:54:45[INFO] gpu-4 is busy, wait 300s
20240227 21:55:30[INFO] gpu-1 is busy, wait 300s
20240227 21:55:36[INFO] gpu-2 is busy, wait 300s
20240227 21:55:39[INFO] gpu-6 is busy, wait 300s
20240227 21:55:51[INFO] gpu-5 is busy, wait 300s
20240227 21:56:54[INFO] gpu-7 is busy, wait 300s
20240227 21:57:05[INFO] gpu-0 is busy, wait 300s
20240227 21:59:41[INFO] gpu-3 is busy, wait 300s
20240227 21:59:49[INFO] gpu-4 is busy, wait 300s
20240227 22:00:35[INFO] gpu-1 is busy, wait 300s
20240227 22:00:42[INFO] gpu-2 is busy, wait 300s
20240227 22:00:47[INFO] gpu-6 is busy, wait 300s
20240227 22:01:17[INFO] gpu-5 is busy, wait 300s
20240227 22:02:08[INFO] gpu-7 is busy, wait 300s
20240227 22:02:13[INFO] gpu-0 is busy, wait 300s
20240227 22:04:55[INFO] gpu-3 is busy, wait 300s
20240227 22:04:58[INFO] gpu-4 is busy, wait 300s
20240227 22:05:41[INFO] gpu-1 is busy, wait 300s
20240227 22:05:49[INFO] gpu-2 is busy, wait 300s
20240227 22:05:53[INFO] gpu-6 is busy, wait 300s
20240227 22:06:29[INFO] gpu-5 is busy, wait 300s
20240227 22:07:24[INFO] gpu-0 is busy, wait 300s
20240227 22:07:33[INFO] gpu-7 is busy, wait 300s
20240227 22:10:02[INFO] gpu-4 is busy, wait 300s
20240227 22:10:14[INFO] gpu-3 is busy, wait 300s
20240227 22:10:56[INFO] gpu-1 is busy, wait 300s
20240227 22:11:00[INFO] gpu-2 is busy, wait 300s
20240227 22:11:09[INFO] gpu-6 is busy, wait 300s
20240227 22:11:37[INFO] gpu-5 is busy, wait 300s
20240227 22:12:38[INFO] gpu-0 is busy, wait 300s
20240227 22:12:40[INFO] gpu-7 is busy, wait 300s
20240227 22:15:21[INFO] gpu-4 is busy, wait 300s
20240227 22:15:22[INFO] gpu-3 is busy, wait 300s
20240227 22:16:17[INFO] gpu-2 is busy, wait 300s
20240227 22:16:20[INFO] gpu-6 is busy, wait 300s
20240227 22:16:23[INFO] gpu-1 is busy, wait 300s
20240227 22:16:50[INFO] gpu-5 is busy, wait 300s
20240227 22:17:45[INFO] gpu-0 is busy, wait 300s
20240227 22:18:13[INFO] gpu-7 is busy, wait 300s
20240227 22:20:25[INFO] gpu-4 is busy, wait 300s
20240227 22:20:33[INFO] gpu-3 is busy, wait 300s
20240227 22:21:27[INFO] gpu-1 is busy, wait 300s
20240227 22:21:35[INFO] gpu-2 is busy, wait 300s
20240227 22:21:46[INFO] gpu-6 is busy, wait 300s
20240227 22:21:54[INFO] gpu-5 is busy, wait 300s
20240227 22:22:57[INFO] gpu-0 is busy, wait 300s
20240227 22:23:20[INFO] gpu-7 is busy, wait 300s
20240227 22:25:31[INFO] gpu-4 is busy, wait 300s
20240227 22:25:38[INFO] gpu-3 is busy, wait 300s
20240227 22:26:34[INFO] gpu-1 is busy, wait 300s
20240227 22:26:42[INFO] gpu-2 is busy, wait 300s
20240227 22:27:01[INFO] gpu-6 is busy, wait 300s
20240227 22:27:08[INFO] gpu-5 is busy, wait 300s
20240227 22:28:02[INFO] gpu-0 is busy, wait 300s
20240227 22:28:28[INFO] gpu-7 is busy, wait 300s
20240227 22:30:44[INFO] gpu-4 is busy, wait 300s
20240227 22:31:02[INFO] gpu-3 is busy, wait 300s
20240227 22:31:46[INFO] gpu-1 is busy, wait 300s
20240227 22:31:47[INFO] gpu-2 is busy, wait 300s
20240227 22:32:20[INFO] gpu-6 is busy, wait 300s
20240227 22:37:51[INFO] gpu-6 is busy, wait 300s
20240227 22:43:18[INFO] gpu-6 is busy, wait 300s
20240227 22:48:29[INFO] gpu-6 is busy, wait 300s
20240227 22:53:36[INFO] gpu-6 is busy, wait 300s
20240227 22:59:09[INFO] gpu-6 is busy, wait 300s
20240227 23:04:36[INFO] gpu-6 is busy, wait 300s
20240227 23:09:48[INFO] gpu-6 is busy, wait 300s
20240227 23:15:19[INFO] gpu-6 is busy, wait 300s
20240227 23:20:24[INFO] gpu-6 is busy, wait 300s
20240227 23:25:49[INFO] gpu-6 is busy, wait 300s
20240227 23:30:55[INFO] gpu-6 is busy, wait 300s
20240227 23:36:43[INFO] gpu-6 is busy, wait 300s
20240227 23:42:03[INFO] gpu-6 is busy, wait 300s
20240227 23:47:17[INFO] gpu-6 is busy, wait 300s
20240227 23:52:42[INFO] gpu-6 is busy, wait 300s
20240227 23:58:42[INFO] gpu-6 is busy, wait 300s
20240228 00:03:48[INFO] gpu-6 is busy, wait 300s
20240228 00:09:05[INFO] gpu-6 is busy, wait 300s
20240228 00:14:19[INFO] gpu-6 is busy, wait 300s
20240228 00:19:33[INFO] gpu-6 is busy, wait 300s
20240228 00:24:38[INFO] gpu-6 is busy, wait 300s
20240228 00:30:04[INFO] gpu-6 is busy, wait 300s
20240228 00:35:31[INFO] gpu-6 is busy, wait 300s
20240228 00:40:47[INFO] gpu-6 is busy, wait 300s
20240228 00:46:38[INFO] gpu-6 is busy, wait 300s
20240228 00:52:20[INFO] gpu-6 is busy, wait 300s
20240228 00:57:38[INFO] gpu-6 is busy, wait 300s
20240228 01:02:54[INFO] gpu-6 is busy, wait 300s
20240228 01:08:18[INFO] gpu-6 is busy, wait 300s
20240228 01:13:47[INFO] gpu-6 is busy, wait 300s
20240228 01:18:54[INFO] gpu-6 is busy, wait 300s
20240228 01:23:59[INFO] gpu-6 is busy, wait 300s
20240228 01:29:15[INFO] gpu-6 is busy, wait 300s
20240228 01:35:15[INFO] gpu-6 is busy, wait 300s
20240228 01:41:06[INFO] gpu-6 is busy, wait 300s
20240228 01:46:47[INFO] gpu-6 is busy, wait 300s
20240228 01:52:04[INFO] gpu-6 is busy, wait 300s
20240228 01:57:38[INFO] gpu-6 is busy, wait 300s
20240228 02:03:55[INFO] gpu-6 is busy, wait 300s
20240228 02:09:02[INFO] gpu-6 is busy, wait 300s
20240228 02:14:09[INFO] gpu-6 is busy, wait 300s
20240228 02:19:24[INFO] gpu-6 is busy, wait 300s
20240228 02:24:38[INFO] gpu-6 is busy, wait 300s
20240228 02:29:45[INFO] gpu-6 is busy, wait 300s
20240228 02:34:51[INFO] gpu-6 is busy, wait 300s
20240228 02:40:03[INFO] gpu-6 is busy, wait 300s
20240228 02:45:10[INFO] gpu-6 is busy, wait 300s
20240228 02:50:27[INFO] gpu-6 is busy, wait 300s
20240228 02:55:32[INFO] gpu-6 is busy, wait 300s
20240228 03:00:40[INFO] gpu-6 is busy, wait 300s
20240228 03:05:59[INFO] gpu-6 is busy, wait 300s
20240228 03:11:04[INFO] gpu-6 is busy, wait 300s
20240228 03:16:31[INFO] gpu-6 is busy, wait 300s
20240228 03:21:39[INFO] gpu-6 is busy, wait 300s
20240228 03:26:59[INFO] gpu-6 is busy, wait 300s
20240228 03:32:06[INFO] gpu-6 is busy, wait 300s
20240228 03:37:42[INFO] gpu-6 is busy, wait 300s
20240228 03:43:05[INFO] gpu-6 is busy, wait 300s
20240228 03:48:37[INFO] gpu-6 is busy, wait 300s
20240228 03:53:44[INFO] gpu-6 is busy, wait 300s
20240228 03:58:54[INFO] gpu-6 is busy, wait 300s
20240228 04:04:15[INFO] gpu-6 is busy, wait 300s
20240228 04:09:23[INFO] gpu-6 is busy, wait 300s
20240228 04:14:32[INFO] gpu-6 is busy, wait 300s
20240228 04:19:37[INFO] gpu-6 is busy, wait 300s
20240228 04:25:29[INFO] gpu-6 is busy, wait 300s
20240228 04:30:34[INFO] gpu-6 is busy, wait 300s
20240228 04:35:42[INFO] gpu-6 is busy, wait 300s
20240228 04:40:51[INFO] gpu-6 is busy, wait 300s
20240228 04:45:59[INFO] gpu-6 is busy, wait 300s
20240228 04:51:05[INFO] gpu-6 is busy, wait 300s
20240228 04:56:31[INFO] gpu-6 is busy, wait 300s
20240228 05:02:00[INFO] gpu-6 is busy, wait 300s
20240228 05:07:15[INFO] gpu-6 is busy, wait 300s
20240228 05:12:27[INFO] gpu-6 is busy, wait 300s
20240228 05:18:10[INFO] gpu-6 is busy, wait 300s
20240228 05:24:05[INFO] gpu-6 is busy, wait 300s
20240228 05:29:38[INFO] gpu-6 is busy, wait 300s
20240228 05:35:08[INFO] gpu-6 is busy, wait 300s
20240228 05:40:27[INFO] gpu-6 is busy, wait 300s
20240228 05:45:35[INFO] gpu-6 is busy, wait 300s
20240228 05:51:06[INFO] gpu-6 is busy, wait 300s
20240228 05:56:37[INFO] gpu-6 is busy, wait 300s
20240228 06:01:51[INFO] gpu-6 is busy, wait 300s
20240228 06:06:59[INFO] gpu-6 is busy, wait 300s
20240228 06:12:12[INFO] gpu-6 is busy, wait 300s
20240228 06:17:25[INFO] gpu-6 is busy, wait 300s
20240228 06:22:39[INFO] gpu-6 is busy, wait 300s
20240228 06:28:12[INFO] gpu-6 is busy, wait 300s
20240228 06:33:43[INFO] gpu-6 is busy, wait 300s
20240228 06:39:06[INFO] gpu-6 is busy, wait 300s
20240228 06:44:19[INFO] gpu-6 is busy, wait 300s
20240228 06:49:56[INFO] gpu-6 is busy, wait 300s
20240228 06:55:11[INFO] gpu-6 is busy, wait 300s
20240228 07:01:15[INFO] gpu-6 is busy, wait 300s
20240228 07:06:22[INFO] gpu-6 is busy, wait 300s
20240228 07:11:58[INFO] gpu-6 is busy, wait 300s
20240228 07:17:26[INFO] gpu-6 is busy, wait 300s
20240228 07:22:48[INFO] gpu-6 is busy, wait 300s
20240228 07:27:54[INFO] gpu-6 is busy, wait 300s
20240228 07:33:06[INFO] gpu-6 is busy, wait 300s
20240228 07:38:27[INFO] gpu-6 is busy, wait 300s
20240228 07:43:33[INFO] gpu-6 is busy, wait 300s
20240228 07:49:02[INFO] gpu-6 is busy, wait 300s
20240228 07:54:09[INFO] gpu-6 is busy, wait 300s
20240228 07:59:22[INFO] gpu-6 is busy, wait 300s
20240228 08:04:36[INFO] gpu-6 is busy, wait 300s
20240228 08:09:58[INFO] gpu-6 is busy, wait 300s
20240228 08:15:15[INFO] gpu-6 is busy, wait 300s
20240228 08:20:24[INFO] gpu-6 is busy, wait 300s
20240228 08:25:52[INFO] gpu-6 is busy, wait 300s
20240228 08:31:00[INFO] gpu-6 is busy, wait 300s
20240228 08:36:15[INFO] gpu-6 is busy, wait 300s
20240228 08:41:32[INFO] gpu-6 is busy, wait 300s
20240228 08:46:50[INFO] gpu-6 is busy, wait 300s
20240228 08:52:06[INFO] gpu-6 is busy, wait 300s
20240228 08:57:21[INFO] gpu-6 is busy, wait 300s
20240228 09:02:33[INFO] gpu-6 is busy, wait 300s
20240228 09:07:41[INFO] gpu-6 is busy, wait 300s
20240228 09:12:47[INFO] gpu-6 is busy, wait 300s
20240228 09:18:01[INFO] gpu-6 is busy, wait 300s
20240228 09:23:41[INFO] gpu-6 is busy, wait 300s
20240228 09:28:47[INFO] gpu-6 is busy, wait 300s
20240228 09:34:06[INFO] gpu-6 is busy, wait 300s
20240228 09:39:20[INFO] gpu-6 is busy, wait 300s
20240228 09:44:34[INFO] gpu-6 is busy, wait 300s
20240228 09:49:47[INFO] gpu-6 is busy, wait 300s
20240228 09:55:21[INFO] gpu-6 is busy, wait 300s
20240228 10:00:30[INFO] gpu-6 is busy, wait 300s
20240228 10:05:39[INFO] gpu-6 is busy, wait 300s
20240228 10:11:16[INFO] gpu-6 is busy, wait 300s
20240228 10:16:30[INFO] gpu-6 is busy, wait 300s
20240228 10:21:52[INFO] gpu-6 is busy, wait 300s
20240228 10:27:23[INFO] gpu-6 is busy, wait 300s
20240228 10:32:31[INFO] gpu-6 is busy, wait 300s
20240228 10:34:51[INFO] gpu-0 is busy, wait 300s
20240228 10:34:54[INFO] gpu-1 is busy, wait 300s
20240228 10:34:54[INFO] gpu-5 is busy, wait 300s
20240228 10:34:55[INFO] gpu-4 is busy, wait 300s
20240228 10:34:56[INFO] gpu-2 is busy, wait 300s
20240228 10:35:04[INFO] gpu-3 is busy, wait 300s
20240228 10:35:28[INFO] gpu-7 is busy, wait 300s
20240228 10:37:54[INFO] gpu-6 is busy, wait 300s
20240228 10:40:00[INFO] gpu-5 is busy, wait 300s
20240228 10:40:01[INFO] gpu-4 is busy, wait 300s
20240228 10:40:06[INFO] gpu-0 is busy, wait 300s
20240228 10:40:10[INFO] gpu-2 is busy, wait 300s
20240228 10:40:14[INFO] gpu-1 is busy, wait 300s
20240228 10:40:16[INFO] gpu-3 is busy, wait 300s
20240228 10:40:35[INFO] gpu-7 is busy, wait 300s
20240228 10:43:00[INFO] gpu-6 is busy, wait 300s
20240228 10:45:04[INFO] gpu-5 is busy, wait 300s
20240228 10:45:06[INFO] gpu-4 is busy, wait 300s
20240228 10:45:16[INFO] gpu-0 is busy, wait 300s
20240228 10:45:27[INFO] gpu-3 is busy, wait 300s
20240228 10:45:28[INFO] gpu-2 is busy, wait 300s
20240228 10:45:43[INFO] gpu-1 is busy, wait 300s
20240228 10:45:50[INFO] gpu-7 is busy, wait 300s
20240228 10:48:08[INFO] gpu-6 is busy, wait 300s
20240228 10:50:12[INFO] gpu-4 is busy, wait 300s
20240228 10:50:22[INFO] gpu-5 is busy, wait 300s
20240228 10:50:23[INFO] gpu-0 is busy, wait 300s
20240228 10:50:28[INFO] gpu-3 is busy, wait 300s
20240228 10:50:29[INFO] gpu-2 is busy, wait 300s
20240228 10:50:49[INFO] gpu-1 is busy, wait 300s
20240228 10:51:16[INFO] gpu-7 is busy, wait 300s
20240228 10:53:21[INFO] gpu-6 is busy, wait 300s
20240228 10:55:13[INFO] gpu-4 is busy, wait 300s
20240228 10:55:32[INFO] gpu-3 is busy, wait 300s
20240228 10:55:34[INFO] gpu-2 is busy, wait 300s
20240228 10:56:35[INFO] gpu-1 is busy, wait 300s
20240228 10:56:38[INFO] gpu-5 is busy, wait 300s
20240228 10:56:41[INFO] gpu-7 is busy, wait 300s
20240228 10:56:41[INFO] gpu-0 is busy, wait 300s
20240228 10:58:28[INFO] gpu-6 is busy, wait 300s
20240228 11:00:19[INFO] gpu-4 is busy, wait 300s
20240228 11:00:39[INFO] gpu-3 is busy, wait 300s
20240228 11:00:40[INFO] gpu-2 is busy, wait 300s
20240228 11:01:41[INFO] gpu-1 is busy, wait 300s
20240228 11:01:45[INFO] gpu-5 is busy, wait 300s
20240228 11:01:47[INFO] gpu-7 is busy, wait 300s
20240228 11:02:10[INFO] gpu-0 is busy, wait 300s
20240228 11:03:34[INFO] gpu-6 is busy, wait 300s
20240228 11:05:47[INFO] gpu-2 is busy, wait 300s
20240228 11:06:00[INFO] gpu-3 is busy, wait 300s
20240228 11:06:17[INFO] gpu-4 is busy, wait 300s
20240228 11:07:04[INFO] gpu-5 is busy, wait 300s
20240228 11:07:10[INFO] gpu-1 is busy, wait 300s
20240228 11:07:24[INFO] gpu-7 is busy, wait 300s
20240228 11:07:31[INFO] gpu-0 is busy, wait 300s
20240228 11:08:53[INFO] gpu-6 is busy, wait 300s
20240228 11:10:54[INFO] gpu-2 is busy, wait 300s
20240228 11:11:06[INFO] gpu-3 is busy, wait 300s
20240228 11:11:24[INFO] gpu-4 is busy, wait 300s
20240228 11:12:23[INFO] gpu-5 is busy, wait 300s
20240228 11:12:24[INFO] gpu-1 is busy, wait 300s
20240228 11:12:44[INFO] gpu-7 is busy, wait 300s
20240228 11:12:49[INFO] gpu-0 is busy, wait 300s
20240228 11:14:00[INFO] gpu-6 is busy, wait 300s
20240228 11:15:58[INFO] gpu-2 is busy, wait 300s
20240228 11:16:12[INFO] gpu-3 is busy, wait 300s
20240228 11:16:43[INFO] gpu-4 is busy, wait 300s
20240228 11:17:31[INFO] gpu-5 is busy, wait 300s
20240228 11:17:44[INFO] gpu-1 is busy, wait 300s
20240228 11:18:01[INFO] gpu-0 is busy, wait 300s
20240228 11:18:32[INFO] gpu-7 is busy, wait 300s
20240228 11:19:06[INFO] gpu-6 is busy, wait 300s
20240228 11:21:11[INFO] gpu-2 is busy, wait 300s
20240228 11:21:19[INFO] gpu-3 is busy, wait 300s
20240228 11:22:00[INFO] gpu-4 is busy, wait 300s
20240228 11:22:49[INFO] gpu-5 is busy, wait 300s
20240228 11:22:56[INFO] gpu-1 is busy, wait 300s
20240228 11:23:07[INFO] gpu-0 is busy, wait 300s
20240228 11:24:14[INFO] gpu-7 is busy, wait 300s
20240228 11:24:32[INFO] gpu-6 is busy, wait 300s
20240228 11:26:15[INFO] gpu-2 is busy, wait 300s
20240228 11:26:47[INFO] gpu-3 is busy, wait 300s
20240228 11:27:14[INFO] gpu-4 is busy, wait 300s
20240228 11:28:15[INFO] gpu-0 is busy, wait 300s
20240228 11:29:20[INFO] gpu-5 is busy, wait 300s
20240228 11:29:35[INFO] gpu-1 is busy, wait 300s
20240228 11:29:38[INFO] gpu-6 is busy, wait 300s
20240228 11:30:00[INFO] gpu-7 is busy, wait 300s
20240228 11:31:23[INFO] gpu-2 is busy, wait 300s
20240228 11:32:07[INFO] gpu-3 is busy, wait 300s
20240228 11:32:22[INFO] gpu-4 is busy, wait 300s
20240228 11:33:45[INFO] gpu-0 is busy, wait 300s
20240228 11:34:27[INFO] gpu-5 is busy, wait 300s
20240228 11:35:05[INFO] gpu-1 is busy, wait 300s
20240228 11:35:12[INFO] gpu-6 is busy, wait 300s
20240228 11:35:20[INFO] gpu-7 is busy, wait 300s
20240228 11:36:42[INFO] gpu-2 is busy, wait 300s
20240228 11:37:13[INFO] gpu-3 is busy, wait 300s
20240228 11:37:28[INFO] gpu-4 is busy, wait 300s
20240228 11:38:58[INFO] gpu-0 is busy, wait 300s
20240228 11:39:32[INFO] gpu-5 is busy, wait 300s
20240228 11:40:11[INFO] gpu-1 is busy, wait 300s
20240228 11:40:36[INFO] gpu-7 is busy, wait 300s
20240228 11:40:41[INFO] gpu-6 is busy, wait 300s
20240228 11:41:58[INFO] gpu-2 is busy, wait 300s
20240228 11:42:43[INFO] gpu-4 is busy, wait 300s
20240228 11:42:55[INFO] gpu-3 is busy, wait 300s
20240228 11:44:04[INFO] gpu-0 is busy, wait 300s
20240228 11:45:04[INFO] gpu-5 is busy, wait 300s
20240228 11:45:44[INFO] gpu-7 is busy, wait 300s
20240228 11:45:45[INFO] gpu-1 is busy, wait 300s
20240228 11:46:04[INFO] gpu-6 is busy, wait 300s
20240228 11:47:12[INFO] gpu-2 is busy, wait 300s
20240228 11:47:48[INFO] gpu-4 is busy, wait 300s
20240228 11:48:06[INFO] gpu-3 is busy, wait 300s
20240228 11:49:41[INFO] gpu-0 is busy, wait 300s
20240228 11:50:20[INFO] gpu-5 is busy, wait 300s
20240228 11:50:50[INFO] gpu-7 is busy, wait 300s
20240228 11:50:51[INFO] gpu-1 is busy, wait 300s
20240228 11:51:16[INFO] gpu-6 is busy, wait 300s
20240228 11:52:39[INFO] gpu-2 is busy, wait 300s
20240228 11:52:59[INFO] gpu-4 is busy, wait 300s
20240228 11:53:14[INFO] gpu-3 is busy, wait 300s
20240228 11:54:53[INFO] gpu-0 is busy, wait 300s
20240228 11:55:26[INFO] gpu-5 is busy, wait 300s
20240228 11:55:56[INFO] gpu-7 is busy, wait 300s
20240228 11:56:03[INFO] gpu-1 is busy, wait 300s
20240228 11:56:55[INFO] gpu-6 is busy, wait 300s
20240228 11:57:52[INFO] gpu-2 is busy, wait 300s
20240228 11:58:30[INFO] gpu-3 is busy, wait 300s
20240228 11:58:47[INFO] gpu-4 is busy, wait 300s
20240228 12:00:01[INFO] gpu-0 is busy, wait 300s
20240228 12:00:47[INFO] gpu-5 is busy, wait 300s
20240228 12:01:17[INFO] gpu-1 is busy, wait 300s
20240228 12:01:20[INFO] gpu-7 is busy, wait 300s
20240228 12:02:58[INFO] gpu-2 is busy, wait 300s
20240228 12:03:37[INFO] gpu-6 is busy, wait 300s
20240228 12:03:39[INFO] gpu-3 is busy, wait 300s
20240228 12:04:07[INFO] gpu-4 is busy, wait 300s
20240228 12:05:12[INFO] gpu-0 is busy, wait 300s
20240228 12:06:06[INFO] gpu-5 is busy, wait 300s
20240228 12:06:24[INFO] gpu-1 is busy, wait 300s
20240228 12:07:28[INFO] gpu-7 is busy, wait 300s
20240228 12:08:18[INFO] gpu-2 is busy, wait 300s
20240228 12:09:02[INFO] gpu-6 is busy, wait 300s
20240228 12:09:11[INFO] gpu-3 is busy, wait 300s
20240228 12:09:19[INFO] gpu-4 is busy, wait 300s
20240228 12:10:17[INFO] gpu-0 is busy, wait 300s
20240228 12:11:12[INFO] gpu-5 is busy, wait 300s
20240228 12:11:32[INFO] gpu-1 is busy, wait 300s
20240228 12:12:35[INFO] gpu-7 is busy, wait 300s
20240228 12:13:25[INFO] gpu-2 is busy, wait 300s
20240228 12:14:15[INFO] gpu-6 is busy, wait 300s
20240228 12:14:25[INFO] gpu-3 is busy, wait 300s
20240228 12:14:34[INFO] gpu-4 is busy, wait 300s
20240228 12:15:30[INFO] gpu-0 is busy, wait 300s
20240228 12:16:32[INFO] gpu-5 is busy, wait 300s
20240228 12:16:39[INFO] gpu-1 is busy, wait 300s
20240228 12:17:49[INFO] gpu-7 is busy, wait 300s
20240228 12:18:45[INFO] gpu-2 is busy, wait 300s
20240228 12:19:20[INFO] gpu-6 is busy, wait 300s
20240228 12:19:38[INFO] gpu-3 is busy, wait 300s
20240228 12:19:41[INFO] gpu-4 is busy, wait 300s
20240228 12:20:38[INFO] gpu-0 is busy, wait 300s
20240228 12:21:54[INFO] gpu-1 is busy, wait 300s
20240228 12:21:58[INFO] gpu-5 is busy, wait 300s
20240228 12:23:00[INFO] gpu-7 is busy, wait 300s
20240228 12:23:58[INFO] gpu-2 is busy, wait 300s
20240228 12:24:27[INFO] gpu-6 is busy, wait 300s
20240228 12:25:01[INFO] gpu-4 is busy, wait 300s
20240228 12:25:10[INFO] gpu-3 is busy, wait 300s
20240228 12:25:54[INFO] gpu-0 is busy, wait 300s
20240228 12:27:06[INFO] gpu-5 is busy, wait 300s
20240228 12:27:30[INFO] gpu-1 is busy, wait 300s
20240228 12:28:15[INFO] gpu-7 is busy, wait 300s
20240228 12:29:06[INFO] gpu-2 is busy, wait 300s
20240228 12:29:32[INFO] gpu-6 is busy, wait 300s
20240228 12:30:13[INFO] gpu-4 is busy, wait 300s
20240228 12:30:15[INFO] gpu-3 is busy, wait 300s
20240228 12:31:01[INFO] gpu-0 is busy, wait 300s
20240228 12:32:12[INFO] gpu-5 is busy, wait 300s
20240228 12:32:37[INFO] gpu-1 is busy, wait 300s
20240228 12:33:29[INFO] gpu-7 is busy, wait 300s
20240228 12:34:17[INFO] gpu-2 is busy, wait 300s
20240228 12:34:51[INFO] gpu-6 is busy, wait 300s
20240228 12:35:56[INFO] gpu-3 is busy, wait 300s
20240228 12:36:08[INFO] gpu-0 is busy, wait 300s
20240228 12:36:33[INFO] gpu-4 is busy, wait 300s
20240228 12:37:34[INFO] gpu-5 is busy, wait 300s
20240228 12:37:45[INFO] gpu-1 is busy, wait 300s
20240228 12:38:42[INFO] gpu-7 is busy, wait 300s
20240228 12:39:29[INFO] gpu-2 is busy, wait 300s
20240228 12:41:08[INFO] gpu-6 is busy, wait 300s
20240228 12:41:13[INFO] gpu-0 is busy, wait 300s
20240228 12:41:19[INFO] gpu-3 is busy, wait 300s
20240228 12:41:40[INFO] gpu-4 is busy, wait 300s
20240228 12:42:40[INFO] gpu-5 is busy, wait 300s
20240228 12:42:53[INFO] gpu-1 is busy, wait 300s
20240228 12:43:50[INFO] gpu-7 is busy, wait 300s
20240228 12:44:34[INFO] gpu-2 is busy, wait 300s
20240228 12:46:19[INFO] gpu-0 is busy, wait 300s
20240228 12:46:21[INFO] gpu-6 is busy, wait 300s
20240228 12:46:26[INFO] gpu-3 is busy, wait 300s
20240228 12:46:46[INFO] gpu-4 is busy, wait 300s
20240228 12:48:03[INFO] gpu-5 is busy, wait 300s
20240228 12:48:08[INFO] gpu-1 is busy, wait 300s
20240228 12:48:56[INFO] gpu-7 is busy, wait 300s
20240228 12:49:56[INFO] gpu-2 is busy, wait 300s
20240228 12:51:51[INFO] gpu-4 is busy, wait 300s
20240228 12:51:53[INFO] gpu-6 is busy, wait 300s
20240228 12:52:09[INFO] gpu-3 is busy, wait 300s
20240228 12:52:18[INFO] gpu-0 is busy, wait 300s
20240228 12:53:18[INFO] gpu-1 is busy, wait 300s
20240228 12:53:25[INFO] gpu-5 is busy, wait 300s
20240228 12:54:35[INFO] gpu-7 is busy, wait 300s
20240228 12:55:13[INFO] gpu-2 is busy, wait 300s
20240228 12:56:58[INFO] gpu-4 is busy, wait 300s
20240228 12:57:53[INFO] gpu-3 is busy, wait 300s
20240228 12:58:13[INFO] gpu-0 is busy, wait 300s
20240228 12:58:15[INFO] gpu-6 is busy, wait 300s
20240228 12:58:30[INFO] gpu-5 is busy, wait 300s
20240228 12:58:40[INFO] gpu-1 is busy, wait 300s
20240228 13:03:50[INFO] gpu-0 is busy, wait 300s
20240228 13:04:23[INFO] gpu-6 is busy, wait 300s
20240228 13:09:20[INFO] gpu-0 is busy, wait 300s
20240228 13:09:34[INFO] gpu-6 is busy, wait 300s
20240228 13:14:44[INFO] gpu-6 is busy, wait 300s
20240228 13:21:35[INFO] gpu-6 is busy, wait 300s
20240228 13:27:12[INFO] gpu-6 is busy, wait 300s
20240228 13:33:03[INFO] gpu-6 is busy, wait 300s
20240228 13:38:41[INFO] gpu-6 is busy, wait 300s
20240228 13:39:31[INFO] gpu-0 is busy, wait 300s
20240228 13:44:51[INFO] gpu-6 is busy, wait 300s
20240228 13:46:01[INFO] gpu-0 is busy, wait 300s
20240228 13:50:53[INFO] gpu-6 is busy, wait 300s
20240228 13:51:02[INFO] gpu-0 is busy, wait 300s
20240228 13:56:15[INFO] gpu-6 is busy, wait 300s
20240228 13:56:49[INFO] gpu-0 is busy, wait 300s
20240228 14:01:50[INFO] gpu-0 is busy, wait 300s
20240228 14:02:38[INFO] gpu-6 is busy, wait 300s
20240228 14:08:55[INFO] gpu-6 is busy, wait 300s
20240228 14:09:06[INFO] gpu-0 is busy, wait 300s
20240228 14:14:59[INFO] gpu-6 is busy, wait 300s
20240228 14:21:05[INFO] gpu-0 is busy, wait 300s
20240228 14:21:47[INFO] gpu-6 is busy, wait 300s
20240228 14:27:16[INFO] gpu-0 is busy, wait 300s
20240228 14:28:18[INFO] gpu-6 is busy, wait 300s
20240228 14:34:22[INFO] gpu-0 is busy, wait 300s
20240228 14:34:36[INFO] gpu-6 is busy, wait 300s
20240228 14:39:45[INFO] gpu-6 is busy, wait 300s
20240228 14:42:17[INFO] gpu-0 is busy, wait 300s
20240228 14:46:53[INFO] gpu-1 is busy, wait 300s
20240228 14:47:01[INFO] gpu-4 is busy, wait 300s
20240228 14:47:19[INFO] gpu-0 is busy, wait 300s
20240228 14:47:32[INFO] gpu-7 is busy, wait 300s
20240228 14:47:44[INFO] gpu-6 is busy, wait 300s
20240228 14:47:47[INFO] gpu-3 is busy, wait 300s
20240228 14:47:48[INFO] gpu-2 is busy, wait 300s
20240228 14:48:01[INFO] gpu-5 is busy, wait 300s
20240228 14:52:58[INFO] gpu-6 is busy, wait 300s
20240228 14:56:34[INFO] gpu-0 is busy, wait 300s
20240228 14:58:49[INFO] gpu-6 is busy, wait 300s
20240228 15:04:17[INFO] gpu-6 is busy, wait 300s
20240228 15:12:19[INFO] gpu-6 is busy, wait 300s
20240228 15:17:26[INFO] gpu-6 is busy, wait 300s
20240228 15:22:50[INFO] gpu-6 is busy, wait 300s
20240228 15:28:05[INFO] gpu-6 is busy, wait 300s
20240228 15:33:11[INFO] gpu-6 is busy, wait 300s
20240228 15:39:16[INFO] gpu-6 is busy, wait 300s
20240228 15:44:34[INFO] gpu-6 is busy, wait 300s
20240228 15:49:43[INFO] gpu-6 is busy, wait 300s
